{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "727b72fb-b718-4ed3-8340-5376050147ed",
   "metadata": {},
   "source": [
    "# TomatoMAP-Cls Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fad99b1c-b47d-4386-bc62-433dda73b066",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-07T06:59:05.120408Z",
     "start_time": "2025-07-07T06:58:55.424375Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment checker:\n",
      "  PyTorch version: 2.7.1+cu126\n",
      "  CUDA version: True\n",
      "  GPU device: Tesla V100-PCIE-16GB\n",
      "  GPU ram: 15.8 GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import argparse\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from PIL import ImageDraw, ImageFont, Image\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import models\n",
    "from torchvision.models import (\n",
    "    MobileNet_V3_Large_Weights,\n",
    "    MobileNet_V3_Small_Weights,\n",
    "    MobileNet_V2_Weights,\n",
    "    ResNet18_Weights,\n",
    ")\n",
    "\n",
    "# env checker\n",
    "print(\"Environment checker:\")\n",
    "print(f\"  PyTorch version: {torch.__version__}\")\n",
    "print(f\"  CUDA version: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"  GPU device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"  GPU ram: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35beeeb4-ca9c-47b4-9893-7d2bf01a1894",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-07T07:00:42.814782Z",
     "start_time": "2025-07-07T07:00:42.797777Z"
    }
   },
   "outputs": [],
   "source": [
    "def save_model(model, path):\n",
    "    torch.save(model.state_dict(), path)\n",
    "    print(f\"model saved at: {path}\")\n",
    "\n",
    "def load_model(model, path, device='cpu'):\n",
    "    model.load_state_dict(torch.load(path, map_location=device))\n",
    "    model.eval()\n",
    "    print(f\"model loaded from: {path}\")\n",
    "\n",
    "def save_checkpoint(model, optimizer, epoch, path):\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict()\n",
    "    }, path)\n",
    "\n",
    "def load_checkpoint(path, model, optimizer, device):\n",
    "    checkpoint = torch.load(path, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    start_epoch = checkpoint['epoch'] + 1\n",
    "    print(f\"from epoch {start_epoch} re-training\")\n",
    "    return start_epoch\n",
    "\n",
    "def get_font(size=30, bold=False):\n",
    "    font_paths = [\n",
    "        \"C:/Windows/Fonts/arialbd.ttf\" if bold else \"C:/Windows/Fonts/arial.ttf\",\n",
    "        \"/usr/share/fonts/truetype/dejavu/DejaVuSans-Bold.ttf\" if bold else \"/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf\",\n",
    "        \"/System/Library/Fonts/Supplemental/Arial-Bold.ttf\" if bold else \"/System/Library/Fonts/Supplemental/Arial.ttf\",\n",
    "    ]\n",
    "    for path in font_paths:\n",
    "        try:\n",
    "            return ImageFont.truetype(path, size=size)\n",
    "        except:\n",
    "            continue\n",
    "    return ImageFont.load_default()\n",
    "\n",
    "def denormalize(img_tensor, mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)):\n",
    "    mean = torch.tensor(mean).view(3, 1, 1).to(img_tensor.device)\n",
    "    std = torch.tensor(std).view(3, 1, 1).to(img_tensor.device)\n",
    "    return torch.clamp(img_tensor * std + mean, 0, 1)\n",
    "\n",
    "def get_model(name, num_classes, pretrained=True):\n",
    "    print(f\"build model: {name}, class number: {num_classes}\")\n",
    "    \n",
    "    if name == 'mobilenet_v3_large':\n",
    "        weights = MobileNet_V3_Large_Weights.DEFAULT if pretrained else None\n",
    "        model = models.mobilenet_v3_large(weights=weights)\n",
    "        model.classifier[3] = nn.Linear(model.classifier[3].in_features, num_classes)\n",
    "    elif name == 'mobilenet_v3_small':\n",
    "        weights = MobileNet_V3_Small_Weights.DEFAULT if pretrained else None\n",
    "        model = models.mobilenet_v3_small(weights=weights)\n",
    "        model.classifier[3] = nn.Linear(model.classifier[3].in_features, num_classes)\n",
    "    elif name == 'mobilenet_v2':\n",
    "        weights = MobileNet_V2_Weights.DEFAULT if pretrained else None\n",
    "        model = models.mobilenet_v2(weights=weights)\n",
    "        model.classifier[1] = nn.Linear(model.classifier[1].in_features, num_classes)\n",
    "    elif name == 'resnet18':\n",
    "        weights = ResNet18_Weights.DEFAULT if pretrained else None\n",
    "        model = models.resnet18(weights=weights)\n",
    "        model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "    else:\n",
    "        raise ValueError(f\"Model {name} not supported.\")\n",
    "\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"parameter info: Total: {total_params:,}, Trainable: {trainable_params:,}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "class BBCHDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, data_dir, split='train', transform=None):\n",
    "        self.data_dir = os.path.join(data_dir, split)\n",
    "        self.transform = transform\n",
    "        \n",
    "        if not os.path.exists(self.data_dir):\n",
    "            raise FileNotFoundError(f\"Directory not found: {self.data_dir}\")\n",
    "        \n",
    "        # get all classes\n",
    "        self.classes = sorted([d for d in os.listdir(self.data_dir)\n",
    "                              if os.path.isdir(os.path.join(self.data_dir, d))])\n",
    "        self.class_to_idx = {cls: idx for idx, cls in enumerate(self.classes)}\n",
    "\n",
    "        self.samples = []\n",
    "        for class_name in self.classes:\n",
    "            class_dir = os.path.join(self.data_dir, class_name)\n",
    "            class_idx = self.class_to_idx[class_name]\n",
    "            \n",
    "            for img_name in os.listdir(class_dir):\n",
    "                if img_name.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                    img_path = os.path.join(class_dir, img_name)\n",
    "                    self.samples.append((img_path, class_idx))\n",
    "        \n",
    "        print(f\"loading {split} dataset: {len(self.samples)} images, {len(self.classes)} classes\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label = self.samples[idx]\n",
    "        \n",
    "        try:\n",
    "            image = Image.open(img_path).convert('RGB')\n",
    "        except Exception as e:\n",
    "            print(f\"failed to load image: {img_path}, error: {e}\")\n",
    "            image = Image.new('RGB', (224, 224), (0, 0, 0))\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label\n",
    "\n",
    "# data enhance\n",
    "def get_transforms(target_size=(640, 640)):\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.Resize(target_size),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.RandomRotation(10),\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    val_transform = transforms.Compose([\n",
    "        transforms.Resize(target_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    return train_transform, val_transform\n",
    "\n",
    "def get_dataloaders(data_dir, batch_size=32, target_size=(640, 640), num_workers=8, include_test=False):\n",
    "    print(f\"building dataloader: {data_dir}\")\n",
    "    \n",
    "    train_transform, val_transform = get_transforms(target_size)\n",
    "    \n",
    "    train_dataset = BBCHDataset(data_dir, 'train', train_transform)\n",
    "    val_dataset = BBCHDataset(data_dir, 'val', val_transform)\n",
    "\n",
    "    # for windows users\n",
    "    import platform\n",
    "    if platform.system() == 'Windows':\n",
    "        num_workers = 0\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, batch_size=batch_size, shuffle=True,\n",
    "        num_workers=num_workers, pin_memory=torch.cuda.is_available()\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset, batch_size=batch_size, shuffle=False,\n",
    "        num_workers=num_workers, pin_memory=torch.cuda.is_available()\n",
    "    )\n",
    "    \n",
    "    test_loader = None\n",
    "    if include_test:\n",
    "        test_dir = os.path.join(data_dir, 'test')\n",
    "        if os.path.exists(test_dir):\n",
    "            test_dataset = BBCHDataset(data_dir, 'test', val_transform)\n",
    "            test_loader = DataLoader(\n",
    "                test_dataset, batch_size=batch_size, shuffle=False,\n",
    "                num_workers=num_workers, pin_memory=torch.cuda.is_available()\n",
    "            )\n",
    "        else:\n",
    "            print(\"test set not found, using val as test\")\n",
    "            test_loader = val_loader\n",
    "\n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a707cf3-f677-4963-bff3-043bdc24f791",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-07T07:03:52.327730Z",
     "start_time": "2025-07-07T07:03:52.313731Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config:\n",
      "  data_dir: TomatoMAP/TomatoMAP-Cls\n",
      "  model_name: mobilenet_v3_large\n",
      "  num_classes: 50\n",
      "  batch_size: 32\n",
      "  num_epochs: 30\n",
      "  learning_rate: 0.0001\n",
      "  target_size: (640, 640)\n",
      "  patience: 3\n",
      "  save_interval: 20\n"
     ]
    }
   ],
   "source": [
    "CLASSIFICATION_CONFIG = {\n",
    "    'data_dir': 'TomatoMAP/TomatoMAP-Cls',\n",
    "    'model_name': 'mobilenet_v3_large',  # 'mobilenet_v3_small', 'mobilenet_v2', 'resnet18'\n",
    "    'num_classes': 50,\n",
    "    'batch_size': 32,\n",
    "    'num_epochs': 30,\n",
    "    'learning_rate': 1e-4,\n",
    "    'target_size': (640, 640),\n",
    "    'patience': 3,\n",
    "    'save_interval': 20\n",
    "}\n",
    "\n",
    "print(\"config:\")\n",
    "for key, value in CLASSIFICATION_CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b168c68-baaf-426d-b81d-7794e3c0a099",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TomatoMAP-Cls Trainer\n",
      "============================================================\n",
      "data founded at: TomatoMAP/TomatoMAP-Cls\n",
      "TomatoMAP-Cls is well structured.\n",
      "\n",
      " training config:\n",
      "   data_dir: TomatoMAP/TomatoMAP-Cls\n",
      "   model_name: mobilenet_v3_large\n",
      "   num_classes: 50\n",
      "   batch_size: 32\n",
      "   num_epochs: 30\n",
      "   learning_rate: 0.0001\n",
      "   target_size: (640, 640)\n",
      "   patience: 3\n",
      "   save_interval: 20\n",
      "\n",
      " training start.\n",
      "Using device: cuda\n",
      "building dataloader: TomatoMAP/TomatoMAP-Cls\n",
      "loading train dataset: 45099 images, 50 classes\n",
      "loading val dataset: 12870 images, 50 classes\n",
      "loading test dataset: 6495 images, 50 classes\n",
      "build model: mobilenet_v3_large, class number: 50\n",
      "parameter info: Total: 4,266,082, Trainable: 4,266,082\n",
      "Training start with 30 epoch(s),\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/30 [Train]: 100%|████████████████| 1410/1410 [06:55<00:00,  3.39it/s, Loss=1.8002, Acc=32.31%]\n",
      "Epoch 1/30 [Val]: 100%|████████████████████| 403/403 [01:00<00:00,  6.67it/s, Loss=1.8581, Acc=40.05%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch 1/30:\n",
      "  Train - Loss: 2.1123, Acc: 32.31%\n",
      "  Val - Loss: 1.6768, Acc: 40.05%\n",
      "  lr: 1.00e-04\n",
      "model saved at: cls/runs/mobilenet_v3_large_cls/best_mobilenet_v3_large.pth\n",
      "  best val acc: 40.05%\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/30 [Train]: 100%|████████████████| 1410/1410 [06:48<00:00,  3.45it/s, Loss=1.9883, Acc=42.30%]\n",
      "Epoch 2/30 [Val]: 100%|████████████████████| 403/403 [01:07<00:00,  5.99it/s, Loss=0.9373, Acc=45.84%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch 2/30:\n",
      "  Train - Loss: 1.6399, Acc: 42.30%\n",
      "  Val - Loss: 1.4930, Acc: 45.84%\n",
      "  lr: 1.00e-04\n",
      "model saved at: cls/runs/mobilenet_v3_large_cls/best_mobilenet_v3_large.pth\n",
      "  best val acc: 45.84%\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/30 [Train]: 100%|████████████████| 1410/1410 [06:52<00:00,  3.42it/s, Loss=1.7607, Acc=47.10%]\n",
      "Epoch 3/30 [Val]: 100%|████████████████████| 403/403 [01:00<00:00,  6.64it/s, Loss=0.9665, Acc=47.71%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch 3/30:\n",
      "  Train - Loss: 1.4578, Acc: 47.10%\n",
      "  Val - Loss: 1.4351, Acc: 47.71%\n",
      "  lr: 1.00e-04\n",
      "model saved at: cls/runs/mobilenet_v3_large_cls/best_mobilenet_v3_large.pth\n",
      "  best val acc: 47.71%\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/30 [Train]: 100%|████████████████| 1410/1410 [06:46<00:00,  3.47it/s, Loss=1.2591, Acc=51.96%]\n",
      "Epoch 4/30 [Val]: 100%|████████████████████| 403/403 [01:06<00:00,  6.06it/s, Loss=1.2630, Acc=53.02%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch 4/30:\n",
      "  Train - Loss: 1.3072, Acc: 51.96%\n",
      "  Val - Loss: 1.2602, Acc: 53.02%\n",
      "  lr: 1.00e-04\n",
      "model saved at: cls/runs/mobilenet_v3_large_cls/best_mobilenet_v3_large.pth\n",
      "  best val acc: 53.02%\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/30 [Train]:  28%|████▊            | 395/1410 [01:58<04:26,  3.80it/s, Loss=1.2057, Acc=55.83%]"
     ]
    }
   ],
   "source": [
    "def train_model(config):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    output_dir = Path(f\"cls/runs/{config['model_name']}_cls\")\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    train_loader, val_loader, test_loader = get_dataloaders(\n",
    "        config['data_dir'], \n",
    "        batch_size=config['batch_size'],\n",
    "        target_size=config['target_size'],\n",
    "        num_workers=8,\n",
    "        include_test=True\n",
    "    )\n",
    "    \n",
    "    model = get_model(config['model_name'], config['num_classes'], pretrained=True)\n",
    "    model = model.to(device)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=config['learning_rate'], weight_decay=0.01)\n",
    "    \n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_accuracies = []\n",
    "    val_accuracies = []\n",
    "    \n",
    "    best_val_acc = 0.0\n",
    "    patience_counter = 0\n",
    "    \n",
    "    print(f\"Training start with {config['num_epochs']} epoch(s),\")\n",
    "    \n",
    "    for epoch in range(config['num_epochs']):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        \n",
    "        train_pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{config['num_epochs']} [Train]\")\n",
    "        \n",
    "        for batch_idx, (images, labels) in enumerate(train_pbar):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            train_total += labels.size(0)\n",
    "            train_correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            current_acc = 100 * train_correct / train_total\n",
    "            train_pbar.set_postfix({\n",
    "                'Loss': f'{loss.item():.4f}',\n",
    "                'Acc': f'{current_acc:.2f}%'\n",
    "            })\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            val_pbar = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{config['num_epochs']} [Val]\")\n",
    "            \n",
    "            for images, labels in val_pbar:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += (predicted == labels).sum().item()\n",
    "                \n",
    "                current_acc = 100 * val_correct / val_total\n",
    "                val_pbar.set_postfix({\n",
    "                    'Loss': f'{loss.item():.4f}',\n",
    "                    'Acc': f'{current_acc:.2f}%'\n",
    "                })\n",
    "        \n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        train_acc = 100 * train_correct / train_total\n",
    "        val_acc = 100 * val_correct / val_total\n",
    "        \n",
    "        train_losses.append(avg_train_loss)\n",
    "        val_losses.append(avg_val_loss)\n",
    "        train_accuracies.append(train_acc)\n",
    "        val_accuracies.append(val_acc)\n",
    "        \n",
    "        scheduler.step()\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        \n",
    "        print(f\"\\n Epoch {epoch+1}/{config['num_epochs']}:\")\n",
    "        print(f\"  Train - Loss: {avg_train_loss:.4f}, Acc: {train_acc:.2f}%\")\n",
    "        print(f\"  Val - Loss: {avg_val_loss:.4f}, Acc: {val_acc:.2f}%\")\n",
    "        print(f\"  lr: {current_lr:.2e}\")\n",
    "        \n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_model_path = output_dir / f\"best_{config['model_name']}.pth\"\n",
    "            save_model(model, best_model_path)\n",
    "            patience_counter = 0\n",
    "            print(f\"  best val acc: {best_val_acc:.2f}%\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f\"  val acc not raised ({patience_counter}/{config['patience']})\")\n",
    "        \n",
    "        if (epoch + 1) % config['save_interval'] == 0:\n",
    "            checkpoint_path = output_dir / f\"checkpoint_epoch_{epoch+1}.pth\"\n",
    "            save_checkpoint(model, optimizer, epoch, checkpoint_path)\n",
    "        \n",
    "        if patience_counter >= config['patience']:\n",
    "            print(f\"\\n Trigger early stop. Val acc has {config['patience']} epochs no improve\")\n",
    "            break\n",
    "        \n",
    "        print(\"-\" * 60)\n",
    "    \n",
    "    final_model_path = output_dir / f\"final_{config['model_name']}.pth\"\n",
    "    save_model(model, final_model_path)\n",
    "    \n",
    "    print(f\"\\n TomatoMAP-Cls is trained!\")\n",
    "    print(f\"  best val acc: {best_val_acc:.2f}%\")\n",
    "    print(f\"  model saved at: {output_dir}\")\n",
    "    \n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.plot(train_losses, label='train loss', color='blue')\n",
    "    plt.plot(val_losses, label='val loss', color='red')\n",
    "    plt.title('training loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.plot(train_accuracies, label='train acc', color='blue')\n",
    "    plt.plot(val_accuracies, label='val acc', color='red')\n",
    "    plt.title('training acc')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.subplot(1, 3, 3)\n",
    "    lrs = []\n",
    "    for i in range(len(train_losses)):\n",
    "        if i < 30:\n",
    "            lrs.append(config['learning_rate'])\n",
    "        elif i < 60:\n",
    "            lrs.append(config['learning_rate'] * 0.1)\n",
    "        else:\n",
    "            lrs.append(config['learning_rate'] * 0.01)\n",
    "    plt.plot(lrs, label='lr', color='green')\n",
    "    plt.title('lr changes')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Learning Rate')\n",
    "    plt.yscale('log')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_dir / 'training_curves.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    history_df = pd.DataFrame({\n",
    "        'epoch': range(1, len(train_losses) + 1),\n",
    "        'train_loss': train_losses,\n",
    "        'val_loss': val_losses,\n",
    "        'train_acc': train_accuracies,\n",
    "        'val_acc': val_accuracies\n",
    "    })\n",
    "    history_df.to_csv(output_dir / 'training_history.csv', index=False)\n",
    "    print(f\" training log saved at: {output_dir / 'training_history.csv'}\")\n",
    "    \n",
    "    return model, best_val_acc, output_dir, test_loader\n",
    "\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"TomatoMAP-Cls Trainer\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if not os.path.exists(CLASSIFICATION_CONFIG['data_dir']):\n",
    "    print(f\"dataset not exist\")\n",
    "    print(f\"   path: {CLASSIFICATION_CONFIG['data_dir']}\")\n",
    "    print(f\"   please check data structure\")\n",
    "else:\n",
    "    print(f\"data founded at: {CLASSIFICATION_CONFIG['data_dir']}\")\n",
    "    \n",
    "    train_dir = os.path.join(CLASSIFICATION_CONFIG['data_dir'], 'train')\n",
    "    val_dir = os.path.join(CLASSIFICATION_CONFIG['data_dir'], 'val')\n",
    "    test_dir = os.path.join(CLASSIFICATION_CONFIG['data_dir'], 'test')\n",
    "    \n",
    "    if not os.path.exists(train_dir):\n",
    "        print(f\"training subset not exist: {train_dir}\")\n",
    "    elif not os.path.exists(val_dir):\n",
    "        print(f\"val subset not exist: {val_dir}\")\n",
    "    elif not os.path.exists(test_dir):\n",
    "        print(f\"test subset not exist: {test_dir}\")\n",
    "        print(f\"   using val subset for test\")\n",
    "    else:\n",
    "        print(f\"TomatoMAP-Cls is well structured.\")\n",
    "        \n",
    "        print(\"\\n training config:\")\n",
    "        for key, value in CLASSIFICATION_CONFIG.items():\n",
    "            print(f\"   {key}: {value}\")\n",
    "        \n",
    "        print(\"\\n training start.\")\n",
    "        \n",
    "        try:\n",
    "            model, best_acc, output_dir, test_loader = train_model(CLASSIFICATION_CONFIG)\n",
    "            \n",
    "            print(\"\\n\" + \"=\" * 60)\n",
    "            print(\"\\n training finished!\")\n",
    "            print(f\"   best val acc is: {best_acc:.2f}%\")\n",
    "            print(f\"   model saved at: {output_dir}\")\n",
    "            \n",
    "            print(\"\\n evaluating on test subset...\")\n",
    "            model.eval()\n",
    "            test_correct = 0\n",
    "            test_total = 0\n",
    "            test_predictions = []\n",
    "            test_labels = []\n",
    "            \n",
    "            device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                test_pbar = tqdm(test_loader, desc=\"evaluating\")\n",
    "                for images, labels in test_pbar:\n",
    "                    images, labels = images.to(device), labels.to(device)\n",
    "                    outputs = model(images)\n",
    "                    _, predicted = torch.max(outputs.data, 1)\n",
    "                    \n",
    "                    test_total += labels.size(0)\n",
    "                    test_correct += (predicted == labels).sum().item()\n",
    "\n",
    "                    test_predictions.extend(predicted.cpu().numpy())\n",
    "                    test_labels.extend(labels.cpu().numpy())\n",
    "                    \n",
    "                    current_acc = 100 * test_correct / test_total\n",
    "                    test_pbar.set_postfix({'Acc': f'{current_acc:.2f}%'})\n",
    "            \n",
    "            test_accuracy = 100 * test_correct / test_total\n",
    "            print(f\" test subset acc: {test_accuracy:.2f}%\")\n",
    "\n",
    "            print(\"\\n building confusion matrix\")\n",
    "            \n",
    "            train_dataset = test_loader.dataset\n",
    "            class_names = train_dataset.classes\n",
    "            \n",
    "            cm = confusion_matrix(test_labels, test_predictions)\n",
    "            \n",
    "            cm_df = pd.DataFrame(cm, index=class_names, columns=class_names)\n",
    "            cm_df.to_csv(output_dir / 'confusion_matrix.csv')\n",
    "\n",
    "            normalized_cm = cm_df.div(cm_df.sum(axis=1), axis=0).fillna(0)\n",
    "            \n",
    "            matrix = normalized_cm.T.to_numpy()\n",
    "            \n",
    "            from matplotlib import rcParams\n",
    "            # rcParams['font.family'] = 'Calibri' # Ubuntu doesn't own this when training on ubuntu VM\n",
    "            rcParams['font.size'] = 8\n",
    "            \n",
    "            masked_matrix = np.ma.masked_where(matrix == 0, matrix)\n",
    "            \n",
    "            from matplotlib.colors import Normalize\n",
    "            cmap = plt.cm.jet\n",
    "            cmap.set_bad(color='white')\n",
    "            norm = Normalize(vmin=0.1, vmax=1)\n",
    "            \n",
    "            fig_width_in = 3.1\n",
    "            fig_height_in = fig_width_in\n",
    "            fig, ax = plt.subplots(figsize=(fig_width_in, fig_height_in))\n",
    "            \n",
    "            im = ax.imshow(masked_matrix, cmap=cmap, norm=norm)\n",
    "\n",
    "            # For further process for publishing purpose, labels are removed :)\n",
    "            ax.set_xlabel(\"\")\n",
    "            ax.set_ylabel(\"\")\n",
    "            \n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.savefig(output_dir / 'normalized_confusion_matrix.png', format='png', dpi=300)\n",
    "            plt.show()\n",
    "            \n",
    "            # plt.figure(figsize=(12, 10))\n",
    "            # disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)\n",
    "            # disp.plot(cmap='Blues', values_format='d')\n",
    "            # plt.title(f'Detailed Confusion Matrix (test acc: {test_accuracy:.2f}%)', fontsize=8)\n",
    "            # plt.xticks(rotation=45, ha='right')\n",
    "            # plt.yticks(rotation=0)\n",
    "            # plt.tight_layout()\n",
    "            # plt.savefig(output_dir / 'detailed_confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
    "            # plt.show()\n",
    "            \n",
    "            test_results = {\n",
    "                'test_accuracy': test_accuracy,\n",
    "                'total_samples': test_total,\n",
    "                'correct_predictions': test_correct,\n",
    "                'num_classes': len(class_names),\n",
    "                'class_names': class_names\n",
    "            }\n",
    "            \n",
    "            import json\n",
    "            with open(output_dir / 'test_results.json', 'w', encoding='utf-8') as f:\n",
    "                json.dump(test_results, f, indent=2, ensure_ascii=False)\n",
    "            \n",
    "            print(\"\\n\" + \"=\" * 60)\n",
    "            print(\" Evaluation results:\")\n",
    "            print(f\"   best val acc: {best_acc:.2f}%\")\n",
    "            print(f\"   test acc: {test_accuracy:.2f}%\")\n",
    "            print(f\"   class number: {len(class_names)}\")\n",
    "            print(f\"   test data size: {test_total}\")\n",
    "            print(f\"   results saved at: {output_dir}\")\n",
    "            print(f\"   GGWP!\")\n",
    "            print(\"=\" * 60)\n",
    "            \n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\n training interruptted\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\n error during training:\")\n",
    "            print(f\"   error info: {str(e)}\")\n",
    "            print(\"\\nDetails:\")\n",
    "            import traceback\n",
    "            traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "270bc778-324d-46cf-aec4-37f9b4cd6cde",
   "metadata": {},
   "source": [
    "# TomatoMAP-Det Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517ad488-17f7-44d5-aa3d-0b23e131d49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "from ultralytics import RTDETR\n",
    "\n",
    "# using proper libiary?\n",
    "import ultralytics\n",
    "ultralytics.checks()\n",
    "print(ultralytics.__file__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3872b57-2251-4d4d-a179-ff0c7d312236",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.use_deterministic_algorithms(False)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TomatoMAP-Det Trainer\")\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "\n",
    "print(\"downloading pretrained model: \")\n",
    "\n",
    "model = YOLO(\"yolo11l.pt\")\n",
    "\n",
    "print(\"model info: \")\n",
    "\n",
    "train_result = model.train(\n",
    "    data=\"det/TomatoMAP-Det.yaml\",\n",
    "    epochs=500,\n",
    "    imgsz=640,\n",
    "    device=[0],\n",
    "    batch=32,\n",
    "    patience=10,\n",
    "    project=\"det/output\",\n",
    "    cfg=\"det/best_hyperparameters.yaml\", # fine-tuned hyperparameters, ready to use, details please contact us per email\n",
    "    #profile=True,\n",
    "    plots=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c9228d-dc78-42af-8b78-7a41685f25b5",
   "metadata": {},
   "source": [
    "# TomatoMAP-Seg Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de9f46fe-5dea-4098-9ac0-20249a648788",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import json\n",
    "import yaml\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from collections import OrderedDict\n",
    "\n",
    "# Detectron2\n",
    "import detectron2\n",
    "from detectron2.engine import DefaultTrainer, DefaultPredictor, HookBase\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.data import MetadataCatalog, build_detection_test_loader, build_detection_train_loader\n",
    "from detectron2.data.datasets import register_coco_instances\n",
    "from detectron2.evaluation import COCOEvaluator, inference_on_dataset\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "from detectron2.utils.events import get_event_storage\n",
    "from detectron2.utils.logger import setup_logger\n",
    "\n",
    "setup_logger()\n",
    "\n",
    "print(f\"  Detectron2 version: {detectron2.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce474b7-4d66-4a5a-957b-475a14a237b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_segmentation(points):\n",
    "    #format points [[x,y],[x,y]] to [x1,y1,x2,y2,...]\n",
    "    return [coord for pair in points for coord in pair]\n",
    "\n",
    "def load_categories_from_yaml(yaml_path):\n",
    "    with open(yaml_path, 'r', encoding='utf-8') as f:\n",
    "        data = yaml.safe_load(f)\n",
    "    categories = []\n",
    "    cat_map = {}\n",
    "    cat_id = 1\n",
    "    for item in data['label']:\n",
    "        name = item['name']\n",
    "        if name == '__background__':\n",
    "            continue\n",
    "        categories.append({\n",
    "            \"id\": cat_id,\n",
    "            \"name\": name,\n",
    "            \"supercategory\": \"none\"\n",
    "        })\n",
    "        cat_map[name] = cat_id\n",
    "        cat_id += 1\n",
    "    return categories, cat_map\n",
    "\n",
    "def convert_isat_folder_to_coco(task_dir, label_dir, yaml_path, output_dir, train_ratio=0.7, val_ratio=0.2):\n",
    "    print(\"ISAT2COCO...\")\n",
    "    \n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    categories, category_map = load_categories_from_yaml(yaml_path)\n",
    "    print(f\"loaded {len(categories)} classes\")\n",
    "\n",
    "    if not os.path.exists(task_dir):\n",
    "        print(f\"image folder not exist: {task_dir}\")\n",
    "        return False\n",
    "    \n",
    "    if not os.path.exists(label_dir):\n",
    "        print(f\"label folder not exist: {label_dir}\")\n",
    "        return False\n",
    "\n",
    "    images = [f for f in os.listdir(task_dir) if f.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp'))]\n",
    "    json_map = {os.path.splitext(f)[0]: f for f in os.listdir(label_dir) if f.endswith(\".json\")}\n",
    "\n",
    "    print(f\"found {len(images)} images\")\n",
    "    print(f\"found {len(json_map)} labels\")\n",
    "\n",
    "    # matching image and labels\n",
    "    dataset = []\n",
    "    unmatched_images = []\n",
    "    \n",
    "    for img_name in tqdm(images, desc=\"matching image and labels\"):\n",
    "        base = os.path.splitext(img_name)[0]\n",
    "        if base in json_map:\n",
    "            dataset.append({\n",
    "                \"img_file\": img_name,\n",
    "                \"json_file\": json_map[base]\n",
    "            })\n",
    "        else:\n",
    "            unmatched_images.append(img_name)\n",
    "\n",
    "    print(f\"successfully matches {len(dataset)} pairs\")\n",
    "    if unmatched_images:\n",
    "        print(f\" {len(unmatched_images)} unmatches images\")\n",
    "\n",
    "    if len(dataset) == 0:\n",
    "        print(\"no matched pairs\")\n",
    "        return False\n",
    "\n",
    "    random.shuffle(dataset)\n",
    "    total = len(dataset)\n",
    "    train_end = int(total * train_ratio)\n",
    "    val_end = int(total * (train_ratio + val_ratio))\n",
    "\n",
    "    splits = {\n",
    "        \"train\": dataset[:train_end],\n",
    "        \"val\": dataset[train_end:val_end],\n",
    "        \"test\": dataset[val_end:]\n",
    "    }\n",
    "\n",
    "    print(f\"\\n spilting dataset:\")\n",
    "    for split_name, split_data in splits.items():\n",
    "        print(f\"  {split_name}: {len(split_data)} iamges ({len(split_data)/total*100:.1f}%)\")\n",
    "\n",
    "    conversion_stats = {}\n",
    "    \n",
    "    for split_name, split_data in splits.items():\n",
    "        if len(split_data) == 0:\n",
    "            print(f\" {split_name} dataset empty, skipping\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"\\n transforming {split_name} dataset.\")\n",
    "        \n",
    "        coco = {\n",
    "            \"images\": [],\n",
    "            \"annotations\": [],\n",
    "            \"categories\": categories\n",
    "        }\n",
    "        ann_id = 1\n",
    "        img_id = 1\n",
    "        \n",
    "        processed_annotations = 0\n",
    "        skipped_annotations = 0\n",
    "\n",
    "        for item in tqdm(split_data, desc=f\"处理{split_name}\"):\n",
    "            img_path = os.path.join(task_dir, item[\"img_file\"])\n",
    "            json_path = os.path.join(label_dir, item[\"json_file\"])\n",
    "\n",
    "            if not os.path.exists(json_path):\n",
    "                print(f\" label files not exist: {json_path}\")\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                with open(json_path, 'r', encoding='utf-8') as f:\n",
    "                    isat = json.load(f)\n",
    "            except Exception as e:\n",
    "                print(f\"failed to load label file {json_path}: {e}\")\n",
    "                continue\n",
    "\n",
    "            info = isat['info']\n",
    "            coco[\"images\"].append({\n",
    "                \"file_name\": item[\"img_file\"],\n",
    "                \"id\": img_id,\n",
    "                \"width\": info[\"width\"],\n",
    "                \"height\": info[\"height\"]\n",
    "            })\n",
    "\n",
    "            for obj in isat.get('objects', []):\n",
    "                cat = obj['category']\n",
    "                if cat not in category_map:\n",
    "                    skipped_annotations += 1\n",
    "                    continue\n",
    "\n",
    "                seg_flat = flatten_segmentation(obj[\"segmentation\"])\n",
    "                if len(seg_flat) < 6:\n",
    "                    skipped_annotations += 1\n",
    "                    continue\n",
    "\n",
    "                coco[\"annotations\"].append({\n",
    "                    \"id\": ann_id,\n",
    "                    \"image_id\": img_id,\n",
    "                    \"category_id\": category_map[cat],\n",
    "                    \"segmentation\": [seg_flat],\n",
    "                    \"bbox\": obj[\"bbox\"],\n",
    "                    \"area\": obj[\"area\"],\n",
    "                    \"iscrowd\": obj.get(\"iscrowd\", 0),\n",
    "                    \"group_id\": obj.get(\"group\", None)\n",
    "                })\n",
    "                ann_id += 1\n",
    "                processed_annotations += 1\n",
    "                \n",
    "            img_id += 1\n",
    "\n",
    "        output_file = os.path.join(output_dir, f\"{split_name}.json\")\n",
    "        with open(output_file, \"w\", encoding='utf-8') as f:\n",
    "            json.dump(coco, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "        conversion_stats[split_name] = {\n",
    "            'images': len(split_data),\n",
    "            'annotations': processed_annotations,\n",
    "            'skipped': skipped_annotations\n",
    "        }\n",
    "\n",
    "        print(f\"  generated {split_name}.json\")\n",
    "        print(f\"     images: {len(split_data)}\")\n",
    "        print(f\"     labels: {processed_annotations}\")\n",
    "        print(f\"     skipped: {skipped_annotations}\")\n",
    "\n",
    "    print(f\"\\n ISAT2COCO finished\")\n",
    "    print(f\" output at: {output_dir}\")\n",
    "    print(f\" transform info:\")\n",
    "    \n",
    "    total_images = sum(stats['images'] for stats in conversion_stats.values())\n",
    "    total_annotations = sum(stats['annotations'] for stats in conversion_stats.values())\n",
    "    total_skipped = sum(stats['skipped'] for stats in conversion_stats.values())\n",
    "    \n",
    "    print(f\"  total images: {total_images}\")\n",
    "    print(f\"  total labels: {total_annotations}\")\n",
    "    print(f\"  skipped labels: {total_skipped}\")\n",
    "    \n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15c7807-6686-49b8-8c2b-c9fc5a960936",
   "metadata": {},
   "outputs": [],
   "source": [
    "ISAT_CONFIG = {\n",
    "    'task_dir': \"TomatoMAP/TomatoMAP-Seg/images\",\n",
    "    'label_dir': \"TomatoMAP/TomatoMAP-Seg/labels\",\n",
    "    'yaml_path': \"TomatoMAP/TomatoMAP-Seg/labels/isat.yaml\",\n",
    "    'output_dir': \"TomatoMAP/TomatoMAP-Seg/cocoOut\",\n",
    "    'train_ratio': 0.7,\n",
    "    'val_ratio': 0.2,    # rest 0.1 is test\n",
    "    'auto_convert': True\n",
    "}\n",
    "\n",
    "DATASET_CONFIG = {\n",
    "    'dataset_root': \"TomatoMAP/TomatoMAP-Seg/\",\n",
    "    'img_dir': \"TomatoMAP/TomatoMAP-Seg/images\",\n",
    "    'coco_ann_dir': \"TomatoMAP/TomatoMAP-Seg/cocoOut\",\n",
    "    'isat_yaml_path': \"TomatoMAP/TomatoMAP-Seg/labels/isat.yaml\",\n",
    "    'output_dir': \"TomatoMAP/TomatoMAP-Seg/output\",\n",
    "    'num_classes': 10,    # without background\n",
    "}\n",
    "\n",
    "TRAINING_CONFIG = {\n",
    "    'model_name': \"mask_rcnn_R_50_FPN_3x\",\n",
    "    'batch_size': 4,\n",
    "    'base_lr': 0.00024,\n",
    "    'max_epochs': 100,\n",
    "    'patience': 15,\n",
    "    'num_workers': 8,  # Windows user please set to 0\n",
    "    'score_thresh_test': 0.3,\n",
    "    'input_min_size_train': (640, 672, 704, 736, 768, 800),  # 多尺度训练\n",
    "    'input_max_size_train': 1333,\n",
    "    'checkpoint_period': 10,\n",
    "    'eval_period': 10,\n",
    "}\n",
    "\n",
    "print(\"Configurations:\")\n",
    "print(\"ISAT converter config:\")\n",
    "for key, value in ISAT_CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "print(\"\\n dataset config:\")\n",
    "for key, value in DATASET_CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "print(\"\\n training config:\")\n",
    "for key, value in TRAINING_CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7b82da-fe47-4535-b599-7fe5551181a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ISAT coverting to COCO format\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "need_conversion = ISAT_CONFIG['auto_convert']\n",
    "\n",
    "coco_files_exist = all(\n",
    "    os.path.exists(os.path.join(ISAT_CONFIG['output_dir'], f\"{split}.json\"))\n",
    "    for split in ['train', 'val', 'test']\n",
    ")\n",
    "\n",
    "if coco_files_exist and not need_conversion:\n",
    "    print(\"coco format exist, skipping!\")\n",
    "    print(\"   if wanna reconvert, set ISAT_CONFIG['auto_convert'] = True\")\n",
    "else:\n",
    "    required_isat_paths = [\n",
    "        ISAT_CONFIG['task_dir'],\n",
    "        ISAT_CONFIG['label_dir'], \n",
    "        ISAT_CONFIG['yaml_path']\n",
    "    ]\n",
    "    \n",
    "    missing_paths = [path for path in required_isat_paths if not os.path.exists(path)]\n",
    "    \n",
    "    if missing_paths:\n",
    "        print(\"following ISAT path not exist:\")\n",
    "        for path in missing_paths:\n",
    "            print(f\"   {path}\")\n",
    "        print(\"\\nplease check ISAT_CONFIG path setting\")\n",
    "        conversion_success = False\n",
    "    else:\n",
    "        print(\"ISAT checked, start converting...\")\n",
    "        \n",
    "        conversion_success = convert_isat_folder_to_coco(\n",
    "            task_dir=ISAT_CONFIG['task_dir'],\n",
    "            label_dir=ISAT_CONFIG['label_dir'],\n",
    "            yaml_path=ISAT_CONFIG['yaml_path'],\n",
    "            output_dir=ISAT_CONFIG['output_dir'],\n",
    "            train_ratio=ISAT_CONFIG['train_ratio'],\n",
    "            val_ratio=ISAT_CONFIG['val_ratio']\n",
    "        )\n",
    "\n",
    "if 'conversion_success' not in locals():\n",
    "    conversion_success = True\n",
    "\n",
    "if conversion_success:\n",
    "    DATASET_CONFIG['coco_ann_dir'] = ISAT_CONFIG['output_dir']\n",
    "    DATASET_CONFIG['img_dir'] = ISAT_CONFIG['task_dir'] \n",
    "    DATASET_CONFIG['isat_yaml_path'] = ISAT_CONFIG['yaml_path']\n",
    "    print(f\"\\n Configuration of dataset is updated:\")\n",
    "    print(f\"   image path: {DATASET_CONFIG['img_dir']}\")\n",
    "    print(f\"   label path: {DATASET_CONFIG['coco_ann_dir']}\")\n",
    "else:\n",
    "    print(\"\\n Transfer failed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e72146-ee66-4937-a2c8-6949de40cc4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_dataset_areas():\n",
    "    print(f\"\\n analyze dataset object areas...\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    for split in ['train', 'val', 'test']:\n",
    "        ann_file = os.path.join(DATASET_CONFIG['coco_ann_dir'], f\"{split}.json\")\n",
    "        if not os.path.exists(ann_file):\n",
    "            print(f\"label file {ann_file} not exist\")\n",
    "            continue\n",
    "            \n",
    "        with open(ann_file, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        image_info = {img['id']: img for img in data['images']}\n",
    "        \n",
    "        areas_original = []\n",
    "        areas_scaled = []\n",
    "        \n",
    "        min_size = min(TRAINING_CONFIG['input_min_size_train'])\n",
    "        max_size = TRAINING_CONFIG['input_max_size_train']\n",
    "        \n",
    "        for ann in data['annotations']:\n",
    "            if 'area' in ann:\n",
    "                area = ann['area']\n",
    "            else:\n",
    "                bbox = ann.get('bbox', [0, 0, 0, 0])\n",
    "                area = bbox[2] * bbox[3]\n",
    "            areas_original.append(area)\n",
    "            \n",
    "            img_id = ann['image_id']\n",
    "            if img_id in image_info:\n",
    "                img = image_info[img_id]\n",
    "                orig_w, orig_h = img['width'], img['height']\n",
    "                \n",
    "                size = max(orig_w, orig_h)\n",
    "                if size > max_size:\n",
    "                    scale = max_size / size\n",
    "                else:\n",
    "                    scale = min_size / min(orig_w, orig_h)\n",
    "                    if scale * size > max_size:\n",
    "                        scale = max_size / size\n",
    "                \n",
    "                scaled_area = area * (scale ** 2)\n",
    "                areas_scaled.append(scaled_area)\n",
    "        \n",
    "        areas_original = np.array(areas_original)\n",
    "        areas_scaled = np.array(areas_scaled) if areas_scaled else areas_original\n",
    "        \n",
    "        print(f\"\\n{split.upper()} dataset analysis:\")\n",
    "        print(f\"-\" * 40)\n",
    "        \n",
    "        if len(data['images']) > 0:\n",
    "            avg_width = np.mean([img['width'] for img in data['images']])\n",
    "            avg_height = np.mean([img['height'] for img in data['images']])\n",
    "            print(f\"average size: {avg_width:.0f} x {avg_height:.0f}\")\n",
    "        \n",
    "        print(f\"total object amount: {len(areas_original)}\")\n",
    "        \n",
    "        print(f\"\\n original object image size distribution:\")\n",
    "        small_orig = np.sum(areas_original < 32**2)\n",
    "        medium_orig = np.sum((areas_original >= 32**2) & (areas_original < 96**2))\n",
    "        large_orig = np.sum(areas_original >= 96**2)\n",
    "        \n",
    "        print(f\"  small object (<32²): {small_orig} ({small_orig/len(areas_original)*100:.1f}%)\")\n",
    "        print(f\"  mid object (32²-96²): {medium_orig} ({medium_orig/len(areas_original)*100:.1f}%)\")\n",
    "        print(f\"  big object (>96²): {large_orig} ({large_orig/len(areas_original)*100:.1f}%)\")\n",
    "        print(f\"  min area: {np.min(areas_original):.0f} pixel²\")\n",
    "        print(f\"  max area: {np.max(areas_original):.0f} pixel²\")\n",
    "        print(f\"  mean area: {np.mean(areas_original):.0f} pixel²\")\n",
    "        \n",
    "        print(f\"\\nscaled to {min_size}-{max_size} :\")\n",
    "        small_scaled = np.sum(areas_scaled < 32**2)\n",
    "        medium_scaled = np.sum((areas_scaled >= 32**2) & (areas_scaled < 96**2))\n",
    "        large_scaled = np.sum(areas_scaled >= 96**2)\n",
    "        \n",
    "        print(f\"  small object (<32²): {small_scaled} ({small_scaled/len(areas_scaled)*100:.1f}%)\")\n",
    "        print(f\"  mid object (32²-96²): {medium_scaled} ({medium_scaled/len(areas_scaled)*100:.1f}%)\")\n",
    "        print(f\"  big object (>96²): {large_scaled} ({large_scaled/len(areas_scaled)*100:.1f}%)\")\n",
    "        \n",
    "        if small_scaled == 0:\n",
    "            print(f\"\\n after scale, no small object - APs set to -1\")\n",
    "        if medium_scaled == 0:\n",
    "            print(f\" after scale, no mid object - APm set to -1\")\n",
    "        if large_scaled == 0:\n",
    "            print(f\" after scale, no big object - APl set to -1\")\n",
    "\n",
    "def get_dataset_info():\n",
    "    print(f\"\\n TomatoMAP-Seg info:\")\n",
    "    print(f\"{'='*40}\")\n",
    "    \n",
    "    for split in ['train', 'val', 'test']:\n",
    "        ann_file = os.path.join(DATASET_CONFIG['coco_ann_dir'], f\"{split}.json\")\n",
    "        if os.path.exists(ann_file):\n",
    "            with open(ann_file, 'r') as f:\n",
    "                data = json.load(f)\n",
    "            print(f\"{split}: {len(data['images'])} images, {len(data['annotations'])} labels\")\n",
    "    \n",
    "    return True\n",
    "\n",
    "if conversion_success:\n",
    "    get_dataset_info()\n",
    "    analyze_dataset_areas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ca5eb8-9ece-4fdc-8240-54e8df7b7f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BestModelHook(HookBase):\n",
    "    # hook to save the best model based on validation segmentation mAP\n",
    "    \n",
    "    def __init__(self, cfg, eval_period, patience=10):\n",
    "        self.cfg = cfg.clone()\n",
    "        self.eval_period = eval_period\n",
    "        self.patience = patience\n",
    "        self.best_score = 0  # 使用0而不是-1\n",
    "        self.best_metric_name = None\n",
    "        self.best_epoch = -1\n",
    "        self.epochs_without_improvement = 0\n",
    "        self.should_stop = False\n",
    "        self.history = []  # 记录历史\n",
    "        \n",
    "    def get_valid_score(self, segm_results):\n",
    "        priority_metrics = [\"AP\", \"AP50\", \"AP75\", \"APm\", \"APl\"]\n",
    "        \n",
    "        for metric in priority_metrics:\n",
    "            value = segm_results.get(metric, -1)\n",
    "            if value != -1:\n",
    "                return metric, value\n",
    "        \n",
    "        return None, None\n",
    "    \n",
    "    def after_step(self):\n",
    "        next_iter = self.trainer.iter + 1\n",
    "        is_final_iter = next_iter == self.trainer.max_iter\n",
    "        \n",
    "        if (next_iter % self.eval_period == 0 and not is_final_iter):\n",
    "            current_epoch = (next_iter // self.eval_period)\n",
    "            \n",
    "            results = self._do_eval()\n",
    "            if results is None:\n",
    "                print(f\"Epoch {current_epoch}: evaluate failed\")\n",
    "                return\n",
    "            \n",
    "            segm_results = results.get(\"segm\", {})\n",
    "            bbox_results = results.get(\"bbox\", {})\n",
    "            \n",
    "            metric_name, current_score = self.get_valid_score(segm_results)\n",
    "            \n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(f\"Epoch {current_epoch} evaluate result:\")\n",
    "            print(f\"{'='*60}\")\n",
    "            \n",
    "            print(\"\\n bbox metrics:\")\n",
    "            for key in [\"AP\", \"AP50\", \"AP75\", \"APs\", \"APm\", \"APl\"]:\n",
    "                value = bbox_results.get(key, -1)\n",
    "                if value != -1:\n",
    "                    print(f\"  {key}: {value:.4f} ✓\")\n",
    "                else:\n",
    "                    print(f\"  {key}: N/A\")\n",
    "            \n",
    "            print(\"\\n seg metrics:\")\n",
    "            for key in [\"AP\", \"AP50\", \"AP75\", \"APs\", \"APm\", \"APl\"]:\n",
    "                value = segm_results.get(key, -1)\n",
    "                if value != -1:\n",
    "                    print(f\"  {key}: {value:.4f} ✓\")\n",
    "                else:\n",
    "                    print(f\"  {key}: N/A (no object for this class)\")\n",
    "            \n",
    "            if metric_name is None:\n",
    "                print(\"\\n waring! no useful metrics\")\n",
    "                print(\"please check TomatoMAP-Seg structure\")\n",
    "\n",
    "                metric_name, current_score = self.get_valid_score(bbox_results)\n",
    "                if metric_name is not None:\n",
    "                    print(f\"using bbox metric: {metric_name} = {current_score:.4f}\")\n",
    "                else:\n",
    "                    return\n",
    "            \n",
    "            print(f\"\\n main metrics: {metric_name} = {current_score:.4f}\")\n",
    "            \n",
    "            self.history.append({\n",
    "                'epoch': current_epoch,\n",
    "                'metric': metric_name,\n",
    "                'score': current_score,\n",
    "                'all_metrics': {**segm_results, **{'bbox_' + k: v for k, v in bbox_results.items()}}\n",
    "            })\n",
    "            \n",
    "            if current_score > self.best_score:\n",
    "                improvement = current_score - self.best_score\n",
    "                self.best_score = current_score\n",
    "                self.best_metric_name = metric_name\n",
    "                self.best_epoch = current_epoch\n",
    "                self.epochs_without_improvement = 0\n",
    "                \n",
    "                self.trainer.checkpointer.save(\"model_best\")\n",
    "                print(f\"\\n best model saved\")\n",
    "                print(f\"   score: {current_score:.4f} (↑{improvement:.4f})\")\n",
    "                \n",
    "                best_results_file = os.path.join(self.cfg.OUTPUT_DIR, \"best_results.json\")\n",
    "                with open(best_results_file, 'w') as f:\n",
    "                    json.dump({\n",
    "                        'epoch': current_epoch,\n",
    "                        'metric': metric_name,\n",
    "                        'score': current_score,\n",
    "                        'segm_results': segm_results,\n",
    "                        'bbox_results': bbox_results\n",
    "                    }, f, indent=2)\n",
    "            else:\n",
    "                self.epochs_without_improvement += 1\n",
    "                gap = self.best_score - current_score\n",
    "                print(f\"\\ncurrent: {current_score:.4f} | best: {self.best_score:.4f} (gap: {gap:.4f})\")\n",
    "                print(f\"continuted {self.epochs_without_improvement}/{self.patience} epoch no improve\")\n",
    "            \n",
    "            if self.epochs_without_improvement >= self.patience:\n",
    "                print(f\"\\n{'='*60}\")\n",
    "                print(f\"early stop triggered\")\n",
    "                print(f\"   best {self.best_metric_name}: {self.best_score:.4f} (epoch {self.best_epoch})\")\n",
    "                print(f\"   total epochs: {current_epoch}\")\n",
    "                print(f\"{'='*60}\")\n",
    "                self.should_stop = True\n",
    "\n",
    "                self.trainer.storage._iter = self.trainer.max_iter\n",
    "    \n",
    "    def _do_eval(self):\n",
    "\n",
    "        try:\n",
    "            evaluator = COCOEvaluator(\"tomato_val\", self.cfg, False, \n",
    "                                    output_dir=os.path.join(self.cfg.OUTPUT_DIR, \"inference\"))\n",
    "            val_loader = build_detection_test_loader(self.cfg, \"tomato_val\")\n",
    "            results = inference_on_dataset(self.trainer.model, val_loader, evaluator)\n",
    "            return results\n",
    "        except Exception as e:\n",
    "            print(f\"evaluate failed: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            return None\n",
    "\n",
    "class MyTrainer(DefaultTrainer):\n",
    "    \n",
    "    def __init__(self, cfg, patience=None):\n",
    "        self.patience = patience if patience is not None else TRAINING_CONFIG['patience']\n",
    "        super().__init__(cfg)\n",
    "    \n",
    "    @classmethod\n",
    "    def build_evaluator(cls, cfg, dataset_name, output_folder=None):\n",
    "        if output_folder is None:\n",
    "            output_folder = os.path.join(cfg.OUTPUT_DIR, \"inference\")\n",
    "        return COCOEvaluator(\n",
    "            dataset_name=dataset_name,\n",
    "            distributed=False,\n",
    "            output_dir=output_folder,\n",
    "            use_fast_impl=True,\n",
    "            tasks=(\"bbox\", \"segm\"),\n",
    "        )\n",
    "    \n",
    "    def build_hooks(self):\n",
    "        hooks = super().build_hooks()\n",
    "        \n",
    "        try:\n",
    "            train_loader = build_detection_train_loader(self.cfg)\n",
    "            iters_per_epoch = len(train_loader) // self.cfg.SOLVER.IMS_PER_BATCH\n",
    "            print(f\"iters per epoch: {iters_per_epoch}\")\n",
    "        except:\n",
    "            iters_per_epoch = 106\n",
    "            print(f\"iters per epoch: {iters_per_epoch}\")\n",
    "        \n",
    "        eval_period = iters_per_epoch * TRAINING_CONFIG['eval_period']\n",
    "        \n",
    "        best_model_hook = BestModelHook(self.cfg, eval_period, self.patience)\n",
    "        hooks.append(best_model_hook)\n",
    "        \n",
    "        self.best_model_hook = best_model_hook\n",
    "        \n",
    "        return hooks\n",
    "    \n",
    "    def run_step(self):\n",
    "        super().run_step()\n",
    "        \n",
    "        if hasattr(self, 'best_model_hook') and self.best_model_hook.should_stop:\n",
    "            print(\"early stop triggered, training stop\")\n",
    "            self.storage._iter = self.max_iter\n",
    "    \n",
    "    def train(self):\n",
    "        super().train()\n",
    "        \n",
    "        if hasattr(self, 'best_model_hook'):\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(f\"training info:\")\n",
    "            print(f\"{'='*60}\")\n",
    "            if self.best_model_hook.best_score > 0:\n",
    "                print(f\"best {self.best_model_hook.best_metric_name}: {self.best_model_hook.best_score:.4f}\")\n",
    "                print(f\"best epoch: {self.best_model_hook.best_epoch}\")\n",
    "                print(f\"best model saved as: model_best.pth\")\n",
    "            else:\n",
    "                print(\"no metrics found for training\")\n",
    "            \n",
    "            history_file = os.path.join(self.cfg.OUTPUT_DIR, \"training_history.json\")\n",
    "            with open(history_file, 'w') as f:\n",
    "                json.dump(self.best_model_hook.history, f, indent=2)\n",
    "            print(f\"training log saved at: {history_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f48af04-6a2d-4ee7-8647-53efb20c376f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def register_all_datasets():\n",
    "    print(\"register dataset\")\n",
    "    \n",
    "    try:\n",
    "        with open(DATASET_CONFIG['isat_yaml_path'], 'r', encoding='utf-8') as f:\n",
    "            data = yaml.safe_load(f)\n",
    "        labels = [item['name'] for item in data['label'] if item['name'] != '__background__']\n",
    "        print(f\"loading label classes: {len(labels)} classes\")\n",
    "        for i, label in enumerate(labels):\n",
    "            print(f\"  {i}: {label}\")\n",
    "    except Exception as e:\n",
    "        print(f\"class label loading failed: {e}\")\n",
    "        return None\n",
    "    \n",
    "    datasets = ['train', 'val', 'test']\n",
    "    \n",
    "    for dataset_name in datasets:\n",
    "        dataset_key = f\"tomato_{dataset_name}\"\n",
    "        \n",
    "        try:\n",
    "            from detectron2.data.datasets.coco import _PREDEFINED_SPLITS_COCO\n",
    "            if dataset_key in _PREDEFINED_SPLITS_COCO:\n",
    "                del _PREDEFINED_SPLITS_COCO[dataset_key]\n",
    "        except ImportError:\n",
    "            try:\n",
    "                from detectron2.data.datasets.builtin import _PREDEFINED_SPLITS_COCO\n",
    "                if dataset_key in _PREDEFINED_SPLITS_COCO:\n",
    "                    del _PREDEFINED_SPLITS_COCO[dataset_key]\n",
    "            except ImportError:\n",
    "                try:\n",
    "                    from detectron2.data.datasets.register_coco import _PREDEFINED_SPLITS_COCO\n",
    "                    if dataset_key in _PREDEFINED_SPLITS_COCO:\n",
    "                        del _PREDEFINED_SPLITS_COCO[dataset_key]\n",
    "                except ImportError:\n",
    "                    print(f\"  can't clean {dataset_key} version cap)\")\n",
    "        \n",
    "        try:\n",
    "            if MetadataCatalog.has(dataset_key):\n",
    "                MetadataCatalog.remove(dataset_key)\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    for dataset_name in datasets:\n",
    "        coco_json = os.path.join(DATASET_CONFIG['coco_ann_dir'], f\"{dataset_name}.json\")\n",
    "        dataset_key = f\"tomato_{dataset_name}\"\n",
    "        \n",
    "        if os.path.exists(coco_json):\n",
    "            abs_coco_json = os.path.abspath(coco_json)\n",
    "            abs_img_dir = os.path.abspath(DATASET_CONFIG['img_dir'])\n",
    "            \n",
    "            try:\n",
    "                register_coco_instances(\n",
    "                    dataset_key, \n",
    "                    {}, \n",
    "                    abs_coco_json, \n",
    "                    abs_img_dir\n",
    "                )\n",
    "                MetadataCatalog.get(dataset_key).thing_classes = labels\n",
    "                print(f\"  registered {dataset_key}\")\n",
    "            except Exception as e:\n",
    "                print(f\"  register failed: {e}\")\n",
    "                try:\n",
    "                    MetadataCatalog.get(dataset_key).thing_classes = labels\n",
    "                    print(f\"  re-setting {dataset_key} meta data\")\n",
    "                except Exception as e2:\n",
    "                    print(f\"  meta setting failed: {e2}\")\n",
    "        else:\n",
    "            print(f\"  can't find {coco_json}\")\n",
    "    \n",
    "    return labels\n",
    "\n",
    "def build_cfg():\n",
    "    cfg = get_cfg()\n",
    "    \n",
    "    model_config_file = f\"COCO-InstanceSegmentation/{TRAINING_CONFIG['model_name']}.yaml\"\n",
    "    cfg.merge_from_file(model_zoo.get_config_file(model_config_file))\n",
    "    \n",
    "    cfg.DATASETS.TRAIN = (\"tomato_train\",)\n",
    "    cfg.DATASETS.TEST = (\"tomato_val\",)\n",
    "    \n",
    "    cfg.DATALOADER.NUM_WORKERS = TRAINING_CONFIG['num_workers']\n",
    "    \n",
    "    cfg.MODEL.ROI_HEADS.NUM_CLASSES = DATASET_CONFIG['num_classes']\n",
    "    cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(model_config_file)\n",
    "    \n",
    "    cfg.SOLVER.IMS_PER_BATCH = TRAINING_CONFIG['batch_size']\n",
    "    cfg.SOLVER.BASE_LR = TRAINING_CONFIG['base_lr']\n",
    "    \n",
    "    estimated_iters_per_epoch = 106\n",
    "    cfg.SOLVER.MAX_ITER = estimated_iters_per_epoch * TRAINING_CONFIG['max_epochs']\n",
    "    \n",
    "    cfg.SOLVER.STEPS = (int(cfg.SOLVER.MAX_ITER * 0.7), int(cfg.SOLVER.MAX_ITER * 0.9))\n",
    "    cfg.SOLVER.GAMMA = 0.1\n",
    "    \n",
    "    cfg.INPUT.MIN_SIZE_TRAIN = TRAINING_CONFIG['input_min_size_train']\n",
    "    cfg.INPUT.MAX_SIZE_TRAIN = TRAINING_CONFIG['input_max_size_train']\n",
    "    \n",
    "    cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = TRAINING_CONFIG['score_thresh_test']\n",
    "    \n",
    "    cfg.OUTPUT_DIR = DATASET_CONFIG['output_dir']\n",
    "    os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
    "    \n",
    "    cfg.MODEL.DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    \n",
    "    cfg.SOLVER.CHECKPOINT_PERIOD = estimated_iters_per_epoch * TRAINING_CONFIG['checkpoint_period']\n",
    "    \n",
    "    return cfg\n",
    "\n",
    "if conversion_success:\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"TomatoMAP-Seg Registeration\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    class_labels = register_all_datasets()\n",
    "    \n",
    "    if class_labels is not None:\n",
    "        print(\"TomatoMAP-Seg is registered\")\n",
    "        \n",
    "        cfg = build_cfg()\n",
    "        print(\"building configed\")\n",
    "        \n",
    "        print(f\"Configuration:\")\n",
    "        print(f\"  model: {TRAINING_CONFIG['model_name']}\")\n",
    "        print(f\"  class num: {DATASET_CONFIG['num_classes']}\")\n",
    "        print(f\"  batch size: {TRAINING_CONFIG['batch_size']}\")\n",
    "        print(f\"  lr: {TRAINING_CONFIG['base_lr']}\")\n",
    "        print(f\"  max epoch: {TRAINING_CONFIG['max_epochs']}\")\n",
    "        print(f\"  patience: {TRAINING_CONFIG['patience']}\")\n",
    "        print(f\"  imput size: {TRAINING_CONFIG['input_min_size_train'][0]}-{TRAINING_CONFIG['input_max_size_train']}\")\n",
    "        print(f\"  output path: {cfg.OUTPUT_DIR}\")\n",
    "        print(f\"  device: {cfg.MODEL.DEVICE}\")\n",
    "    else:\n",
    "        print(\"data registeration failed\")\n",
    "        conversion_success = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c32a6e5-3c8b-4dcc-8330-90af90c680ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model():\n",
    "    print(\"Training TomatoMAP-Seg\")\n",
    "    \n",
    "    trainer = MyTrainer(cfg, patience=TRAINING_CONFIG['patience'])\n",
    "    trainer.resume_or_load(resume=False)\n",
    "    \n",
    "    print(f\"\\n training configuration:\")\n",
    "    print(f\"  model: {TRAINING_CONFIG['model_name']}\")\n",
    "    print(f\"  max epoch: {TRAINING_CONFIG['max_epochs']}\")\n",
    "    print(f\"  patience: {TRAINING_CONFIG['patience']} epochs\")\n",
    "    print(f\"  eval period: per {TRAINING_CONFIG['eval_period']} epochs\")\n",
    "    print(f\"  save check point: per {TRAINING_CONFIG['checkpoint_period']} epochs\")\n",
    "    print(f\"  multi scale training: {TRAINING_CONFIG['input_min_size_train'][0]}-{TRAINING_CONFIG['input_max_size_train']}\")\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"training start\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    try:\n",
    "        trainer.train()\n",
    "        \n",
    "        print(\"\\n training finished\")\n",
    "        \n",
    "        config_path = os.path.join(cfg.OUTPUT_DIR, \"config.yaml\")\n",
    "        with open(config_path, \"w\") as f:\n",
    "            f.write(cfg.dump())\n",
    "        print(f\"config saved: {config_path}\")\n",
    "        \n",
    "        return trainer, cfg\n",
    "        \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n training interrupted\")\n",
    "        return None, cfg\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n error occurs:\")\n",
    "        print(f\"   error info: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None, cfg\n",
    "\n",
    "if 'class_labels' in locals() and class_labels is not None and conversion_success:\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"ready to start training\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    trainer, cfg = train_model()\n",
    "    \n",
    "    if trainer is not None:\n",
    "        print(\"\\n training finished\")\n",
    "        \n",
    "        print(f\"\\n output path:\")\n",
    "        output_dir = Path(cfg.OUTPUT_DIR)\n",
    "        if output_dir.exists():\n",
    "            for file in output_dir.iterdir():\n",
    "                if file.is_file():\n",
    "                    print(f\"  📄 {file.name}\")\n",
    "    else:\n",
    "        print(\"\\n training failed\")\n",
    "else:\n",
    "    print(\"can't start training, please check data structure\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5227b6-3e5a-4f40-8192-9a9148fe2aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model_path=\"model_best.pth\", dataset_name=\"tomato_test\"):\n",
    "    print(f\"evaluating {dataset_name} ...\")\n",
    "    \n",
    "    eval_cfg = build_cfg()\n",
    "    \n",
    "    full_model_path = os.path.join(eval_cfg.OUTPUT_DIR, model_path)\n",
    "    if not os.path.exists(full_model_path):\n",
    "        print(f\"model not exist: {full_model_path}\")\n",
    "\n",
    "        final_model_path = os.path.join(eval_cfg.OUTPUT_DIR, \"model_final.pth\")\n",
    "        if os.path.exists(final_model_path):\n",
    "            full_model_path = final_model_path\n",
    "            print(f\"using final model: {final_model_path}\")\n",
    "        else:\n",
    "            print(\"can't find any models\")\n",
    "            return None\n",
    "    \n",
    "    eval_cfg.MODEL.WEIGHTS = full_model_path\n",
    "    print(f\"load model: {full_model_path}\")\n",
    "    \n",
    "    try:\n",
    "        evaluator = COCOEvaluator(dataset_name, eval_cfg, False, output_dir=eval_cfg.OUTPUT_DIR)\n",
    "        test_loader = build_detection_test_loader(eval_cfg, dataset_name)\n",
    "        \n",
    "        model = MyTrainer.build_model(eval_cfg)\n",
    "        \n",
    "        print(\"start evaluating\")\n",
    "        results = inference_on_dataset(model, test_loader, evaluator)\n",
    "        \n",
    "        print(\"\\n evaluation result:\")\n",
    "        \n",
    "        if \"bbox\" in results:\n",
    "            print(\"\\n bbox result:\")\n",
    "            bbox_results = results[\"bbox\"]\n",
    "            for key in [\"AP\", \"AP50\", \"AP75\", \"APs\", \"APm\", \"APl\"]:\n",
    "                value = bbox_results.get(key, -1)\n",
    "                if value != -1:\n",
    "                    print(f\"  {key}: {value:.4f}\")\n",
    "                else:\n",
    "                    print(f\"  {key}: N/A\")\n",
    "        \n",
    "        if \"segm\" in results:\n",
    "            print(\"\\n segm result:\")\n",
    "            segm_results = results[\"segm\"]\n",
    "            for key in [\"AP\", \"AP50\", \"AP75\", \"APs\", \"APm\", \"APl\"]:\n",
    "                value = segm_results.get(key, -1)\n",
    "                if value != -1:\n",
    "                    print(f\"  {key}: {value:.4f}\")\n",
    "                else:\n",
    "                    print(f\"  {key}: N/A (no objects in this size)\")\n",
    "        \n",
    "        results_file = os.path.join(eval_cfg.OUTPUT_DIR, f\"eval_results_{dataset_name}.json\")\n",
    "        with open(results_file, 'w') as f:\n",
    "            json.dump(results, f, indent=2)\n",
    "        print(f\"\\n evaluation results saved at: {results_file}\")\n",
    "        \n",
    "        return results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"failed to evaluate: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "if 'trainer' in locals() and trainer is not None:\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"Model evaluation started\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    test_results = evaluate_model(\"model_best.pth\", \"tomato_test\")\n",
    "    \n",
    "    final_results = evaluate_model(\"model_final.pth\", \"tomato_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008a2d8d-18f5-4200-bd84-a57d6f251d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_predictions(dataset_name=\"tomato_test\", num_samples=5, model_path=\"model_best.pth\"):\n",
    "\n",
    "    print(f\"plotting {dataset_name} inference result\")\n",
    "    \n",
    "    vis_cfg = build_cfg()\n",
    "    full_model_path = os.path.join(vis_cfg.OUTPUT_DIR, model_path)\n",
    "    \n",
    "    if not os.path.exists(full_model_path):\n",
    "        print(f\"model not exist: {full_model_path}\")\n",
    "\n",
    "        final_model_path = os.path.join(vis_cfg.OUTPUT_DIR, \"model_final.pth\")\n",
    "        if os.path.exists(final_model_path):\n",
    "            full_model_path = final_model_path\n",
    "            print(f\"using final model: {final_model_path}\")\n",
    "        else:\n",
    "            print(\"no model file exist\")\n",
    "            return\n",
    "    \n",
    "    vis_cfg.MODEL.WEIGHTS = full_model_path\n",
    "    \n",
    "    predictor = DefaultPredictor(vis_cfg)\n",
    "    \n",
    "    try:\n",
    "        metadata = MetadataCatalog.get(dataset_name)\n",
    "    except:\n",
    "        print(f\" can't get {dataset_name} metadata\")\n",
    "        metadata = None\n",
    "    \n",
    "    img_dir = DATASET_CONFIG['img_dir']\n",
    "    if not os.path.exists(img_dir):\n",
    "        print(f\"image folder not eixst: {img_dir}\")\n",
    "        return\n",
    "    \n",
    "    img_list = [f for f in os.listdir(img_dir) \n",
    "                if f.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp', '.tiff'))]\n",
    "    \n",
    "    if not img_list:\n",
    "        print(f\"can't find images in {img_dir}\")\n",
    "        return\n",
    "    \n",
    "    random.shuffle(img_list)\n",
    "    shown = 0\n",
    "    \n",
    "    print(f\"using {model_path} generating {num_samples} samples...\")\n",
    "    \n",
    "    for file in img_list:\n",
    "        try:\n",
    "            img_path = os.path.join(img_dir, file)\n",
    "            im = cv2.imread(img_path)\n",
    "            \n",
    "            if im is None:\n",
    "                print(f\"failed to load image: {img_path}\")\n",
    "                continue\n",
    "            \n",
    "            outputs = predictor(im)\n",
    "            \n",
    "            v = Visualizer(im[:, :, ::-1], metadata=metadata, scale=1.2)\n",
    "            v._default_font_size = 20\n",
    "            out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
    "            \n",
    "            save_path = os.path.join(vis_cfg.OUTPUT_DIR, f\"prediction_{shown+1}_{file}\")\n",
    "            cv2.imwrite(save_path, out.get_image()[:, :, ::-1])\n",
    "            print(f\"  saved at: {save_path}\")\n",
    "            \n",
    "            shown += 1\n",
    "            if shown >= num_samples:\n",
    "                break\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"error when processing {file} : {e}\")\n",
    "            continue\n",
    "    \n",
    "    print(\"Plotting finished!\")\n",
    "\n",
    "def plot_training_history():\n",
    "    history_file = os.path.join(cfg.OUTPUT_DIR, \"training_history.json\")\n",
    "    \n",
    "    if not os.path.exists(history_file):\n",
    "        print(f\"can't find training log: {history_file}\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        with open(history_file, 'r') as f:\n",
    "            history = json.load(f)\n",
    "        \n",
    "        if not history:\n",
    "            print(\"training log is empty\")\n",
    "            return\n",
    "        \n",
    "        epochs = [h['epoch'] for h in history]\n",
    "        scores = [h['score'] for h in history]\n",
    "        \n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(epochs, scores, 'b-o', linewidth=2, markersize=6)\n",
    "        plt.title(f'training log - {history[0][\"metric\"]}', fontsize=14)\n",
    "        plt.xlabel('Epoch', fontsize=8)\n",
    "        plt.ylabel(f'{history[0][\"metric\"]}', fontsize=12)\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        best_idx = scores.index(max(scores))\n",
    "        plt.annotate(f'best: {max(scores):.4f}', \n",
    "                    xy=(epochs[best_idx], scores[best_idx]),\n",
    "                    xytext=(10, 10), textcoords='offset points',\n",
    "                    bbox=dict(boxstyle='round,pad=0.3', facecolor='yellow', alpha=0.7),\n",
    "                    arrowprops=dict(arrowstyle='->', connectionstyle='arc3,rad=0'))\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        curve_path = os.path.join(cfg.OUTPUT_DIR, \"training_curve.png\")\n",
    "        plt.savefig(curve_path, dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        print(f\"training curve saved at: {curve_path}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"failed to plot training log: {e}\")\n",
    "\n",
    "if 'trainer' in locals() and trainer is not None:\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"plotting start\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    visualize_predictions(\"tomato_test\", num_samples=3, model_path=\"model_best.pth\")\n",
    "    \n",
    "    plot_training_history()\n",
    "    \n",
    "    print(\"plotting finished\")\n",
    "\n",
    "print(\"Output saved.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
