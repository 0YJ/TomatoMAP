{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "727b72fb-b718-4ed3-8340-5376050147ed",
   "metadata": {},
   "source": [
    "# TomatoMAP-Cls Trainer"
   ]
  },
  {
   "cell_type": "code",
   "id": "fad99b1c-b47d-4386-bc62-433dda73b066",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-07T06:59:05.120408Z",
     "start_time": "2025-07-07T06:58:55.424375Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import argparse\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from PIL import ImageDraw, ImageFont\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# env checker\n",
    "print(\"check env:\")\n",
    "print(f\"  PyTorch version: {torch.__version__}\")\n",
    "print(f\"  CUDA version: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"  GPU device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"  GPU ram: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "\n",
    "print(\"env check done\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check env:\n",
      "  PyTorch version: 2.3.0\n",
      "  CUDA version: True\n",
      "  GPU device: NVIDIA GeForce RTX 4060 Laptop GPU\n",
      "  GPU ram: 8.0 GB\n",
      "env check done\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "35beeeb4-ca9c-47b4-9893-7d2bf01a1894",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-07T07:00:42.814782Z",
     "start_time": "2025-07-07T07:00:42.797777Z"
    }
   },
   "source": [
    "def save_model(model, path):\n",
    "    torch.save(model.state_dict(), path)\n",
    "    print(f\"model saved at: {path}\")\n",
    "\n",
    "def load_model(model, path, device='cpu'):\n",
    "    model.load_state_dict(torch.load(path, map_location=device))\n",
    "    model.eval()\n",
    "    print(f\"model loaded from: {path}\")\n",
    "\n",
    "def save_checkpoint(model, optimizer, epoch, path):\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict()\n",
    "    }, path)\n",
    "\n",
    "def load_checkpoint(path, model, optimizer, device):\n",
    "    checkpoint = torch.load(path, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    start_epoch = checkpoint['epoch'] + 1\n",
    "    print(f\"from epoch {start_epoch} re-training\")\n",
    "    return start_epoch\n",
    "\n",
    "def get_font(size=30, bold=False):\n",
    "    font_paths = [\n",
    "        \"C:/Windows/Fonts/arialbd.ttf\" if bold else \"C:/Windows/Fonts/arial.ttf\",\n",
    "        \"/usr/share/fonts/truetype/dejavu/DejaVuSans-Bold.ttf\" if bold else \"/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf\",\n",
    "        \"/System/Library/Fonts/Supplemental/Arial Bold.ttf\" if bold else \"/System/Library/Fonts/Supplemental/Arial.ttf\",\n",
    "    ]\n",
    "    for path in font_paths:\n",
    "        try:\n",
    "            return ImageFont.truetype(path, size=size)\n",
    "        except:\n",
    "            continue\n",
    "    return ImageFont.load_default()\n",
    "\n",
    "def denormalize(img_tensor, mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)):\n",
    "    mean = torch.tensor(mean).view(3, 1, 1).to(img_tensor.device)\n",
    "    std = torch.tensor(std).view(3, 1, 1).to(img_tensor.device)\n",
    "    return torch.clamp(img_tensor * std + mean, 0, 1)\n",
    "\n",
    "print(\"functions defination done!\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "functions defination done!\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "6358a0a3-9b2d-4062-9d7f-55f2c1948918",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-07T07:01:28.424113Z",
     "start_time": "2025-07-07T07:01:28.413604Z"
    }
   },
   "source": [
    "from torchvision import models\n",
    "from torchvision.models import (\n",
    "    MobileNet_V3_Large_Weights,\n",
    "    MobileNet_V3_Small_Weights,\n",
    "    MobileNet_V2_Weights,\n",
    "    ResNet18_Weights,\n",
    ")\n",
    "\n",
    "def get_model(name, num_classes, pretrained=True):\n",
    "    print(f\"build model: {name}, class number: {num_classes}\")\n",
    "    \n",
    "    if name == 'mobilenet_v3_large':\n",
    "        weights = MobileNet_V3_Large_Weights.DEFAULT if pretrained else None\n",
    "        model = models.mobilenet_v3_large(weights=weights)\n",
    "        model.classifier[3] = nn.Linear(model.classifier[3].in_features, num_classes)\n",
    "    elif name == 'mobilenet_v3_small':\n",
    "        weights = MobileNet_V3_Small_Weights.DEFAULT if pretrained else None\n",
    "        model = models.mobilenet_v3_small(weights=weights)\n",
    "        model.classifier[3] = nn.Linear(model.classifier[3].in_features, num_classes)\n",
    "    elif name == 'mobilenet_v2':\n",
    "        weights = MobileNet_V2_Weights.DEFAULT if pretrained else None\n",
    "        model = models.mobilenet_v2(weights=weights)\n",
    "        model.classifier[1] = nn.Linear(model.classifier[1].in_features, num_classes)\n",
    "    elif name == 'resnet18':\n",
    "        weights = ResNet18_Weights.DEFAULT if pretrained else None\n",
    "        model = models.resnet18(weights=weights)\n",
    "        model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "    else:\n",
    "        raise ValueError(f\"Model {name} not supported.\")\n",
    "\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"parameter info: Total{total_params:,}, Trainable{trainable_params:,}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "print(\"model defination done!\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model defination done!\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "06eec09a-fea7-4cd8-8712-f3d9ed59bcbc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-07T07:03:22.933578Z",
     "start_time": "2025-07-07T07:03:22.904573Z"
    }
   },
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image\n",
    "\n",
    "class BBCHDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, data_dir, split='train', transform=None):\n",
    "        self.data_dir = os.path.join(data_dir, split)\n",
    "        self.transform = transform\n",
    "        \n",
    "        if not os.path.exists(self.data_dir):\n",
    "            raise FileNotFoundError(f\"Directory not found: {self.data_dir}\")\n",
    "        \n",
    "        # get all classes\n",
    "        self.classes = sorted([d for d in os.listdir(self.data_dir)\n",
    "                              if os.path.isdir(os.path.join(self.data_dir, d))])\n",
    "        self.class_to_idx = {cls: idx for idx, cls in enumerate(self.classes)}\n",
    "\n",
    "        self.samples = []\n",
    "        for class_name in self.classes:\n",
    "            class_dir = os.path.join(self.data_dir, class_name)\n",
    "            class_idx = self.class_to_idx[class_name]\n",
    "            \n",
    "            for img_name in os.listdir(class_dir):\n",
    "                if img_name.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                    img_path = os.path.join(class_dir, img_name)\n",
    "                    self.samples.append((img_path, class_idx))\n",
    "        \n",
    "        print(f\"loading {split} dataset: {len(self.samples)} images, {len(self.classes)} classes\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label = self.samples[idx]\n",
    "        \n",
    "        try:\n",
    "            image = Image.open(img_path).convert('RGB')\n",
    "        except Exception as e:\n",
    "            print(f\"failed to load image: {img_path}, error: {e}\")\n",
    "            image = Image.new('RGB', (224, 224), (0, 0, 0))\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label\n",
    "\n",
    "def get_transforms(target_size=(640, 640)):\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.Resize(target_size),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.RandomRotation(10),\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    val_transform = transforms.Compose([\n",
    "        transforms.Resize(target_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    return train_transform, val_transform\n",
    "\n",
    "def get_dataloaders(data_dir, batch_size=32, target_size=(640, 640), num_workers=4, include_test=False):\n",
    "    print(f\"building dataloader: {data_dir}\")\n",
    "    \n",
    "    train_transform, val_transform = get_transforms(target_size)\n",
    "    \n",
    "    train_dataset = BBCHDataset(data_dir, 'train', train_transform)\n",
    "    val_dataset = BBCHDataset(data_dir, 'val', val_transform)\n",
    "\n",
    "    import platform\n",
    "    if platform.system() == 'Windows':\n",
    "        num_workers = 0\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, batch_size=batch_size, shuffle=True,\n",
    "        num_workers=num_workers, pin_memory=torch.cuda.is_available()\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset, batch_size=batch_size, shuffle=False,\n",
    "        num_workers=num_workers, pin_memory=torch.cuda.is_available()\n",
    "    )\n",
    "    \n",
    "    test_loader = None\n",
    "    if include_test:\n",
    "        test_dir = os.path.join(data_dir, 'test')\n",
    "        if os.path.exists(test_dir):\n",
    "            test_dataset = BBCHDataset(data_dir, 'test', val_transform)\n",
    "            test_loader = DataLoader(\n",
    "                test_dataset, batch_size=batch_size, shuffle=False,\n",
    "                num_workers=num_workers, pin_memory=torch.cuda.is_available()\n",
    "            )\n",
    "        else:\n",
    "            print(\"test set not found, using val as test\")\n",
    "            test_loader = val_loader\n",
    "\n",
    "    return train_loader, val_loader, test_loader\n",
    "print(f\"dataloader setup!\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataloader setup!\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "id": "7a707cf3-f677-4963-bff3-043bdc24f791",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-07T07:03:52.327730Z",
     "start_time": "2025-07-07T07:03:52.313731Z"
    }
   },
   "source": [
    "CLASSIFICATION_CONFIG = {\n",
    "    'data_dir': 'TomatoMAP-Cls',\n",
    "    'model_name': 'mobilenet_v3_large',  # 'mobilenet_v3_large', 'mobilenet_v3_small', 'mobilenet_v2', 'resnet18'\n",
    "    'num_classes': 50,\n",
    "    'batch_size': 32,\n",
    "    'num_epochs': 150,\n",
    "    'learning_rate': 1e-4,\n",
    "    'target_size': (640, 640),\n",
    "    'patience': 15,\n",
    "    'save_interval': 20\n",
    "}\n",
    "\n",
    "print(\"config:\")\n",
    "for key, value in CLASSIFICATION_CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config:\n",
      "  data_dir: TomatoMAP-Cls\n",
      "  model_name: mobilenet_v3_large\n",
      "  num_classes: 50\n",
      "  batch_size: 32\n",
      "  num_epochs: 150\n",
      "  learning_rate: 0.0001\n",
      "  target_size: (640, 640)\n",
      "  patience: 15\n",
      "  save_interval: 20\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e3ce449-a5a7-47c5-a82c-bdcddf30b367",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ PyTorch分类训练函数定义完成\n"
     ]
    }
   ],
   "source": [
    "def plot_training_curve(log_path, model_name):\n",
    "    \"\"\"绘制训练曲线\"\"\"\n",
    "    if not os.path.exists(log_path):\n",
    "        print(\"📭 没有找到训练日志\")\n",
    "        return\n",
    "        \n",
    "    df = pd.read_csv(log_path)\n",
    "    \n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # 损失曲线\n",
    "    ax1.plot(df['epoch'], df['train_loss'], 'b-', label='训练损失', linewidth=2)\n",
    "    ax1.plot(df['epoch'], df['val_loss'], 'r-', label='验证损失', linewidth=2)\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.set_title('训练/验证损失')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 准确率曲线\n",
    "    ax2.plot(df['epoch'], df['train_accuracy'], 'g-', label='训练准确率', linewidth=2)\n",
    "    ax2.plot(df['epoch'], df['val_accuracy'], 'orange', label='验证准确率', linewidth=2)\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Accuracy')\n",
    "    ax2.set_title('训练/验证准确率')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 学习率曲线（如果有的话）\n",
    "    ax3.plot(df['epoch'], df['train_loss'], 'b-', alpha=0.7)\n",
    "    ax3.set_xlabel('Epoch')\n",
    "    ax3.set_ylabel('Train Loss')\n",
    "    ax3.set_title('训练损失详细')\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 过拟合检测\n",
    "    overfitting = df['train_accuracy'] - df['val_accuracy']\n",
    "    ax4.plot(df['epoch'], overfitting, 'purple', label='过拟合程度', linewidth=2)\n",
    "    ax4.axhline(y=0, color='black', linestyle='--', alpha=0.5)\n",
    "    ax4.set_xlabel('Epoch')\n",
    "    ax4.set_ylabel('Train Acc - Val Acc')\n",
    "    ax4.set_title('过拟合检测')\n",
    "    ax4.legend()\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"checkpoints/{model_name}_detailed_curves.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(f\"📈 训练曲线已保存: checkpoints/{model_name}_detailed_curves.png\")\n",
    "\n",
    "def evaluate_classification(model, dataloader, criterion, device):\n",
    "    \"\"\"评估分类模型\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = correct / total\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "def train_classification(config):\n",
    "    \"\"\"PyTorch分类训练主函数\"\"\"\n",
    "    print(\"🚀 开始PyTorch分类训练\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # 设备检查\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"🔥 使用设备: {device}\")\n",
    "    \n",
    "    try:\n",
    "        # 数据加载器\n",
    "        train_loader, val_loader, _ = get_dataloaders(\n",
    "            config['data_dir'], \n",
    "            config['batch_size'], \n",
    "            config['target_size']\n",
    "        )\n",
    "        \n",
    "        # 获取类别信息\n",
    "        num_classes = len(train_loader.dataset.classes)\n",
    "        class_names = train_loader.dataset.classes\n",
    "        config['num_classes'] = num_classes  # 更新配置\n",
    "        \n",
    "        print(f\"📊 数据集信息:\")\n",
    "        print(f\"  训练批次: {len(train_loader)}\")\n",
    "        print(f\"  验证批次: {len(val_loader)}\")\n",
    "        print(f\"  实际类别数: {num_classes}\")\n",
    "        print(f\"  类别名称: {class_names[:5]}...\")\n",
    "        \n",
    "        # 创建模型\n",
    "        model = get_model(config['model_name'], num_classes).to(device)\n",
    "        \n",
    "        # 优化器和损失函数\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=config['learning_rate'])\n",
    "        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=config['num_epochs'])\n",
    "        \n",
    "        # 创建保存目录\n",
    "        os.makedirs(\"checkpoints\", exist_ok=True)\n",
    "        log_path = f\"checkpoints/{config['model_name']}_train_log.csv\"\n",
    "        \n",
    "        if not os.path.exists(log_path):\n",
    "            with open(log_path, 'w') as f:\n",
    "                f.write(\"epoch,train_loss,train_accuracy,val_loss,val_accuracy\\\\n\")\n",
    "        \n",
    "        print(f\"💾 保存设置:\")\n",
    "        print(f\"  检查点目录: checkpoints/\")\n",
    "        print(f\"  训练日志: {log_path}\")\n",
    "        \n",
    "        # 训练循环\n",
    "        best_val_loss = float('inf')\n",
    "        patience_counter = 0\n",
    "        \n",
    "        for epoch in range(config['num_epochs']):\n",
    "            # 训练阶段\n",
    "            model.train()\n",
    "            running_loss = 0.0\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            \n",
    "            print(f\"\\\\n📅 Epoch {epoch+1}/{config['num_epochs']}\")\n",
    "            \n",
    "            progress_bar = tqdm(train_loader, desc=f\"训练Epoch {epoch+1}\")\n",
    "            for batch_idx, (images, labels) in enumerate(progress_bar):\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                running_loss += loss.item()\n",
    "                _, predicted = outputs.max(1)\n",
    "                total += labels.size(0)\n",
    "                correct += predicted.eq(labels).sum().item()\n",
    "                \n",
    "                # 更新进度条\n",
    "                current_acc = correct / total\n",
    "                progress_bar.set_postfix({\n",
    "                    'Loss': f'{loss.item():.4f}',\n",
    "                    'Acc': f'{current_acc:.4f}'\n",
    "                })\n",
    "                \n",
    "                # 定期显示详细信息\n",
    "                if (batch_idx + 1) % 50 == 0:\n",
    "                    print(f\"    Batch {batch_idx+1}/{len(train_loader)} - Loss: {loss.item():.4f}, Acc: {current_acc:.4f}\")\n",
    "            \n",
    "            # 计算训练指标\n",
    "            train_loss = running_loss / len(train_loader)\n",
    "            train_acc = correct / total\n",
    "            \n",
    "            # 验证阶段\n",
    "            val_loss, val_acc = evaluate_classification(model, val_loader, criterion, device)\n",
    "            \n",
    "            # 学习率调度\n",
    "            scheduler.step()\n",
    "            current_lr = scheduler.get_last_lr()[0]\n",
    "            \n",
    "            print(f\"📊 Epoch {epoch+1} 结果:\")\n",
    "            print(f\"  训练: Loss={train_loss:.4f}, Acc={train_acc:.4f}\")\n",
    "            print(f\"  验证: Loss={val_loss:.4f}, Acc={val_acc:.4f}\")\n",
    "            print(f\"  学习率: {current_lr:.6f}\")\n",
    "            \n",
    "            # 记录训练日志\n",
    "            with open(log_path, 'a') as f:\n",
    "                f.write(f\"{epoch+1},{train_loss:.4f},{train_acc:.4f},{val_loss:.4f},{val_acc:.4f}\\\\n\")\n",
    "            \n",
    "            # 保存检查点\n",
    "            save_checkpoint(model, optimizer, epoch, \"checkpoints/last_checkpoint.pth\")\n",
    "            \n",
    "            # 保存最佳模型\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                patience_counter = 0\n",
    "                save_model(model, f\"checkpoints/{config['model_name']}_best.pth\")\n",
    "                print(\"🌟 保存最佳模型!\")\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                print(f\"⏰ 早停计数: {patience_counter}/{config['patience']}\")\n",
    "            \n",
    "            # 定期保存\n",
    "            if (epoch + 1) % config['save_interval'] == 0:\n",
    "                save_checkpoint(model, optimizer, epoch, \n",
    "                              f\"checkpoints/{config['model_name']}_epoch{epoch+1}.pth\")\n",
    "                print(f\"💾 保存检查点: epoch{epoch+1}\")\n",
    "            \n",
    "            # 检查早停\n",
    "            if patience_counter >= config['patience']:\n",
    "                print(\"🛑 触发早停!\")\n",
    "                break\n",
    "        \n",
    "        # 保存最终模型\n",
    "        save_model(model, f\"{config['model_name']}_final.pth\")\n",
    "        \n",
    "        # 绘制训练曲线\n",
    "        plot_training_curve(log_path, config['model_name'])\n",
    "        \n",
    "        print(f\"\\\\n🎉 训练完成!\")\n",
    "        print(f\"📁 最佳模型: checkpoints/{config['model_name']}_best.pth\")\n",
    "        print(f\"📁 最终模型: {config['model_name']}_final.pth\")\n",
    "        \n",
    "        return model, train_loader, val_loader\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ 训练失败: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None, None, None\n",
    "\n",
    "print(\"✅ PyTorch分类训练函数定义完成\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2f7cc2e7-1ee6-4c22-9d25-ef8911af5dfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "🎯 TomatoMAP 分类模型训练\n",
      "============================================================\n",
      "✅ 数据目录存在: TomatoMAP-Cls\n",
      "✅ 训练、验证和测试目录都存在\n",
      "\n",
      "⚙️ 训练配置:\n",
      "   data_dir: TomatoMAP-Cls\n",
      "   model_name: mobilenet_v3_large\n",
      "   num_classes: 50\n",
      "   batch_size: 32\n",
      "   num_epochs: 150\n",
      "   learning_rate: 0.0001\n",
      "   target_size: (640, 640)\n",
      "   patience: 15\n",
      "   save_interval: 20\n",
      "\n",
      "🚀 开始训练...\n",
      "🔧 使用设备: cpu\n",
      "🔧 创建数据加载器: TomatoMAP-Cls\n",
      "📊 加载 train 数据集: 45099 张图片, 50 个类别\n",
      "📊 加载 val 数据集: 12870 张图片, 50 个类别\n",
      "📊 加载 test 数据集: 6495 张图片, 50 个类别\n",
      "✅ 数据加载器创建完成\n",
      "🤖 创建模型: mobilenet_v3_large, 类别数: 50\n",
      "📊 参数统计: 总数4,266,082, 可训练4,266,082\n",
      "🚀 开始训练 150 个epoch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/150 [训练]:   0%|▏                                                                                                                            | 2/1410 [00:21<4:10:10, 10.66s/it, Loss=3.9196, Acc=4.69%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "⏹️ 训练被用户中断\n"
     ]
    }
   ],
   "source": [
    "# 完整的训练循环\n",
    "def train_model(config):\n",
    "    \"\"\"完整的分类模型训练函数\"\"\"\n",
    "    \n",
    "    # 设置设备\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"🔧 使用设备: {device}\")\n",
    "    \n",
    "    # 创建输出目录\n",
    "    output_dir = Path(f\"runs/{config['model_name']}_cls\")\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # 获取数据加载器（包含test集）\n",
    "    train_loader, val_loader, test_loader = get_dataloaders(\n",
    "        config['data_dir'], \n",
    "        batch_size=config['batch_size'],\n",
    "        target_size=config['target_size'],\n",
    "        num_workers=4,\n",
    "        include_test=True\n",
    "    )\n",
    "    \n",
    "    # 创建模型\n",
    "    model = get_model(config['model_name'], config['num_classes'], pretrained=True)\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # 损失函数和优化器\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=config['learning_rate'], weight_decay=0.01)\n",
    "    \n",
    "    # 学习率调度器\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)\n",
    "    \n",
    "    # 训练历史记录\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_accuracies = []\n",
    "    val_accuracies = []\n",
    "    \n",
    "    # 早停参数\n",
    "    best_val_acc = 0.0\n",
    "    patience_counter = 0\n",
    "    \n",
    "    print(f\"🚀 开始训练 {config['num_epochs']} 个epoch...\")\n",
    "    \n",
    "    for epoch in range(config['num_epochs']):\n",
    "        # 训练阶段\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        \n",
    "        train_pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{config['num_epochs']} [训练]\")\n",
    "        \n",
    "        for batch_idx, (images, labels) in enumerate(train_pbar):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            # 前向传播\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # 反向传播\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # 统计\n",
    "            train_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            train_total += labels.size(0)\n",
    "            train_correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            # 更新进度条\n",
    "            current_acc = 100 * train_correct / train_total\n",
    "            train_pbar.set_postfix({\n",
    "                'Loss': f'{loss.item():.4f}',\n",
    "                'Acc': f'{current_acc:.2f}%'\n",
    "            })\n",
    "        \n",
    "        # 验证阶段\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            val_pbar = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{config['num_epochs']} [验证]\")\n",
    "            \n",
    "            for images, labels in val_pbar:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += (predicted == labels).sum().item()\n",
    "                \n",
    "                current_acc = 100 * val_correct / val_total\n",
    "                val_pbar.set_postfix({\n",
    "                    'Loss': f'{loss.item():.4f}',\n",
    "                    'Acc': f'{current_acc:.2f}%'\n",
    "                })\n",
    "        \n",
    "        # 计算平均损失和准确率\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        train_acc = 100 * train_correct / train_total\n",
    "        val_acc = 100 * val_correct / val_total\n",
    "        \n",
    "        # 记录历史\n",
    "        train_losses.append(avg_train_loss)\n",
    "        val_losses.append(avg_val_loss)\n",
    "        train_accuracies.append(train_acc)\n",
    "        val_accuracies.append(val_acc)\n",
    "        \n",
    "        # 更新学习率\n",
    "        scheduler.step()\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        \n",
    "        # 打印epoch结果\n",
    "        print(f\"\\n📊 Epoch {epoch+1}/{config['num_epochs']}:\")\n",
    "        print(f\"  训练 - Loss: {avg_train_loss:.4f}, Acc: {train_acc:.2f}%\")\n",
    "        print(f\"  验证 - Loss: {avg_val_loss:.4f}, Acc: {val_acc:.2f}%\")\n",
    "        print(f\"  学习率: {current_lr:.2e}\")\n",
    "        \n",
    "        # 保存最佳模型\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_model_path = output_dir / f\"best_{config['model_name']}.pth\"\n",
    "            save_model(model, best_model_path)\n",
    "            patience_counter = 0\n",
    "            print(f\"  ⭐ 新的最佳验证准确率: {best_val_acc:.2f}%\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f\"  ⏳ 验证准确率未提升 ({patience_counter}/{config['patience']})\")\n",
    "        \n",
    "        # 定期保存检查点\n",
    "        if (epoch + 1) % config['save_interval'] == 0:\n",
    "            checkpoint_path = output_dir / f\"checkpoint_epoch_{epoch+1}.pth\"\n",
    "            save_checkpoint(model, optimizer, epoch, checkpoint_path)\n",
    "        \n",
    "        # 早停检查\n",
    "        if patience_counter >= config['patience']:\n",
    "            print(f\"\\n🛑 早停触发! 验证准确率连续 {config['patience']} 个epoch未提升\")\n",
    "            break\n",
    "        \n",
    "        print(\"-\" * 60)\n",
    "    \n",
    "    # 训练完成，保存最终模型\n",
    "    final_model_path = output_dir / f\"final_{config['model_name']}.pth\"\n",
    "    save_model(model, final_model_path)\n",
    "    \n",
    "    print(f\"\\n🎉 训练完成!\")\n",
    "    print(f\"  最佳验证准确率: {best_val_acc:.2f}%\")\n",
    "    print(f\"  模型保存路径: {output_dir}\")\n",
    "    \n",
    "    # 绘制训练曲线\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    # 损失曲线\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.plot(train_losses, label='训练损失', color='blue')\n",
    "    plt.plot(val_losses, label='验证损失', color='red')\n",
    "    plt.title('训练损失曲线')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 准确率曲线\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.plot(train_accuracies, label='训练准确率', color='blue')\n",
    "    plt.plot(val_accuracies, label='验证准确率', color='red')\n",
    "    plt.title('训练准确率曲线')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 学习率曲线\n",
    "    plt.subplot(1, 3, 3)\n",
    "    lrs = []\n",
    "    for i in range(len(train_losses)):\n",
    "        if i < 30:\n",
    "            lrs.append(config['learning_rate'])\n",
    "        elif i < 60:\n",
    "            lrs.append(config['learning_rate'] * 0.1)\n",
    "        else:\n",
    "            lrs.append(config['learning_rate'] * 0.01)\n",
    "    plt.plot(lrs, label='学习率', color='green')\n",
    "    plt.title('学习率变化')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Learning Rate')\n",
    "    plt.yscale('log')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_dir / 'training_curves.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # 保存训练历史\n",
    "    history_df = pd.DataFrame({\n",
    "        'epoch': range(1, len(train_losses) + 1),\n",
    "        'train_loss': train_losses,\n",
    "        'val_loss': val_losses,\n",
    "        'train_acc': train_accuracies,\n",
    "        'val_acc': val_accuracies\n",
    "    })\n",
    "    history_df.to_csv(output_dir / 'training_history.csv', index=False)\n",
    "    print(f\"📈 训练历史已保存: {output_dir / 'training_history.csv'}\")\n",
    "    \n",
    "    return model, best_val_acc, output_dir, test_loader\n",
    "\n",
    "\n",
    "# 🚀 开始执行训练\n",
    "print(\"=\" * 60)\n",
    "print(\"🎯 TomatoMAP 分类模型训练\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 首先检查数据目录\n",
    "if not os.path.exists(CLASSIFICATION_CONFIG['data_dir']):\n",
    "    print(f\"❌ 错误: 数据目录不存在\")\n",
    "    print(f\"   路径: {CLASSIFICATION_CONFIG['data_dir']}\")\n",
    "    print(f\"   请确保数据目录存在并包含 train/ 和 val/ 子目录\")\n",
    "else:\n",
    "    print(f\"✅ 数据目录存在: {CLASSIFICATION_CONFIG['data_dir']}\")\n",
    "    \n",
    "    # 检查train、val和test目录\n",
    "    train_dir = os.path.join(CLASSIFICATION_CONFIG['data_dir'], 'train')\n",
    "    val_dir = os.path.join(CLASSIFICATION_CONFIG['data_dir'], 'val')\n",
    "    test_dir = os.path.join(CLASSIFICATION_CONFIG['data_dir'], 'test')\n",
    "    \n",
    "    if not os.path.exists(train_dir):\n",
    "        print(f\"❌ 错误: 训练目录不存在: {train_dir}\")\n",
    "    elif not os.path.exists(val_dir):\n",
    "        print(f\"❌ 错误: 验证目录不存在: {val_dir}\")\n",
    "    elif not os.path.exists(test_dir):\n",
    "        print(f\"⚠️ 警告: 测试目录不存在: {test_dir}\")\n",
    "        print(f\"   将使用验证集作为测试集\")\n",
    "    else:\n",
    "        print(f\"✅ 训练、验证和测试目录都存在\")\n",
    "        \n",
    "        # 显示训练配置\n",
    "        print(\"\\n⚙️ 训练配置:\")\n",
    "        for key, value in CLASSIFICATION_CONFIG.items():\n",
    "            print(f\"   {key}: {value}\")\n",
    "        \n",
    "        print(\"\\n🚀 开始训练...\")\n",
    "        \n",
    "        try:\n",
    "            # 执行训练\n",
    "            model, best_acc, output_dir, test_loader = train_model(CLASSIFICATION_CONFIG)\n",
    "            \n",
    "            print(\"\\n\" + \"=\" * 60)\n",
    "            print(\"🎉 训练成功完成!\")\n",
    "            print(f\"   最佳验证准确率: {best_acc:.2f}%\")\n",
    "            print(f\"   模型保存目录: {output_dir}\")\n",
    "            \n",
    "            # 在测试集上评估\n",
    "            print(\"\\n🧪 在测试集上评估模型...\")\n",
    "            model.eval()\n",
    "            test_correct = 0\n",
    "            test_total = 0\n",
    "            test_predictions = []\n",
    "            test_labels = []\n",
    "            \n",
    "            device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                test_pbar = tqdm(test_loader, desc=\"测试集评估\")\n",
    "                for images, labels in test_pbar:\n",
    "                    images, labels = images.to(device), labels.to(device)\n",
    "                    outputs = model(images)\n",
    "                    _, predicted = torch.max(outputs.data, 1)\n",
    "                    \n",
    "                    test_total += labels.size(0)\n",
    "                    test_correct += (predicted == labels).sum().item()\n",
    "                    \n",
    "                    # 收集预测结果用于混淆矩阵\n",
    "                    test_predictions.extend(predicted.cpu().numpy())\n",
    "                    test_labels.extend(labels.cpu().numpy())\n",
    "                    \n",
    "                    current_acc = 100 * test_correct / test_total\n",
    "                    test_pbar.set_postfix({'Acc': f'{current_acc:.2f}%'})\n",
    "            \n",
    "            test_accuracy = 100 * test_correct / test_total\n",
    "            print(f\"🎯 测试集准确率: {test_accuracy:.2f}%\")\n",
    "            \n",
    "            # 绘制混淆矩阵\n",
    "            print(\"\\n📊 生成混淆矩阵...\")\n",
    "            \n",
    "            # 获取类别名称\n",
    "            train_dataset = test_loader.dataset\n",
    "            class_names = train_dataset.classes\n",
    "            \n",
    "            # 计算混淆矩阵\n",
    "            cm = confusion_matrix(test_labels, test_predictions)\n",
    "            \n",
    "            # 绘制混淆矩阵\n",
    "            plt.figure(figsize=(12, 10))\n",
    "            disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)\n",
    "            disp.plot(cmap='Blues', values_format='d')\n",
    "            plt.title(f'混淆矩阵 (测试集准确率: {test_accuracy:.2f}%)', fontsize=16)\n",
    "            plt.xticks(rotation=45, ha='right')\n",
    "            plt.yticks(rotation=0)\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(output_dir / 'confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
    "            plt.show()\n",
    "            \n",
    "            # 保存测试结果\n",
    "            test_results = {\n",
    "                'test_accuracy': test_accuracy,\n",
    "                'total_samples': test_total,\n",
    "                'correct_predictions': test_correct,\n",
    "                'num_classes': len(class_names),\n",
    "                'class_names': class_names\n",
    "            }\n",
    "            \n",
    "            import json\n",
    "            with open(output_dir / 'test_results.json', 'w', encoding='utf-8') as f:\n",
    "                json.dump(test_results, f, indent=2, ensure_ascii=False)\n",
    "            \n",
    "            print(\"\\n\" + \"=\" * 60)\n",
    "            print(\"📈 完整评估结果:\")\n",
    "            print(f\"   验证集最佳准确率: {best_acc:.2f}%\")\n",
    "            print(f\"   测试集准确率: {test_accuracy:.2f}%\")\n",
    "            print(f\"   类别数量: {len(class_names)}\")\n",
    "            print(f\"   测试样本数: {test_total}\")\n",
    "            print(f\"   结果保存路径: {output_dir}\")\n",
    "            print(\"=\" * 60)\n",
    "            \n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\n⏹️ 训练被用户中断\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\n❌ 训练过程中出现错误:\")\n",
    "            print(f\"   错误信息: {str(e)}\")\n",
    "            print(\"\\n详细错误信息:\")\n",
    "            import traceback\n",
    "            traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c9228d-dc78-42af-8b78-7a41685f25b5",
   "metadata": {},
   "source": [
    "# TomatoMAP-Seg Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e455442e-145e-431b-9193-0f3a1c62f348",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 环境检查:\n",
      "  PyTorch版本: 2.3.0+cu121\n",
      "  CUDA可用: False\n",
      "  Detectron2版本: 0.6\n",
      "✅ 环境检查完成\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/envs/py310/lib/python3.10/site-packages/torch/cuda/__init__.py:118: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 11040). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import json\n",
    "import yaml\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from pycocotools import mask as mask_utils\n",
    "\n",
    "# Detectron2相关导入\n",
    "import detectron2\n",
    "from detectron2.engine import DefaultTrainer, DefaultPredictor\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.data import MetadataCatalog, build_detection_test_loader\n",
    "from detectron2.data.datasets import register_coco_instances\n",
    "from detectron2.evaluation import COCOEvaluator, inference_on_dataset\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "from detectron2.data.datasets.coco import load_coco_json\n",
    "from detectron2.utils.logger import setup_logger\n",
    "\n",
    "# 设置日志\n",
    "setup_logger()\n",
    "\n",
    "print(\"🔧 环境检查:\")\n",
    "print(f\"  PyTorch版本: {torch.__version__}\")\n",
    "print(f\"  CUDA可用: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"  GPU设备: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"  GPU内存: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "print(f\"  Detectron2版本: {detectron2.__version__}\")\n",
    "print(\"✅ 环境检查完成\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df4df010-b134-4435-9017-27871d82dd85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚙️ 语义实例分割配置:\n",
      "📁 数据集配置:\n",
      "  dataset_root: ./\n",
      "  img_dir: images\n",
      "  coco_ann_dir: labels\n",
      "  isat_yaml_path: isat.yaml\n",
      "  output_dir: ./output\n",
      "  num_classes: 10\n",
      "\n",
      "🎯 训练配置:\n",
      "  model_name: mask_rcnn_R_50_FPN_3x\n",
      "  batch_size: 2\n",
      "  base_lr: 0.00025\n",
      "  max_iter: 1000\n",
      "  num_workers: 0\n",
      "  score_thresh_test: 0.5\n",
      "  input_min_size_test: 800\n",
      "  input_max_size_test: 1333\n"
     ]
    }
   ],
   "source": [
    "# 数据集配置\n",
    "DATASET_CONFIG = {\n",
    "    'dataset_root': \"./\",\n",
    "    'img_dir': \"images\",  # 图像目录\n",
    "    'coco_ann_dir': \"labels\",  # COCO格式标注目录\n",
    "    'isat_yaml_path': \"isat.yaml\",  # ISAT配置文件\n",
    "    'output_dir': \"./output\",  # 输出目录\n",
    "    'num_classes': 10,  # 类别数量（不包括背景）\n",
    "}\n",
    "\n",
    "# 训练配置\n",
    "TRAINING_CONFIG = {\n",
    "    'model_name': \"mask_rcnn_R_50_FPN_3x\",\n",
    "    'batch_size': 2,\n",
    "    'base_lr': 0.00025,\n",
    "    'max_iter': 1000,\n",
    "    'num_workers': 0,  # Windows设为0，Linux可设为4\n",
    "    'score_thresh_test': 0.5,\n",
    "    'input_min_size_test': 800,\n",
    "    'input_max_size_test': 1333,\n",
    "}\n",
    "\n",
    "print(\"⚙️ 语义实例分割配置:\")\n",
    "print(\"📁 数据集配置:\")\n",
    "for key, value in DATASET_CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "print(\"\\n🎯 训练配置:\")\n",
    "for key, value in TRAINING_CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a302a659-d993-40b7-ac3f-ef4b4cf42788",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 验证数据集完整性...\n",
      "  ❌ images - 路径不存在!\n",
      "  ✅ labels\n",
      "  ❌ isat.yaml - 路径不存在!\n",
      "  ⚠️ train.json - 文件不存在\n",
      "  ⚠️ val.json - 文件不存在\n",
      "  ⚠️ test.json - 文件不存在\n",
      "❌ 错误: 没有找到任何COCO格式的标注文件!\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: 数据集验证和注册\n",
    "print(\"🔍 验证数据集完整性...\")\n",
    "\n",
    "# 检查必要的文件和目录\n",
    "required_paths = [\n",
    "    DATASET_CONFIG['img_dir'],\n",
    "    DATASET_CONFIG['coco_ann_dir'],\n",
    "    DATASET_CONFIG['isat_yaml_path']\n",
    "]\n",
    "\n",
    "for path in required_paths:\n",
    "    if os.path.exists(path):\n",
    "        print(f\"  ✅ {path}\")\n",
    "    else:\n",
    "        print(f\"  ❌ {path} - 路径不存在!\")\n",
    "\n",
    "# 检查COCO格式的标注文件\n",
    "coco_files = ['train.json', 'val.json', 'test.json']\n",
    "available_splits = []\n",
    "\n",
    "for coco_file in coco_files:\n",
    "    coco_path = os.path.join(DATASET_CONFIG['coco_ann_dir'], coco_file)\n",
    "    if os.path.exists(coco_path):\n",
    "        print(f\"  ✅ {coco_file}\")\n",
    "        available_splits.append(coco_file.replace('.json', ''))\n",
    "        \n",
    "        # 检查标注文件的基本信息\n",
    "        with open(coco_path, 'r') as f:\n",
    "            coco_data = json.load(f)\n",
    "        print(f\"    - 图像数量: {len(coco_data['images'])}\")\n",
    "        print(f\"    - 标注数量: {len(coco_data['annotations'])}\")\n",
    "        print(f\"    - 类别数量: {len(coco_data['categories'])}\")\n",
    "    else:\n",
    "        print(f\"  ⚠️ {coco_file} - 文件不存在\")\n",
    "\n",
    "if not available_splits:\n",
    "    print(\"❌ 错误: 没有找到任何COCO格式的标注文件!\")\n",
    "else:\n",
    "    print(f\"\\n📊 可用数据集: {available_splits}\")\n",
    "    \n",
    "    # 注册数据集\n",
    "    class_labels = register_all_datasets()\n",
    "    print(\"✅ 数据集注册完成\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6fad585b-4d6a-4c66-a172-16b10c609076",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ 无法开始训练，请检查数据集配置\n"
     ]
    }
   ],
   "source": [
    "def train_model():\n",
    "    \"\"\"执行模型训练\"\"\"\n",
    "    print(\"🚀 开始训练语义实例分割模型...\")\n",
    "    \n",
    "    # 构建配置\n",
    "    cfg = build_cfg()\n",
    "    \n",
    "    # 保存配置\n",
    "    save_config(cfg, cfg.OUTPUT_DIR)\n",
    "    \n",
    "    # 打印训练信息\n",
    "    print(f\"\\n📋 训练信息:\")\n",
    "    print(f\"  模型: {TRAINING_CONFIG['model_name']}\")\n",
    "    print(f\"  类别数: {DATASET_CONFIG['num_classes']}\")\n",
    "    print(f\"  批次大小: {TRAINING_CONFIG['batch_size']}\")\n",
    "    print(f\"  学习率: {TRAINING_CONFIG['base_lr']}\")\n",
    "    print(f\"  最大迭代: {TRAINING_CONFIG['max_iter']}\")\n",
    "    print(f\"  输出目录: {cfg.OUTPUT_DIR}\")\n",
    "    print(f\"  设备: {cfg.MODEL.DEVICE}\")\n",
    "    \n",
    "    # 创建训练器\n",
    "    trainer = CustomTrainer(cfg)\n",
    "    trainer.resume_or_load(resume=False)\n",
    "    \n",
    "    print(f\"\\n🎯 开始训练 {TRAINING_CONFIG['max_iter']} 步...\")\n",
    "    \n",
    "    try:\n",
    "        # 开始训练\n",
    "        trainer.train()\n",
    "        \n",
    "        print(\"\\n🎉 训练完成!\")\n",
    "        \n",
    "        # 保存最终模型路径\n",
    "        final_model_path = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")\n",
    "        if os.path.exists(final_model_path):\n",
    "            print(f\"💾 最终模型已保存: {final_model_path}\")\n",
    "        \n",
    "        return trainer, cfg\n",
    "        \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n⏹️ 训练被用户中断\")\n",
    "        return None, cfg\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n❌ 训练过程中出现错误:\")\n",
    "        print(f\"   错误信息: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None, cfg\n",
    "\n",
    "# 检查是否可以开始训练\n",
    "if 'class_labels' in locals() and available_splits:\n",
    "    print(\"=\" * 60)\n",
    "    print(\"🎯 准备开始训练...\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # 执行训练\n",
    "    trainer, cfg = train_model()\n",
    "    \n",
    "    if trainer is not None:\n",
    "        print(\"\\n✅ 训练流程完成!\")\n",
    "        \n",
    "        # 显示训练结果位置\n",
    "        print(f\"\\n📁 输出文件:\")\n",
    "        output_dir = Path(cfg.OUTPUT_DIR)\n",
    "        if output_dir.exists():\n",
    "            for file in output_dir.iterdir():\n",
    "                if file.is_file():\n",
    "                    print(f\"  📄 {file.name}\")\n",
    "    else:\n",
    "        print(\"\\n❌ 训练未成功完成\")\n",
    "else:\n",
    "    print(\"❌ 无法开始训练，请检查数据集配置\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ca1be6aa-5a4b-4808-9e77-7e366ee333fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(dataset_name=\"tomato_test\", config_override=None):\n",
    "    \"\"\"评估训练好的模型\"\"\"\n",
    "    print(f\"📊 在 {dataset_name} 上评估模型...\")\n",
    "    \n",
    "    # 构建配置\n",
    "    cfg = build_cfg()\n",
    "    if config_override:\n",
    "        cfg.update(config_override)\n",
    "    \n",
    "    # 加载训练好的模型\n",
    "    model_path = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")\n",
    "    if not os.path.exists(model_path):\n",
    "        print(f\"❌ 模型文件不存在: {model_path}\")\n",
    "        return None\n",
    "    \n",
    "    cfg.MODEL.WEIGHTS = model_path\n",
    "    \n",
    "    # 创建评估器\n",
    "    evaluator = COCOEvaluator(dataset_name, cfg, False, output_dir=cfg.OUTPUT_DIR)\n",
    "    val_loader = build_detection_test_loader(cfg, dataset_name)\n",
    "    \n",
    "    # 构建模型\n",
    "    model = DefaultTrainer.build_model(cfg)\n",
    "    \n",
    "    print(\"🔍 开始评估...\")\n",
    "    results = inference_on_dataset(model, val_loader, evaluator)\n",
    "    \n",
    "    # 显示结果\n",
    "    print(\"\\n📈 评估结果:\")\n",
    "    print(json.dumps(results, indent=2))\n",
    "    \n",
    "    # 保存结果\n",
    "    results_path = os.path.join(cfg.OUTPUT_DIR, f\"eval_results_{dataset_name}.json\")\n",
    "    with open(results_path, 'w') as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    print(f\"💾 评估结果已保存: {results_path}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# 如果训练完成，进行评估\n",
    "if 'trainer' in locals() and trainer is not None:\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"📊 开始模型评估\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # 在测试集上评估\n",
    "    if 'test' in available_splits:\n",
    "        test_results = evaluate_model(\"tomato_test\")\n",
    "    \n",
    "    # 在验证集上评估\n",
    "    if 'val' in available_splits:\n",
    "        val_results = evaluate_model(\"tomato_val\")\n",
    "    \n",
    "    print(\"✅ 评估完成!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b3f394e5-4edb-494d-b281-41765beaa3f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🎉 Detectron2语义实例分割训练流程完成!\n"
     ]
    }
   ],
   "source": [
    "def visualize_predictions(dataset_name=\"tomato_test\", num_samples=5):\n",
    "    \"\"\"可视化预测结果\"\"\"\n",
    "    print(f\"🎨 可视化 {dataset_name} 预测结果...\")\n",
    "    \n",
    "    # 构建配置\n",
    "    cfg = build_cfg()\n",
    "    cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")\n",
    "    \n",
    "    # 创建预测器\n",
    "    predictor = DefaultPredictor(cfg)\n",
    "    metadata = MetadataCatalog.get(dataset_name)\n",
    "    \n",
    "    # 加载数据集\n",
    "    split_name = dataset_name.split('_')[-1]\n",
    "    dataset_dicts = load_coco_json(\n",
    "        os.path.join(DATASET_CONFIG['coco_ann_dir'], f\"{split_name}.json\"), \n",
    "        DATASET_CONFIG['img_dir']\n",
    "    )\n",
    "    \n",
    "    # 随机选择样本进行可视化\n",
    "    sample_data = random.sample(dataset_dicts, min(num_samples, len(dataset_dicts)))\n",
    "    \n",
    "    print(f\"🖼️ 处理 {len(sample_data)} 张图像...\")\n",
    "    \n",
    "    for i, d in enumerate(sample_data):\n",
    "        img_path = d[\"file_name\"]\n",
    "        if not os.path.exists(img_path):\n",
    "            print(f\"⚠️ 图像不存在: {img_path}\")\n",
    "            continue\n",
    "            \n",
    "        # 读取图像\n",
    "        im = cv2.imread(img_path)\n",
    "        \n",
    "        # 预测\n",
    "        outputs = predictor(im)\n",
    "        \n",
    "        # 可视化\n",
    "        v = Visualizer(im[:, :, ::-1], metadata=metadata, scale=1.2)\n",
    "        v._default_font_size = 20\n",
    "        out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
    "        \n",
    "        # 保存结果\n",
    "        save_path = os.path.join(cfg.OUTPUT_DIR, f\"prediction_{i+1}_{os.path.basename(img_path)}\")\n",
    "        cv2.imwrite(save_path, out.get_image()[:, :, ::-1])\n",
    "        print(f\"  💾 {save_path}\")\n",
    "    \n",
    "    print(\"✅ 可视化完成!\")\n",
    "\n",
    "def generate_confusion_matrix(dataset_name=\"tomato_val\"):\n",
    "    \"\"\"生成混淆矩阵\"\"\"\n",
    "    print(f\"📊 生成 {dataset_name} 混淆矩阵...\")\n",
    "    \n",
    "    cfg = build_cfg()\n",
    "    cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.3\n",
    "    cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")\n",
    "    predictor = DefaultPredictor(cfg)\n",
    "\n",
    "    split_name = dataset_name.split('_')[-1]\n",
    "    dataset_dicts = load_coco_json(\n",
    "        os.path.join(DATASET_CONFIG['coco_ann_dir'], f\"{split_name}.json\"),\n",
    "        DATASET_CONFIG['img_dir'],\n",
    "        dataset_name=dataset_name\n",
    "    )\n",
    "\n",
    "    metadata = MetadataCatalog.get(dataset_name)\n",
    "    class_names = metadata.thing_classes\n",
    "    num_classes = len(class_names)\n",
    "\n",
    "    cmatrix_total = np.zeros((num_classes, num_classes), dtype=np.int64)\n",
    "\n",
    "    print(f\"🔍 处理 {len(dataset_dicts)} 张图像...\")\n",
    "    \n",
    "    for data in tqdm(dataset_dicts, desc=\"生成混淆矩阵\"):\n",
    "        height, width = data[\"height\"], data[\"width\"]\n",
    "        image = cv2.imread(data[\"file_name\"])\n",
    "        if image is None:\n",
    "            continue\n",
    "\n",
    "        # 生成真实标注mask\n",
    "        gt_mask = np.zeros((height, width), dtype=np.uint8)\n",
    "        for ann in data.get(\"annotations\", []):\n",
    "            category_id = ann[\"category_id\"]\n",
    "            segmentation = ann[\"segmentation\"]\n",
    "            if isinstance(segmentation, list):\n",
    "                rle = mask_utils.frPyObjects(segmentation, height, width)\n",
    "                rle = mask_utils.merge(rle)\n",
    "            elif isinstance(segmentation, dict) and \"counts\" in segmentation:\n",
    "                rle = segmentation\n",
    "            else:\n",
    "                continue\n",
    "            m = mask_utils.decode(rle)\n",
    "            gt_mask[m == 1] = category_id\n",
    "\n",
    "        # 生成预测mask\n",
    "        outputs = predictor(image)\n",
    "        instances = outputs[\"instances\"].to(\"cpu\")\n",
    "\n",
    "        pred_mask = np.zeros((height, width), dtype=np.uint8)\n",
    "        for i in range(len(instances)):\n",
    "            class_id = int(instances.pred_classes[i])\n",
    "            mask = instances.pred_masks[i].numpy()\n",
    "            pred_mask[mask == 1] = class_id\n",
    "\n",
    "        # 计算混淆矩阵\n",
    "        cm_local = confusion_matrix(\n",
    "            gt_mask.flatten(),\n",
    "            pred_mask.flatten(),\n",
    "            labels=list(range(num_classes))\n",
    "        )\n",
    "        cmatrix_total += cm_local\n",
    "\n",
    "    # 归一化混淆矩阵\n",
    "    cmatrix_norm = np.nan_to_num(cmatrix_total.astype('float') / cmatrix_total.sum(axis=1, keepdims=True))\n",
    "    \n",
    "    # 保存为Excel\n",
    "    df = pd.DataFrame(cmatrix_norm, index=class_names, columns=class_names)\n",
    "    excel_path = os.path.join(cfg.OUTPUT_DIR, f\"confusion_matrix_{split_name}.xlsx\")\n",
    "    df.to_excel(excel_path)\n",
    "    print(f\"💾 混淆矩阵已保存: {excel_path}\")\n",
    "\n",
    "    # 绘制混淆矩阵\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    masked = np.ma.masked_where(cmatrix_norm == 0, cmatrix_norm)\n",
    "    im = ax.imshow(masked, cmap=\"Blues\", vmin=0.0, vmax=1.0)\n",
    "    \n",
    "    # 添加数值标注\n",
    "    for i in range(num_classes):\n",
    "        for j in range(num_classes):\n",
    "            val = cmatrix_norm[i, j]\n",
    "            if val > 0:\n",
    "                color = 'white' if val > 0.5 else 'black'\n",
    "                ax.text(j, i, f\"{val:.2f}\", ha='center', va='center', color=color, fontsize=8)\n",
    "    \n",
    "    ax.set_xticks(np.arange(num_classes))\n",
    "    ax.set_yticks(np.arange(num_classes))\n",
    "    ax.set_xticklabels(class_names, rotation=45, ha='right')\n",
    "    ax.set_yticklabels(class_names)\n",
    "    ax.set_xlabel(\"预测类别\")\n",
    "    ax.set_ylabel(\"真实类别\")\n",
    "    ax.set_title(f\"混淆矩阵 - {dataset_name}\")\n",
    "    \n",
    "    plt.colorbar(im, ax=ax)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # 保存图像\n",
    "    img_path = os.path.join(cfg.OUTPUT_DIR, f\"confusion_matrix_{split_name}.png\")\n",
    "    plt.savefig(img_path, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"🎨 混淆矩阵图已保存: {img_path}\")\n",
    "    print(\"✅ 混淆矩阵生成完成!\")\n",
    "\n",
    "# 如果模型训练完成，进行可视化\n",
    "if 'trainer' in locals() and trainer is not None:\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"🎨 开始结果可视化\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # 可视化预测结果\n",
    "    if 'test' in available_splits:\n",
    "        visualize_predictions(\"tomato_test\", num_samples=3)\n",
    "    \n",
    "    # 生成混淆矩阵\n",
    "    if 'val' in available_splits:\n",
    "        generate_confusion_matrix(\"tomato_val\")\n",
    "    \n",
    "    print(\"✅ 可视化完成!\")\n",
    "\n",
    "print(\"\\n🎉 Detectron2语义实例分割训练流程完成!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad301eb-e358-4630-9490-c6b5089a7a06",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
