{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "727b72fb-b718-4ed3-8340-5376050147ed",
   "metadata": {},
   "source": [
    "# TomatoMAP-Cls Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fad99b1c-b47d-4386-bc62-433dda73b066",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-07T06:59:05.120408Z",
     "start_time": "2025-07-07T06:58:55.424375Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment checker:\n",
      "  PyTorch version: 2.7.1+cu126\n",
      "  CUDA version: True\n",
      "  GPU device: Tesla V100-PCIE-16GB\n",
      "  GPU ram: 15.8 GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import argparse\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from PIL import ImageDraw, ImageFont, Image\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import models\n",
    "from torchvision.models import (\n",
    "    MobileNet_V3_Large_Weights,\n",
    "    MobileNet_V3_Small_Weights,\n",
    "    MobileNet_V2_Weights,\n",
    "    ResNet18_Weights,\n",
    ")\n",
    "\n",
    "# env checker\n",
    "print(\"Environment checker:\")\n",
    "print(f\"  PyTorch version: {torch.__version__}\")\n",
    "print(f\"  CUDA version: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"  GPU device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"  GPU ram: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35beeeb4-ca9c-47b4-9893-7d2bf01a1894",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-07T07:00:42.814782Z",
     "start_time": "2025-07-07T07:00:42.797777Z"
    }
   },
   "outputs": [],
   "source": [
    "def save_model(model, path):\n",
    "    torch.save(model.state_dict(), path)\n",
    "    print(f\"model saved at: {path}\")\n",
    "\n",
    "def load_model(model, path, device='cpu'):\n",
    "    model.load_state_dict(torch.load(path, map_location=device))\n",
    "    model.eval()\n",
    "    print(f\"model loaded from: {path}\")\n",
    "\n",
    "def save_checkpoint(model, optimizer, epoch, path):\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict()\n",
    "    }, path)\n",
    "\n",
    "def load_checkpoint(path, model, optimizer, device):\n",
    "    checkpoint = torch.load(path, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    start_epoch = checkpoint['epoch'] + 1\n",
    "    print(f\"from epoch {start_epoch} re-training\")\n",
    "    return start_epoch\n",
    "\n",
    "def get_font(size=30, bold=False):\n",
    "    font_paths = [\n",
    "        \"C:/Windows/Fonts/arialbd.ttf\" if bold else \"C:/Windows/Fonts/arial.ttf\",\n",
    "        \"/usr/share/fonts/truetype/dejavu/DejaVuSans-Bold.ttf\" if bold else \"/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf\",\n",
    "        \"/System/Library/Fonts/Supplemental/Arial-Bold.ttf\" if bold else \"/System/Library/Fonts/Supplemental/Arial.ttf\",\n",
    "    ]\n",
    "    for path in font_paths:\n",
    "        try:\n",
    "            return ImageFont.truetype(path, size=size)\n",
    "        except:\n",
    "            continue\n",
    "    return ImageFont.load_default()\n",
    "\n",
    "def denormalize(img_tensor, mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)):\n",
    "    mean = torch.tensor(mean).view(3, 1, 1).to(img_tensor.device)\n",
    "    std = torch.tensor(std).view(3, 1, 1).to(img_tensor.device)\n",
    "    return torch.clamp(img_tensor * std + mean, 0, 1)\n",
    "\n",
    "def get_model(name, num_classes, pretrained=True):\n",
    "    print(f\"build model: {name}, class number: {num_classes}\")\n",
    "    \n",
    "    if name == 'mobilenet_v3_large':\n",
    "        weights = MobileNet_V3_Large_Weights.DEFAULT if pretrained else None\n",
    "        model = models.mobilenet_v3_large(weights=weights)\n",
    "        model.classifier[3] = nn.Linear(model.classifier[3].in_features, num_classes)\n",
    "    elif name == 'mobilenet_v3_small':\n",
    "        weights = MobileNet_V3_Small_Weights.DEFAULT if pretrained else None\n",
    "        model = models.mobilenet_v3_small(weights=weights)\n",
    "        model.classifier[3] = nn.Linear(model.classifier[3].in_features, num_classes)\n",
    "    elif name == 'mobilenet_v2':\n",
    "        weights = MobileNet_V2_Weights.DEFAULT if pretrained else None\n",
    "        model = models.mobilenet_v2(weights=weights)\n",
    "        model.classifier[1] = nn.Linear(model.classifier[1].in_features, num_classes)\n",
    "    elif name == 'resnet18':\n",
    "        weights = ResNet18_Weights.DEFAULT if pretrained else None\n",
    "        model = models.resnet18(weights=weights)\n",
    "        model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "    else:\n",
    "        raise ValueError(f\"Model {name} not supported.\")\n",
    "\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"parameter info: Total: {total_params:,}, Trainable: {trainable_params:,}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "class BBCHDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, data_dir, split='train', transform=None):\n",
    "        self.data_dir = os.path.join(data_dir, split)\n",
    "        self.transform = transform\n",
    "        \n",
    "        if not os.path.exists(self.data_dir):\n",
    "            raise FileNotFoundError(f\"Directory not found: {self.data_dir}\")\n",
    "        \n",
    "        # get all classes\n",
    "        self.classes = sorted([d for d in os.listdir(self.data_dir)\n",
    "                              if os.path.isdir(os.path.join(self.data_dir, d))])\n",
    "        self.class_to_idx = {cls: idx for idx, cls in enumerate(self.classes)}\n",
    "\n",
    "        self.samples = []\n",
    "        for class_name in self.classes:\n",
    "            class_dir = os.path.join(self.data_dir, class_name)\n",
    "            class_idx = self.class_to_idx[class_name]\n",
    "            \n",
    "            for img_name in os.listdir(class_dir):\n",
    "                if img_name.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                    img_path = os.path.join(class_dir, img_name)\n",
    "                    self.samples.append((img_path, class_idx))\n",
    "        \n",
    "        print(f\"loading {split} dataset: {len(self.samples)} images, {len(self.classes)} classes\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label = self.samples[idx]\n",
    "        \n",
    "        try:\n",
    "            image = Image.open(img_path).convert('RGB')\n",
    "        except Exception as e:\n",
    "            print(f\"failed to load image: {img_path}, error: {e}\")\n",
    "            image = Image.new('RGB', (224, 224), (0, 0, 0))\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label\n",
    "\n",
    "# data enhance\n",
    "def get_transforms(target_size=(640, 640)):\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.Resize(target_size),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.RandomRotation(10),\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    val_transform = transforms.Compose([\n",
    "        transforms.Resize(target_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    return train_transform, val_transform\n",
    "\n",
    "def get_dataloaders(data_dir, batch_size=32, target_size=(640, 640), num_workers=8, include_test=False):\n",
    "    print(f\"building dataloader: {data_dir}\")\n",
    "    \n",
    "    train_transform, val_transform = get_transforms(target_size)\n",
    "    \n",
    "    train_dataset = BBCHDataset(data_dir, 'train', train_transform)\n",
    "    val_dataset = BBCHDataset(data_dir, 'val', val_transform)\n",
    "\n",
    "    # for windows users\n",
    "    import platform\n",
    "    if platform.system() == 'Windows':\n",
    "        num_workers = 0\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, batch_size=batch_size, shuffle=True,\n",
    "        num_workers=num_workers, pin_memory=torch.cuda.is_available()\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset, batch_size=batch_size, shuffle=False,\n",
    "        num_workers=num_workers, pin_memory=torch.cuda.is_available()\n",
    "    )\n",
    "    \n",
    "    test_loader = None\n",
    "    if include_test:\n",
    "        test_dir = os.path.join(data_dir, 'test')\n",
    "        if os.path.exists(test_dir):\n",
    "            test_dataset = BBCHDataset(data_dir, 'test', val_transform)\n",
    "            test_loader = DataLoader(\n",
    "                test_dataset, batch_size=batch_size, shuffle=False,\n",
    "                num_workers=num_workers, pin_memory=torch.cuda.is_available()\n",
    "            )\n",
    "        else:\n",
    "            print(\"test set not found, using val as test\")\n",
    "            test_loader = val_loader\n",
    "\n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a707cf3-f677-4963-bff3-043bdc24f791",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-07T07:03:52.327730Z",
     "start_time": "2025-07-07T07:03:52.313731Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config:\n",
      "  data_dir: TomatoMAP/TomatoMAP-Cls\n",
      "  model_name: mobilenet_v3_large\n",
      "  num_classes: 50\n",
      "  batch_size: 32\n",
      "  num_epochs: 30\n",
      "  learning_rate: 0.0001\n",
      "  target_size: (640, 640)\n",
      "  patience: 3\n",
      "  save_interval: 20\n"
     ]
    }
   ],
   "source": [
    "CLASSIFICATION_CONFIG = {\n",
    "    'data_dir': 'TomatoMAP/TomatoMAP-Cls',\n",
    "    'model_name': 'mobilenet_v3_large',  # 'mobilenet_v3_small', 'mobilenet_v2', 'resnet18'\n",
    "    'num_classes': 50,\n",
    "    'batch_size': 32,\n",
    "    'num_epochs': 30,\n",
    "    'learning_rate': 1e-4,\n",
    "    'target_size': (640, 640),\n",
    "    'patience': 3,\n",
    "    'save_interval': 20\n",
    "}\n",
    "\n",
    "print(\"config:\")\n",
    "for key, value in CLASSIFICATION_CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b168c68-baaf-426d-b81d-7794e3c0a099",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TomatoMAP-Cls Trainer\n",
      "============================================================\n",
      "data founded at: TomatoMAP/TomatoMAP-Cls\n",
      "TomatoMAP-Cls is well structured.\n",
      "\n",
      " training config:\n",
      "   data_dir: TomatoMAP/TomatoMAP-Cls\n",
      "   model_name: mobilenet_v3_large\n",
      "   num_classes: 50\n",
      "   batch_size: 32\n",
      "   num_epochs: 30\n",
      "   learning_rate: 0.0001\n",
      "   target_size: (640, 640)\n",
      "   patience: 3\n",
      "   save_interval: 20\n",
      "\n",
      " training start.\n",
      "Using device: cuda\n",
      "building dataloader: TomatoMAP/TomatoMAP-Cls\n",
      "loading train dataset: 45099 images, 50 classes\n",
      "loading val dataset: 12870 images, 50 classes\n",
      "loading test dataset: 6495 images, 50 classes\n",
      "build model: mobilenet_v3_large, class number: 50\n",
      "parameter info: Total: 4,266,082, Trainable: 4,266,082\n",
      "Training start with 30 epoch(s),\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/30 [Train]: 100%|████████████████| 1410/1410 [06:55<00:00,  3.39it/s, Loss=1.8002, Acc=32.31%]\n",
      "Epoch 1/30 [Val]: 100%|████████████████████| 403/403 [01:00<00:00,  6.67it/s, Loss=1.8581, Acc=40.05%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch 1/30:\n",
      "  Train - Loss: 2.1123, Acc: 32.31%\n",
      "  Val - Loss: 1.6768, Acc: 40.05%\n",
      "  lr: 1.00e-04\n",
      "model saved at: cls/runs/mobilenet_v3_large_cls/best_mobilenet_v3_large.pth\n",
      "  best val acc: 40.05%\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/30 [Train]: 100%|████████████████| 1410/1410 [06:48<00:00,  3.45it/s, Loss=1.9883, Acc=42.30%]\n",
      "Epoch 2/30 [Val]: 100%|████████████████████| 403/403 [01:07<00:00,  5.99it/s, Loss=0.9373, Acc=45.84%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch 2/30:\n",
      "  Train - Loss: 1.6399, Acc: 42.30%\n",
      "  Val - Loss: 1.4930, Acc: 45.84%\n",
      "  lr: 1.00e-04\n",
      "model saved at: cls/runs/mobilenet_v3_large_cls/best_mobilenet_v3_large.pth\n",
      "  best val acc: 45.84%\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/30 [Train]: 100%|████████████████| 1410/1410 [06:52<00:00,  3.42it/s, Loss=1.7607, Acc=47.10%]\n",
      "Epoch 3/30 [Val]: 100%|████████████████████| 403/403 [01:00<00:00,  6.64it/s, Loss=0.9665, Acc=47.71%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch 3/30:\n",
      "  Train - Loss: 1.4578, Acc: 47.10%\n",
      "  Val - Loss: 1.4351, Acc: 47.71%\n",
      "  lr: 1.00e-04\n",
      "model saved at: cls/runs/mobilenet_v3_large_cls/best_mobilenet_v3_large.pth\n",
      "  best val acc: 47.71%\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/30 [Train]: 100%|████████████████| 1410/1410 [06:46<00:00,  3.47it/s, Loss=1.2591, Acc=51.96%]\n",
      "Epoch 4/30 [Val]: 100%|████████████████████| 403/403 [01:06<00:00,  6.06it/s, Loss=1.2630, Acc=53.02%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch 4/30:\n",
      "  Train - Loss: 1.3072, Acc: 51.96%\n",
      "  Val - Loss: 1.2602, Acc: 53.02%\n",
      "  lr: 1.00e-04\n",
      "model saved at: cls/runs/mobilenet_v3_large_cls/best_mobilenet_v3_large.pth\n",
      "  best val acc: 53.02%\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/30 [Train]: 100%|████████████████| 1410/1410 [06:54<00:00,  3.40it/s, Loss=1.0347, Acc=56.47%]\n",
      "Epoch 5/30 [Val]: 100%|████████████████████| 403/403 [01:00<00:00,  6.63it/s, Loss=1.0445, Acc=54.39%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch 5/30:\n",
      "  Train - Loss: 1.1802, Acc: 56.47%\n",
      "  Val - Loss: 1.2064, Acc: 54.39%\n",
      "  lr: 1.00e-04\n",
      "model saved at: cls/runs/mobilenet_v3_large_cls/best_mobilenet_v3_large.pth\n",
      "  best val acc: 54.39%\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/30 [Train]: 100%|████████████████| 1410/1410 [06:57<00:00,  3.38it/s, Loss=0.8705, Acc=60.32%]\n",
      "Epoch 6/30 [Val]: 100%|████████████████████| 403/403 [01:02<00:00,  6.47it/s, Loss=0.5300, Acc=58.96%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch 6/30:\n",
      "  Train - Loss: 1.0670, Acc: 60.32%\n",
      "  Val - Loss: 1.0961, Acc: 58.96%\n",
      "  lr: 1.00e-04\n",
      "model saved at: cls/runs/mobilenet_v3_large_cls/best_mobilenet_v3_large.pth\n",
      "  best val acc: 58.96%\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/30 [Train]: 100%|████████████████| 1410/1410 [06:46<00:00,  3.47it/s, Loss=0.9907, Acc=64.10%]\n",
      "Epoch 7/30 [Val]: 100%|████████████████████| 403/403 [01:04<00:00,  6.29it/s, Loss=0.5100, Acc=60.51%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch 7/30:\n",
      "  Train - Loss: 0.9635, Acc: 64.10%\n",
      "  Val - Loss: 1.0496, Acc: 60.51%\n",
      "  lr: 1.00e-04\n",
      "model saved at: cls/runs/mobilenet_v3_large_cls/best_mobilenet_v3_large.pth\n",
      "  best val acc: 60.51%\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/30 [Train]: 100%|████████████████| 1410/1410 [06:42<00:00,  3.50it/s, Loss=1.0332, Acc=67.50%]\n",
      "Epoch 8/30 [Val]: 100%|████████████████████| 403/403 [01:00<00:00,  6.68it/s, Loss=0.4854, Acc=62.77%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch 8/30:\n",
      "  Train - Loss: 0.8680, Acc: 67.50%\n",
      "  Val - Loss: 0.9902, Acc: 62.77%\n",
      "  lr: 1.00e-04\n",
      "model saved at: cls/runs/mobilenet_v3_large_cls/best_mobilenet_v3_large.pth\n",
      "  best val acc: 62.77%\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/30 [Train]: 100%|████████████████| 1410/1410 [06:48<00:00,  3.45it/s, Loss=0.8735, Acc=70.62%]\n",
      "Epoch 9/30 [Val]: 100%|████████████████████| 403/403 [01:01<00:00,  6.58it/s, Loss=0.3427, Acc=64.92%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch 9/30:\n",
      "  Train - Loss: 0.7869, Acc: 70.62%\n",
      "  Val - Loss: 0.9535, Acc: 64.92%\n",
      "  lr: 1.00e-04\n",
      "model saved at: cls/runs/mobilenet_v3_large_cls/best_mobilenet_v3_large.pth\n",
      "  best val acc: 64.92%\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/30 [Train]: 100%|███████████████| 1410/1410 [06:53<00:00,  3.41it/s, Loss=0.5824, Acc=73.65%]\n",
      "Epoch 10/30 [Val]: 100%|███████████████████| 403/403 [01:01<00:00,  6.55it/s, Loss=0.1160, Acc=66.73%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch 10/30:\n",
      "  Train - Loss: 0.7029, Acc: 73.65%\n",
      "  Val - Loss: 0.8825, Acc: 66.73%\n",
      "  lr: 1.00e-04\n",
      "model saved at: cls/runs/mobilenet_v3_large_cls/best_mobilenet_v3_large.pth\n",
      "  best val acc: 66.73%\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/30 [Train]: 100%|███████████████| 1410/1410 [07:00<00:00,  3.36it/s, Loss=0.6338, Acc=76.41%]\n",
      "Epoch 11/30 [Val]: 100%|███████████████████| 403/403 [01:00<00:00,  6.68it/s, Loss=0.9800, Acc=70.13%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch 11/30:\n",
      "  Train - Loss: 0.6342, Acc: 76.41%\n",
      "  Val - Loss: 0.8193, Acc: 70.13%\n",
      "  lr: 1.00e-04\n",
      "model saved at: cls/runs/mobilenet_v3_large_cls/best_mobilenet_v3_large.pth\n",
      "  best val acc: 70.13%\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/30 [Train]: 100%|███████████████| 1410/1410 [06:59<00:00,  3.36it/s, Loss=0.6373, Acc=78.85%]\n",
      "Epoch 12/30 [Val]: 100%|███████████████████| 403/403 [00:59<00:00,  6.74it/s, Loss=0.5220, Acc=69.18%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch 12/30:\n",
      "  Train - Loss: 0.5705, Acc: 78.85%\n",
      "  Val - Loss: 0.8643, Acc: 69.18%\n",
      "  lr: 1.00e-04\n",
      "  val acc not raised (1/3)\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/30 [Train]: 100%|███████████████| 1410/1410 [06:56<00:00,  3.38it/s, Loss=0.4635, Acc=80.82%]\n",
      "Epoch 13/30 [Val]: 100%|███████████████████| 403/403 [00:59<00:00,  6.76it/s, Loss=0.4900, Acc=70.23%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch 13/30:\n",
      "  Train - Loss: 0.5188, Acc: 80.82%\n",
      "  Val - Loss: 0.8389, Acc: 70.23%\n",
      "  lr: 1.00e-04\n",
      "model saved at: cls/runs/mobilenet_v3_large_cls/best_mobilenet_v3_large.pth\n",
      "  best val acc: 70.23%\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/30 [Train]: 100%|███████████████| 1410/1410 [06:58<00:00,  3.37it/s, Loss=0.8183, Acc=82.79%]\n",
      "Epoch 14/30 [Val]: 100%|███████████████████| 403/403 [01:01<00:00,  6.53it/s, Loss=0.1427, Acc=72.19%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch 14/30:\n",
      "  Train - Loss: 0.4690, Acc: 82.79%\n",
      "  Val - Loss: 0.7842, Acc: 72.19%\n",
      "  lr: 1.00e-04\n",
      "model saved at: cls/runs/mobilenet_v3_large_cls/best_mobilenet_v3_large.pth\n",
      "  best val acc: 72.19%\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/30 [Train]: 100%|███████████████| 1410/1410 [06:41<00:00,  3.51it/s, Loss=0.6168, Acc=84.51%]\n",
      "Epoch 15/30 [Val]: 100%|███████████████████| 403/403 [01:00<00:00,  6.67it/s, Loss=0.0423, Acc=73.65%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch 15/30:\n",
      "  Train - Loss: 0.4207, Acc: 84.51%\n",
      "  Val - Loss: 0.7464, Acc: 73.65%\n",
      "  lr: 1.00e-04\n",
      "model saved at: cls/runs/mobilenet_v3_large_cls/best_mobilenet_v3_large.pth\n",
      "  best val acc: 73.65%\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/30 [Train]: 100%|███████████████| 1410/1410 [07:09<00:00,  3.29it/s, Loss=0.3393, Acc=86.13%]\n",
      "Epoch 16/30 [Val]: 100%|███████████████████| 403/403 [01:01<00:00,  6.59it/s, Loss=0.0441, Acc=74.16%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch 16/30:\n",
      "  Train - Loss: 0.3839, Acc: 86.13%\n",
      "  Val - Loss: 0.7422, Acc: 74.16%\n",
      "  lr: 1.00e-04\n",
      "model saved at: cls/runs/mobilenet_v3_large_cls/best_mobilenet_v3_large.pth\n",
      "  best val acc: 74.16%\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/30 [Train]: 100%|███████████████| 1410/1410 [06:46<00:00,  3.47it/s, Loss=0.8132, Acc=87.79%]\n",
      "Epoch 17/30 [Val]: 100%|███████████████████| 403/403 [01:01<00:00,  6.60it/s, Loss=0.0240, Acc=73.03%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch 17/30:\n",
      "  Train - Loss: 0.3403, Acc: 87.79%\n",
      "  Val - Loss: 0.8209, Acc: 73.03%\n",
      "  lr: 1.00e-04\n",
      "  val acc not raised (1/3)\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/30 [Train]: 100%|███████████████| 1410/1410 [06:44<00:00,  3.49it/s, Loss=1.2770, Acc=88.62%]\n",
      "Epoch 18/30 [Val]: 100%|███████████████████| 403/403 [01:00<00:00,  6.67it/s, Loss=0.0083, Acc=75.19%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch 18/30:\n",
      "  Train - Loss: 0.3198, Acc: 88.62%\n",
      "  Val - Loss: 0.7634, Acc: 75.19%\n",
      "  lr: 1.00e-04\n",
      "model saved at: cls/runs/mobilenet_v3_large_cls/best_mobilenet_v3_large.pth\n",
      "  best val acc: 75.19%\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/30 [Train]: 100%|███████████████| 1410/1410 [06:47<00:00,  3.46it/s, Loss=0.3210, Acc=89.24%]\n",
      "Epoch 19/30 [Val]: 100%|███████████████████| 403/403 [01:00<00:00,  6.68it/s, Loss=0.2833, Acc=75.68%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch 19/30:\n",
      "  Train - Loss: 0.2938, Acc: 89.24%\n",
      "  Val - Loss: 0.7364, Acc: 75.68%\n",
      "  lr: 1.00e-04\n",
      "model saved at: cls/runs/mobilenet_v3_large_cls/best_mobilenet_v3_large.pth\n",
      "  best val acc: 75.68%\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/30 [Train]: 100%|███████████████| 1410/1410 [06:43<00:00,  3.50it/s, Loss=0.4446, Acc=90.42%]\n",
      "Epoch 20/30 [Val]: 100%|███████████████████| 403/403 [01:01<00:00,  6.57it/s, Loss=0.0088, Acc=74.93%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch 20/30:\n",
      "  Train - Loss: 0.2635, Acc: 90.42%\n",
      "  Val - Loss: 0.7816, Acc: 74.93%\n",
      "  lr: 1.00e-04\n",
      "  val acc not raised (1/3)\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/30 [Train]: 100%|███████████████| 1410/1410 [06:48<00:00,  3.45it/s, Loss=0.2350, Acc=91.08%]\n",
      "Epoch 21/30 [Val]: 100%|███████████████████| 403/403 [01:03<00:00,  6.33it/s, Loss=0.7884, Acc=76.23%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch 21/30:\n",
      "  Train - Loss: 0.2497, Acc: 91.08%\n",
      "  Val - Loss: 0.7626, Acc: 76.23%\n",
      "  lr: 1.00e-04\n",
      "model saved at: cls/runs/mobilenet_v3_large_cls/best_mobilenet_v3_large.pth\n",
      "  best val acc: 76.23%\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/30 [Train]: 100%|███████████████| 1410/1410 [06:45<00:00,  3.48it/s, Loss=0.4563, Acc=92.02%]\n",
      "Epoch 22/30 [Val]: 100%|███████████████████| 403/403 [01:00<00:00,  6.64it/s, Loss=0.0347, Acc=75.87%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch 22/30:\n",
      "  Train - Loss: 0.2265, Acc: 92.02%\n",
      "  Val - Loss: 0.8087, Acc: 75.87%\n",
      "  lr: 1.00e-04\n",
      "  val acc not raised (1/3)\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/30 [Train]: 100%|███████████████| 1410/1410 [07:10<00:00,  3.27it/s, Loss=0.3957, Acc=92.44%]\n",
      "Epoch 23/30 [Val]: 100%|███████████████████| 403/403 [01:06<00:00,  6.06it/s, Loss=0.2525, Acc=76.99%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch 23/30:\n",
      "  Train - Loss: 0.2128, Acc: 92.44%\n",
      "  Val - Loss: 0.7541, Acc: 76.99%\n",
      "  lr: 1.00e-04\n",
      "model saved at: cls/runs/mobilenet_v3_large_cls/best_mobilenet_v3_large.pth\n",
      "  best val acc: 76.99%\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/30 [Train]: 100%|███████████████| 1410/1410 [06:52<00:00,  3.42it/s, Loss=0.1079, Acc=92.88%]\n",
      "Epoch 24/30 [Val]: 100%|███████████████████| 403/403 [01:05<00:00,  6.11it/s, Loss=0.0584, Acc=77.33%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch 24/30:\n",
      "  Train - Loss: 0.2004, Acc: 92.88%\n",
      "  Val - Loss: 0.7255, Acc: 77.33%\n",
      "  lr: 1.00e-04\n",
      "model saved at: cls/runs/mobilenet_v3_large_cls/best_mobilenet_v3_large.pth\n",
      "  best val acc: 77.33%\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/30 [Train]: 100%|███████████████| 1410/1410 [06:45<00:00,  3.48it/s, Loss=0.3559, Acc=93.45%]\n",
      "Epoch 25/30 [Val]: 100%|███████████████████| 403/403 [01:04<00:00,  6.23it/s, Loss=0.0602, Acc=77.47%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch 25/30:\n",
      "  Train - Loss: 0.1869, Acc: 93.45%\n",
      "  Val - Loss: 0.7610, Acc: 77.47%\n",
      "  lr: 1.00e-04\n",
      "model saved at: cls/runs/mobilenet_v3_large_cls/best_mobilenet_v3_large.pth\n",
      "  best val acc: 77.47%\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26/30 [Train]: 100%|███████████████| 1410/1410 [06:46<00:00,  3.47it/s, Loss=0.2543, Acc=93.95%]\n",
      "Epoch 26/30 [Val]: 100%|███████████████████| 403/403 [01:01<00:00,  6.58it/s, Loss=0.0157, Acc=78.09%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch 26/30:\n",
      "  Train - Loss: 0.1716, Acc: 93.95%\n",
      "  Val - Loss: 0.7465, Acc: 78.09%\n",
      "  lr: 1.00e-04\n",
      "model saved at: cls/runs/mobilenet_v3_large_cls/best_mobilenet_v3_large.pth\n",
      "  best val acc: 78.09%\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27/30 [Train]: 100%|███████████████| 1410/1410 [06:47<00:00,  3.46it/s, Loss=0.0405, Acc=94.37%]\n",
      "Epoch 27/30 [Val]: 100%|███████████████████| 403/403 [00:59<00:00,  6.73it/s, Loss=0.2581, Acc=77.28%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch 27/30:\n",
      "  Train - Loss: 0.1627, Acc: 94.37%\n",
      "  Val - Loss: 0.7966, Acc: 77.28%\n",
      "  lr: 1.00e-04\n",
      "  val acc not raised (1/3)\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28/30 [Train]: 100%|███████████████| 1410/1410 [06:44<00:00,  3.48it/s, Loss=0.4891, Acc=94.52%]\n",
      "Epoch 28/30 [Val]: 100%|███████████████████| 403/403 [01:00<00:00,  6.62it/s, Loss=0.0969, Acc=79.19%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch 28/30:\n",
      "  Train - Loss: 0.1564, Acc: 94.52%\n",
      "  Val - Loss: 0.7067, Acc: 79.19%\n",
      "  lr: 1.00e-04\n",
      "model saved at: cls/runs/mobilenet_v3_large_cls/best_mobilenet_v3_large.pth\n",
      "  best val acc: 79.19%\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29/30 [Train]: 100%|███████████████| 1410/1410 [06:52<00:00,  3.41it/s, Loss=0.4975, Acc=94.85%]\n",
      "Epoch 29/30 [Val]: 100%|███████████████████| 403/403 [01:04<00:00,  6.28it/s, Loss=0.2542, Acc=77.54%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch 29/30:\n",
      "  Train - Loss: 0.1497, Acc: 94.85%\n",
      "  Val - Loss: 0.8152, Acc: 77.54%\n",
      "  lr: 1.00e-04\n",
      "  val acc not raised (1/3)\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30/30 [Train]: 100%|███████████████| 1410/1410 [06:47<00:00,  3.46it/s, Loss=0.3944, Acc=95.06%]\n",
      "Epoch 30/30 [Val]: 100%|███████████████████| 403/403 [01:00<00:00,  6.68it/s, Loss=0.1726, Acc=79.02%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch 30/30:\n",
      "  Train - Loss: 0.1404, Acc: 95.06%\n",
      "  Val - Loss: 0.7246, Acc: 79.02%\n",
      "  lr: 1.00e-05\n",
      "  val acc not raised (2/3)\n",
      "------------------------------------------------------------\n",
      "model saved at: cls/runs/mobilenet_v3_large_cls/final_mobilenet_v3_large.pth\n",
      "\n",
      " TomatoMAP-Cls is trained!\n",
      "  best val acc: 79.19%\n",
      "  model saved at: cls/runs/mobilenet_v3_large_cls\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x500 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " training log saved at: cls/runs/mobilenet_v3_large_cls/training_history.csv\n",
      "\n",
      "============================================================\n",
      "\n",
      " training finished!\n",
      "   best val acc is: 79.19%\n",
      "   model saved at: cls/runs/mobilenet_v3_large_cls\n",
      "\n",
      " evaluating on test subset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "evaluating: 100%|███████████████████████████████████████| 203/203 [00:31<00:00,  6.42it/s, Acc=79.01%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " test subset acc: 79.01%\n",
      "\n",
      " building confusion matrix\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 310x310 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      " Evaluation results:\n",
      "   best val acc: 79.19%\n",
      "   test acc: 79.01%\n",
      "   class number: 50\n",
      "   test data size: 6495\n",
      "   results saved at: cls/runs/mobilenet_v3_large_cls\n",
      "   GGWP!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "def train_model(config):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    output_dir = Path(f\"cls/runs/{config['model_name']}_cls\")\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    train_loader, val_loader, test_loader = get_dataloaders(\n",
    "        config['data_dir'], \n",
    "        batch_size=config['batch_size'],\n",
    "        target_size=config['target_size'],\n",
    "        num_workers=8,\n",
    "        include_test=True\n",
    "    )\n",
    "    \n",
    "    model = get_model(config['model_name'], config['num_classes'], pretrained=True)\n",
    "    model = model.to(device)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=config['learning_rate'], weight_decay=0.01)\n",
    "    \n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_accuracies = []\n",
    "    val_accuracies = []\n",
    "    \n",
    "    best_val_acc = 0.0\n",
    "    patience_counter = 0\n",
    "    \n",
    "    print(f\"Training start with {config['num_epochs']} epoch(s),\")\n",
    "    \n",
    "    for epoch in range(config['num_epochs']):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        \n",
    "        train_pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{config['num_epochs']} [Train]\")\n",
    "        \n",
    "        for batch_idx, (images, labels) in enumerate(train_pbar):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            train_total += labels.size(0)\n",
    "            train_correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            current_acc = 100 * train_correct / train_total\n",
    "            train_pbar.set_postfix({\n",
    "                'Loss': f'{loss.item():.4f}',\n",
    "                'Acc': f'{current_acc:.2f}%'\n",
    "            })\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            val_pbar = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{config['num_epochs']} [Val]\")\n",
    "            \n",
    "            for images, labels in val_pbar:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += (predicted == labels).sum().item()\n",
    "                \n",
    "                current_acc = 100 * val_correct / val_total\n",
    "                val_pbar.set_postfix({\n",
    "                    'Loss': f'{loss.item():.4f}',\n",
    "                    'Acc': f'{current_acc:.2f}%'\n",
    "                })\n",
    "        \n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        train_acc = 100 * train_correct / train_total\n",
    "        val_acc = 100 * val_correct / val_total\n",
    "        \n",
    "        train_losses.append(avg_train_loss)\n",
    "        val_losses.append(avg_val_loss)\n",
    "        train_accuracies.append(train_acc)\n",
    "        val_accuracies.append(val_acc)\n",
    "        \n",
    "        scheduler.step()\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        \n",
    "        print(f\"\\n Epoch {epoch+1}/{config['num_epochs']}:\")\n",
    "        print(f\"  Train - Loss: {avg_train_loss:.4f}, Acc: {train_acc:.2f}%\")\n",
    "        print(f\"  Val - Loss: {avg_val_loss:.4f}, Acc: {val_acc:.2f}%\")\n",
    "        print(f\"  lr: {current_lr:.2e}\")\n",
    "        \n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_model_path = output_dir / f\"best_{config['model_name']}.pth\"\n",
    "            save_model(model, best_model_path)\n",
    "            patience_counter = 0\n",
    "            print(f\"  best val acc: {best_val_acc:.2f}%\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f\"  val acc not raised ({patience_counter}/{config['patience']})\")\n",
    "        \n",
    "        if (epoch + 1) % config['save_interval'] == 0:\n",
    "            checkpoint_path = output_dir / f\"checkpoint_epoch_{epoch+1}.pth\"\n",
    "            save_checkpoint(model, optimizer, epoch, checkpoint_path)\n",
    "        \n",
    "        if patience_counter >= config['patience']:\n",
    "            print(f\"\\n Trigger early stop. Val acc has {config['patience']} epochs no improve\")\n",
    "            break\n",
    "        \n",
    "        print(\"-\" * 60)\n",
    "    \n",
    "    final_model_path = output_dir / f\"final_{config['model_name']}.pth\"\n",
    "    save_model(model, final_model_path)\n",
    "    \n",
    "    print(f\"\\n TomatoMAP-Cls is trained!\")\n",
    "    print(f\"  best val acc: {best_val_acc:.2f}%\")\n",
    "    print(f\"  model saved at: {output_dir}\")\n",
    "    \n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.plot(train_losses, label='train loss', color='blue')\n",
    "    plt.plot(val_losses, label='val loss', color='red')\n",
    "    plt.title('training loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.plot(train_accuracies, label='train acc', color='blue')\n",
    "    plt.plot(val_accuracies, label='val acc', color='red')\n",
    "    plt.title('training acc')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.subplot(1, 3, 3)\n",
    "    lrs = []\n",
    "    for i in range(len(train_losses)):\n",
    "        if i < 30:\n",
    "            lrs.append(config['learning_rate'])\n",
    "        elif i < 60:\n",
    "            lrs.append(config['learning_rate'] * 0.1)\n",
    "        else:\n",
    "            lrs.append(config['learning_rate'] * 0.01)\n",
    "    plt.plot(lrs, label='lr', color='green')\n",
    "    plt.title('lr changes')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Learning Rate')\n",
    "    plt.yscale('log')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_dir / 'training_curves.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    history_df = pd.DataFrame({\n",
    "        'epoch': range(1, len(train_losses) + 1),\n",
    "        'train_loss': train_losses,\n",
    "        'val_loss': val_losses,\n",
    "        'train_acc': train_accuracies,\n",
    "        'val_acc': val_accuracies\n",
    "    })\n",
    "    history_df.to_csv(output_dir / 'training_history.csv', index=False)\n",
    "    print(f\" training log saved at: {output_dir / 'training_history.csv'}\")\n",
    "    \n",
    "    return model, best_val_acc, output_dir, test_loader\n",
    "\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"TomatoMAP-Cls Trainer\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if not os.path.exists(CLASSIFICATION_CONFIG['data_dir']):\n",
    "    print(f\"dataset not exist\")\n",
    "    print(f\"   path: {CLASSIFICATION_CONFIG['data_dir']}\")\n",
    "    print(f\"   please check data structure\")\n",
    "else:\n",
    "    print(f\"data founded at: {CLASSIFICATION_CONFIG['data_dir']}\")\n",
    "    \n",
    "    train_dir = os.path.join(CLASSIFICATION_CONFIG['data_dir'], 'train')\n",
    "    val_dir = os.path.join(CLASSIFICATION_CONFIG['data_dir'], 'val')\n",
    "    test_dir = os.path.join(CLASSIFICATION_CONFIG['data_dir'], 'test')\n",
    "    \n",
    "    if not os.path.exists(train_dir):\n",
    "        print(f\"training subset not exist: {train_dir}\")\n",
    "    elif not os.path.exists(val_dir):\n",
    "        print(f\"val subset not exist: {val_dir}\")\n",
    "    elif not os.path.exists(test_dir):\n",
    "        print(f\"test subset not exist: {test_dir}\")\n",
    "        print(f\"   using val subset for test\")\n",
    "    else:\n",
    "        print(f\"TomatoMAP-Cls is well structured.\")\n",
    "        \n",
    "        print(\"\\n training config:\")\n",
    "        for key, value in CLASSIFICATION_CONFIG.items():\n",
    "            print(f\"   {key}: {value}\")\n",
    "        \n",
    "        print(\"\\n training start.\")\n",
    "        \n",
    "        try:\n",
    "            model, best_acc, output_dir, test_loader = train_model(CLASSIFICATION_CONFIG)\n",
    "            \n",
    "            print(\"\\n\" + \"=\" * 60)\n",
    "            print(\"\\n training finished!\")\n",
    "            print(f\"   best val acc is: {best_acc:.2f}%\")\n",
    "            print(f\"   model saved at: {output_dir}\")\n",
    "            \n",
    "            print(\"\\n evaluating on test subset...\")\n",
    "            model.eval()\n",
    "            test_correct = 0\n",
    "            test_total = 0\n",
    "            test_predictions = []\n",
    "            test_labels = []\n",
    "            \n",
    "            device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                test_pbar = tqdm(test_loader, desc=\"evaluating\")\n",
    "                for images, labels in test_pbar:\n",
    "                    images, labels = images.to(device), labels.to(device)\n",
    "                    outputs = model(images)\n",
    "                    _, predicted = torch.max(outputs.data, 1)\n",
    "                    \n",
    "                    test_total += labels.size(0)\n",
    "                    test_correct += (predicted == labels).sum().item()\n",
    "\n",
    "                    test_predictions.extend(predicted.cpu().numpy())\n",
    "                    test_labels.extend(labels.cpu().numpy())\n",
    "                    \n",
    "                    current_acc = 100 * test_correct / test_total\n",
    "                    test_pbar.set_postfix({'Acc': f'{current_acc:.2f}%'})\n",
    "            \n",
    "            test_accuracy = 100 * test_correct / test_total\n",
    "            print(f\" test subset acc: {test_accuracy:.2f}%\")\n",
    "\n",
    "            print(\"\\n building confusion matrix\")\n",
    "            \n",
    "            train_dataset = test_loader.dataset\n",
    "            class_names = train_dataset.classes\n",
    "            \n",
    "            cm = confusion_matrix(test_labels, test_predictions)\n",
    "            \n",
    "            cm_df = pd.DataFrame(cm, index=class_names, columns=class_names)\n",
    "            cm_df.to_csv(output_dir / 'confusion_matrix.csv')\n",
    "\n",
    "            normalized_cm = cm_df.div(cm_df.sum(axis=1), axis=0).fillna(0)\n",
    "            \n",
    "            matrix = normalized_cm.T.to_numpy()\n",
    "            \n",
    "            from matplotlib import rcParams\n",
    "            # rcParams['font.family'] = 'Calibri' # Ubuntu doesn't own this when training on ubuntu VM\n",
    "            rcParams['font.size'] = 8\n",
    "            \n",
    "            masked_matrix = np.ma.masked_where(matrix == 0, matrix)\n",
    "            \n",
    "            from matplotlib.colors import Normalize\n",
    "            cmap = plt.cm.jet\n",
    "            cmap.set_bad(color='white')\n",
    "            norm = Normalize(vmin=0.1, vmax=1)\n",
    "            \n",
    "            fig_width_in = 3.1\n",
    "            fig_height_in = fig_width_in\n",
    "            fig, ax = plt.subplots(figsize=(fig_width_in, fig_height_in))\n",
    "            \n",
    "            im = ax.imshow(masked_matrix, cmap=cmap, norm=norm)\n",
    "\n",
    "            # For further process for publishing purpose, labels are removed :)\n",
    "            ax.set_xlabel(\"\")\n",
    "            ax.set_ylabel(\"\")\n",
    "            \n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.savefig(output_dir / 'normalized_confusion_matrix.png', format='png', dpi=300)\n",
    "            plt.show()\n",
    "            \n",
    "            # plt.figure(figsize=(12, 10))\n",
    "            # disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)\n",
    "            # disp.plot(cmap='Blues', values_format='d')\n",
    "            # plt.title(f'Detailed Confusion Matrix (test acc: {test_accuracy:.2f}%)', fontsize=8)\n",
    "            # plt.xticks(rotation=45, ha='right')\n",
    "            # plt.yticks(rotation=0)\n",
    "            # plt.tight_layout()\n",
    "            # plt.savefig(output_dir / 'detailed_confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
    "            # plt.show()\n",
    "            \n",
    "            test_results = {\n",
    "                'test_accuracy': test_accuracy,\n",
    "                'total_samples': test_total,\n",
    "                'correct_predictions': test_correct,\n",
    "                'num_classes': len(class_names),\n",
    "                'class_names': class_names\n",
    "            }\n",
    "            \n",
    "            import json\n",
    "            with open(output_dir / 'test_results.json', 'w', encoding='utf-8') as f:\n",
    "                json.dump(test_results, f, indent=2, ensure_ascii=False)\n",
    "            \n",
    "            print(\"\\n\" + \"=\" * 60)\n",
    "            print(\" Evaluation results:\")\n",
    "            print(f\"   best val acc: {best_acc:.2f}%\")\n",
    "            print(f\"   test acc: {test_accuracy:.2f}%\")\n",
    "            print(f\"   class number: {len(class_names)}\")\n",
    "            print(f\"   test data size: {test_total}\")\n",
    "            print(f\"   results saved at: {output_dir}\")\n",
    "            print(f\"   GGWP!\")\n",
    "            print(\"=\" * 60)\n",
    "            \n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\n training interruptted\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\n error during training:\")\n",
    "            print(f\"   error info: {str(e)}\")\n",
    "            print(\"\\nDetails:\")\n",
    "            import traceback\n",
    "            traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "270bc778-324d-46cf-aec4-37f9b4cd6cde",
   "metadata": {},
   "source": [
    "# TomatoMAP-Det Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "517ad488-17f7-44d5-aa3d-0b23e131d49a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.63 🚀 Python-3.10.0 torch-2.7.1+cu126 CUDA:0 (Tesla V100-PCIE-16GB, 16144MiB)\n",
      "Setup complete ✅ (24 CPUs, 113.0 GB RAM, 117.4/145.2 GB disk)\n",
      "/home/ubuntu/project/EoC/code/det/ultralytics/__init__.py\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "from ultralytics import RTDETR\n",
    "import torch\n",
    "# using proper libiary?\n",
    "import ultralytics\n",
    "ultralytics.checks()\n",
    "print(ultralytics.__file__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3872b57-2251-4d4d-a179-ff0c7d312236",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "TomatoMAP-Det Trainer\n",
      "\n",
      "============================================================\n",
      "downloading pretrained model: \n",
      "Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11l.pt to 'yolo11l.pt'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 49.0M/49.0M [00:01<00:00, 32.8MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model info: \n",
      "New https://pypi.org/project/ultralytics/8.3.167 available 😃 Update with 'pip install -U ultralytics'\n",
      "Ultralytics 8.3.63 🚀 Python-3.10.0 torch-2.7.1+cu126 CUDA:0 (Tesla V100-PCIE-16GB, 16144MiB)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolo11l.pt, data=det/TomatoMAP-Det.yaml, epochs=500, time=None, patience=10, batch=4, imgsz=640, save=True, save_period=-1, cache=False, device=[0], workers=8, project=det/output, name=train, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.00089, lrf=0.009, momentum=0.6, weight_decay=0.00016, warmup_epochs=7.22893, warmup_momentum=0.6, warmup_bias_lr=0.0, box=5.45995, cls=0.4119, dfl=1.0, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.0181, hsv_s=0.27375, hsv_v=0.06764, degrees=0.0, translate=0.14088, scale=0.06371, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.02976, bgr=0.0, mosaic=0.8, mixup=0.0, copy_paste=0.1, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=det/best_hyperparameters.yaml, tracker=botsort.yaml, save_dir=det/output/train\n",
      "Overriding model.yaml nc=80 with nc=7\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1      1856  ultralytics.nn.modules.conv.Conv             [3, 64, 3, 2]                 \n",
      "  1                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  2                  -1  2    173824  ultralytics.nn.modules.block.C3k2            [128, 256, 2, True, 0.25]     \n",
      "  3                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
      "  4                  -1  2    691712  ultralytics.nn.modules.block.C3k2            [256, 512, 2, True, 0.25]     \n",
      "  5                  -1  1   2360320  ultralytics.nn.modules.conv.Conv             [512, 512, 3, 2]              \n",
      "  6                  -1  2   2234368  ultralytics.nn.modules.block.C3k2            [512, 512, 2, True]           \n",
      "  7                  -1  1   2360320  ultralytics.nn.modules.conv.Conv             [512, 512, 3, 2]              \n",
      "  8                  -1  2   2234368  ultralytics.nn.modules.block.C3k2            [512, 512, 2, True]           \n",
      "  9                  -1  1    656896  ultralytics.nn.modules.block.SPPF            [512, 512, 5]                 \n",
      " 10                  -1  2   1455616  ultralytics.nn.modules.block.C2PSA           [512, 512, 2]                 \n",
      " 11                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 12             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 13                  -1  2   2496512  ultralytics.nn.modules.block.C3k2            [1024, 512, 2, True]          \n",
      " 14                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 15             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 16                  -1  2    756736  ultralytics.nn.modules.block.C3k2            [1024, 256, 2, True]          \n",
      " 17                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
      " 18            [-1, 13]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 19                  -1  2   2365440  ultralytics.nn.modules.block.C3k2            [768, 512, 2, True]           \n",
      " 20                  -1  1   2360320  ultralytics.nn.modules.conv.Conv             [512, 512, 3, 2]              \n",
      " 21            [-1, 10]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 22                  -1  2   2496512  ultralytics.nn.modules.block.C3k2            [1024, 512, 2, True]          \n",
      " 23        [16, 19, 22]  1   1416421  ultralytics.nn.modules.head.Detect           [7, [256, 512, 512]]          \n",
      "YOLO11l summary: 631 layers, 25,315,877 parameters, 25,315,861 gradients, 87.3 GFLOPs\n",
      "\n",
      "Transferred 1009/1015 items from pretrained weights\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir det/output/train', view at http://localhost:6006/\n",
      "Freezing layer 'model.23.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
      "Downloading https://github.com/sunsmarterjie/yolov12/releases/download/turbo/yolov12n.pt to 'yolov12n.pt'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 5.26M/5.26M [00:00<00:00, 30.5MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /home/ubuntu/project/EoC/code/TomatoMAP/TomatoMAP-Det/labels/test.cache... 64463 images, 1 backgrounds, \u001b[0m\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/ubuntu/project/EoC/code/TomatoMAP/TomatoMAP-Det/labels/test.cache... 64463 images, 1 backgrounds, 0 \u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to det/output/train/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.00089' and 'momentum=0.6' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01, momentum=0.9) with parameter groups 167 weight(decay=0.0), 174 weight(decay=0.00016), 173 bias(decay=0.0)\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added ✅\n",
      "Image sizes 640 train, 640 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1mdet/output/train\u001b[0m\n",
      "Starting training for 500 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      1/500      3.34G     0.7181     0.9323      0.937         29        640: 100%|██████████| 16116/16116 [41:42<00:00\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 8058/8058 [09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all      64464     583417      0.818      0.495      0.527      0.391\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      2/500      3.71G     0.6265     0.6749     0.8621         48        640: 100%|██████████| 16116/16116 [39:27<00:00\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 8058/8058 [08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all      64464     583417      0.587      0.568      0.578      0.431\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      3/500      3.68G     0.6143     0.6485     0.8528         49        640:  41%|████      | 6558/16116 [15:46<22:59,\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 13\u001b[0m\n\u001b[1;32m      9\u001b[0m model \u001b[38;5;241m=\u001b[39m YOLO(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myolo11l.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel info: \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 13\u001b[0m train_result \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdet/TomatoMAP-Det.yaml\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimgsz\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m640\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproject\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdet/output\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdet/best_hyperparameters.yaml\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# fine-tuned hyperparameters, ready to use, details please contact us per email\u001b[39;49;00m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m#profile=True,\u001b[39;49;00m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mplots\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m     24\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/project/EoC/code/det/ultralytics/engine/model.py:808\u001b[0m, in \u001b[0;36mModel.train\u001b[0;34m(self, trainer, **kwargs)\u001b[0m\n\u001b[1;32m    805\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mmodel\n\u001b[1;32m    807\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mhub_session \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msession  \u001b[38;5;66;03m# attach optional HUB session\u001b[39;00m\n\u001b[0;32m--> 808\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    809\u001b[0m \u001b[38;5;66;03m# Update model and cfg after training\u001b[39;00m\n\u001b[1;32m    810\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m RANK \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m}:\n",
      "File \u001b[0;32m~/project/EoC/code/det/ultralytics/engine/trainer.py:207\u001b[0m, in \u001b[0;36mBaseTrainer.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    204\u001b[0m         ddp_cleanup(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mstr\u001b[39m(file))\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 207\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mworld_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/project/EoC/code/det/ultralytics/engine/trainer.py:389\u001b[0m, in \u001b[0;36mBaseTrainer._do_train\u001b[0;34m(self, world_size)\u001b[0m\n\u001b[1;32m    384\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtloss \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    385\u001b[0m         (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtloss \u001b[38;5;241m*\u001b[39m i \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_items) \u001b[38;5;241m/\u001b[39m (i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtloss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_items\n\u001b[1;32m    386\u001b[0m     )\n\u001b[1;32m    388\u001b[0m \u001b[38;5;66;03m# Backward\u001b[39;00m\n\u001b[0;32m--> 389\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    391\u001b[0m \u001b[38;5;66;03m# Optimize - https://pytorch.org/docs/master/notes/amp_examples.html\u001b[39;00m\n\u001b[1;32m    392\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ni \u001b[38;5;241m-\u001b[39m last_opt_step \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccumulate:\n",
      "File \u001b[0;32m~/miniconda3/envs/py310/lib/python3.10/site-packages/torch/_tensor.py:648\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    638\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    639\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    640\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    641\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    646\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    647\u001b[0m     )\n\u001b[0;32m--> 648\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    649\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    650\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/py310/lib/python3.10/site-packages/torch/autograd/__init__.py:353\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    348\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    350\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    351\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 353\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    356\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    359\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    360\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    361\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/py310/lib/python3.10/site-packages/torch/autograd/graph.py:824\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    822\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    823\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 824\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    825\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    826\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    827\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    828\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "torch.use_deterministic_algorithms(False)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TomatoMAP-Det Trainer\")\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "\n",
    "print(\"downloading pretrained model: \")\n",
    "\n",
    "model = YOLO(\"yolo11l.pt\")\n",
    "\n",
    "print(\"model info: \")\n",
    "\n",
    "train_result = model.train(\n",
    "    data=\"det/TomatoMAP-Det.yaml\",\n",
    "    epochs=500,\n",
    "    imgsz=640,\n",
    "    device=[0],\n",
    "    batch=4,\n",
    "    patience=10,\n",
    "    project=\"det/output\",\n",
    "    cfg=\"det/best_hyperparameters.yaml\", # fine-tuned hyperparameters, ready to use, details please contact us per email\n",
    "    #profile=True,\n",
    "    plots=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c9228d-dc78-42af-8b78-7a41685f25b5",
   "metadata": {},
   "source": [
    "# TomatoMAP-Seg Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de9f46fe-5dea-4098-9ac0-20249a648788",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Detectron2 version: 0.6\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import json\n",
    "import yaml\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from collections import OrderedDict\n",
    "\n",
    "# Detectron2\n",
    "import detectron2\n",
    "from detectron2.engine import DefaultTrainer, DefaultPredictor, HookBase\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.data import MetadataCatalog, build_detection_test_loader, build_detection_train_loader\n",
    "from detectron2.data.datasets import register_coco_instances\n",
    "from detectron2.evaluation import COCOEvaluator, inference_on_dataset\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "from detectron2.utils.events import get_event_storage\n",
    "from detectron2.utils.logger import setup_logger\n",
    "\n",
    "setup_logger()\n",
    "\n",
    "print(f\"  Detectron2 version: {detectron2.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ce474b7-4d66-4a5a-957b-475a14a237b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_segmentation(points):\n",
    "    # 把 [[x,y],[x,y]] → [x1,y1,x2,y2,...]\n",
    "    return [coord for pair in points for coord in pair]\n",
    "\n",
    "def load_categories_from_yaml(yaml_path):\n",
    "    with open(yaml_path, 'r', encoding='utf-8') as f:\n",
    "        data = yaml.safe_load(f)\n",
    "    categories = []\n",
    "    cat_map = {}\n",
    "    cat_id = 1\n",
    "    for item in data['label']:\n",
    "        name = item['name']\n",
    "        if name == '__background__':\n",
    "            continue\n",
    "        categories.append({\n",
    "            \"id\": cat_id,\n",
    "            \"name\": name,\n",
    "            \"supercategory\": \"none\"\n",
    "        })\n",
    "        cat_map[name] = cat_id\n",
    "        cat_id += 1\n",
    "    return categories, cat_map\n",
    "\n",
    "def convert_isat_folder_to_coco(task_dir, label_dir, yaml_path, output_dir, train_ratio=0.7, val_ratio=0.2):\n",
    "    print(\"ISAT2COCO...\")\n",
    "    \n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    categories, category_map = load_categories_from_yaml(yaml_path)\n",
    "    print(f\"loaded {len(categories)} classes\")\n",
    "\n",
    "    if not os.path.exists(task_dir):\n",
    "        print(f\"image folder not exist: {task_dir}\")\n",
    "        return False\n",
    "    \n",
    "    if not os.path.exists(label_dir):\n",
    "        print(f\"label folder not exist: {label_dir}\")\n",
    "        return False\n",
    "\n",
    "    images = [f for f in os.listdir(task_dir) if f.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp'))]\n",
    "    json_map = {os.path.splitext(f)[0]: f for f in os.listdir(label_dir) if f.endswith(\".json\")}\n",
    "\n",
    "    print(f\"found {len(images)} images\")\n",
    "    print(f\"found {len(json_map)} labels\")\n",
    "\n",
    "    # 匹配图像和标注\n",
    "    dataset = []\n",
    "    unmatched_images = []\n",
    "    \n",
    "    for img_name in tqdm(images, desc=\"匹配图像和标注\"):\n",
    "        base = os.path.splitext(img_name)[0]\n",
    "        if base in json_map:\n",
    "            dataset.append({\n",
    "                \"img_file\": img_name,\n",
    "                \"json_file\": json_map[base]\n",
    "            })\n",
    "        else:\n",
    "            unmatched_images.append(img_name)\n",
    "\n",
    "    print(f\"successfully matched {len(dataset)} pairs\")\n",
    "    if unmatched_images:\n",
    "        print(f\" {len(unmatched_images)} unmatched images\")\n",
    "\n",
    "    if len(dataset) == 0:\n",
    "        print(\"no matched pairs\")\n",
    "        return False\n",
    "\n",
    "    # 设置随机种子以保证可重现性\n",
    "    random.seed(42)\n",
    "    random.shuffle(dataset)\n",
    "    \n",
    "    total = len(dataset)\n",
    "    train_end = int(total * train_ratio)\n",
    "    val_end = int(total * (train_ratio + val_ratio))\n",
    "\n",
    "    splits = {\n",
    "        \"train\": dataset[:train_end],\n",
    "        \"val\": dataset[train_end:val_end],\n",
    "        \"test\": dataset[val_end:]\n",
    "    }\n",
    "\n",
    "    print(f\"\\n splitting dataset:\")\n",
    "    for split_name, split_data in splits.items():\n",
    "        print(f\"  {split_name}: {len(split_data)} images ({len(split_data)/total*100:.1f}%)\")\n",
    "\n",
    "    conversion_stats = {}\n",
    "    \n",
    "    for split_name, split_data in splits.items():\n",
    "        if len(split_data) == 0:\n",
    "            print(f\" {split_name} dataset empty, skipping\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"\\n transforming {split_name} dataset...\")\n",
    "        \n",
    "        coco = {\n",
    "            \"info\": {\n",
    "                \"description\": \"TomatoMAP-Seg Dataset\",\n",
    "                \"version\": \"1.0\",\n",
    "                \"year\": 2024,\n",
    "                \"contributor\": \"TomatoMAP-Seg Project\",\n",
    "                \"date_created\": \"2024-01-01\"\n",
    "            },\n",
    "            \"images\": [],\n",
    "            \"annotations\": [],\n",
    "            \"categories\": categories\n",
    "        }\n",
    "        ann_id = 1\n",
    "        img_id = 1\n",
    "        \n",
    "        processed_annotations = 0\n",
    "        skipped_annotations = 0\n",
    "\n",
    "        for item in tqdm(split_data, desc=f\"processing {split_name}\"):\n",
    "            img_path = os.path.join(task_dir, item[\"img_file\"])\n",
    "            json_path = os.path.join(label_dir, item[\"json_file\"])\n",
    "\n",
    "            if not os.path.exists(json_path):\n",
    "                print(f\" label file not exist: {json_path}\")\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                with open(json_path, 'r', encoding='utf-8') as f:\n",
    "                    isat = json.load(f)\n",
    "            except Exception as e:\n",
    "                print(f\"failed to load label file {json_path}: {e}\")\n",
    "                continue\n",
    "\n",
    "            info = isat['info']\n",
    "            coco[\"images\"].append({\n",
    "                \"file_name\": item[\"img_file\"],\n",
    "                \"id\": img_id,\n",
    "                \"width\": info[\"width\"],\n",
    "                \"height\": info[\"height\"]\n",
    "            })\n",
    "\n",
    "            for obj in isat.get('objects', []):\n",
    "                cat = obj['category']\n",
    "                if cat not in category_map:\n",
    "                    skipped_annotations += 1\n",
    "                    continue\n",
    "\n",
    "                # 修复 segmentation 点格式\n",
    "                seg_flat = flatten_segmentation(obj[\"segmentation\"])\n",
    "                if len(seg_flat) < 6:\n",
    "                    skipped_annotations += 1\n",
    "                    continue  # 跳过非法多边形（小于 3 个点）\n",
    "\n",
    "                coco[\"annotations\"].append({\n",
    "                    \"id\": ann_id,\n",
    "                    \"image_id\": img_id,\n",
    "                    \"category_id\": category_map[cat],\n",
    "                    \"segmentation\": [seg_flat],\n",
    "                    \"bbox\": obj[\"bbox\"],\n",
    "                    \"area\": obj[\"area\"],\n",
    "                    \"iscrowd\": obj.get(\"iscrowd\", 0),\n",
    "                    \"group_id\": obj.get(\"group\", None)\n",
    "                })\n",
    "                ann_id += 1\n",
    "                processed_annotations += 1\n",
    "                \n",
    "            img_id += 1\n",
    "\n",
    "        output_file = os.path.join(output_dir, f\"{split_name}.json\")\n",
    "        with open(output_file, \"w\", encoding='utf-8') as f:\n",
    "            json.dump(coco, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "        conversion_stats[split_name] = {\n",
    "            'images': len(split_data),\n",
    "            'annotations': processed_annotations,\n",
    "            'skipped': skipped_annotations\n",
    "        }\n",
    "\n",
    "        print(f\"  generated {split_name}.json\")\n",
    "        print(f\"     images: {len(split_data)}\")\n",
    "        print(f\"     annotations: {processed_annotations}\")\n",
    "        print(f\"     skipped: {skipped_annotations}\")\n",
    "\n",
    "    print(f\"\\n ISAT2COCO finished\")\n",
    "    print(f\" output at: {output_dir}\")\n",
    "    print(f\" conversion stats:\")\n",
    "    \n",
    "    total_images = sum(stats['images'] for stats in conversion_stats.values())\n",
    "    total_annotations = sum(stats['annotations'] for stats in conversion_stats.values())\n",
    "    total_skipped = sum(stats['skipped'] for stats in conversion_stats.values())\n",
    "    \n",
    "    print(f\"  total images: {total_images}\")\n",
    "    print(f\"  total annotations: {total_annotations}\")\n",
    "    print(f\"  skipped annotations: {total_skipped}\")\n",
    "    \n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f15c7807-6686-49b8-8c2b-c9fc5a960936",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configurations:\n",
      "ISAT converter config:\n",
      "  task_dir: TomatoMAP/TomatoMAP-Seg/images\n",
      "  label_dir: TomatoMAP/TomatoMAP-Seg/labels\n",
      "  yaml_path: TomatoMAP/TomatoMAP-Seg/labels/isat.yaml\n",
      "  output_dir: TomatoMAP/TomatoMAP-Seg/cocoOut\n",
      "  train_ratio: 0.7\n",
      "  val_ratio: 0.2\n",
      "  auto_convert: True\n",
      "\n",
      " dataset config:\n",
      "  dataset_root: TomatoMAP/TomatoMAP-Seg/\n",
      "  img_dir: TomatoMAP/TomatoMAP-Seg/images\n",
      "  coco_ann_dir: TomatoMAP/TomatoMAP-Seg/cocoOut\n",
      "  isat_yaml_path: TomatoMAP/TomatoMAP-Seg/labels/isat.yaml\n",
      "  output_dir: TomatoMAP/TomatoMAP-Seg/output\n",
      "  num_classes: 10\n",
      "\n",
      " training config:\n",
      "  model_name: mask_rcnn_R_50_FPN_1x\n",
      "  batch_size: 4\n",
      "  base_lr: 0.00024\n",
      "  max_epochs: 100\n",
      "  patience: 15\n",
      "  num_workers: 8\n",
      "  score_thresh_test: 0.3\n",
      "  input_min_size_train: (640, 672, 704, 736, 768, 800)\n",
      "  input_max_size_train: 1333\n",
      "  checkpoint_period: 10\n",
      "  eval_period: 10\n"
     ]
    }
   ],
   "source": [
    "ISAT_CONFIG = {\n",
    "    'task_dir': \"TomatoMAP/TomatoMAP-Seg/images\",\n",
    "    'label_dir': \"TomatoMAP/TomatoMAP-Seg/labels\",\n",
    "    'yaml_path': \"TomatoMAP/TomatoMAP-Seg/labels/isat.yaml\",\n",
    "    'output_dir': \"TomatoMAP/TomatoMAP-Seg/cocoOut\",\n",
    "    'train_ratio': 0.7,\n",
    "    'val_ratio': 0.2,    # rest 0.1 is test\n",
    "    'auto_convert': True\n",
    "}\n",
    "\n",
    "DATASET_CONFIG = {\n",
    "    'dataset_root': \"TomatoMAP/TomatoMAP-Seg/\",\n",
    "    'img_dir': \"TomatoMAP/TomatoMAP-Seg/images\",\n",
    "    'coco_ann_dir': \"TomatoMAP/TomatoMAP-Seg/cocoOut\",\n",
    "    'isat_yaml_path': \"TomatoMAP/TomatoMAP-Seg/labels/isat.yaml\",\n",
    "    'output_dir': \"TomatoMAP/TomatoMAP-Seg/output\",\n",
    "    'num_classes': 10,    # without background\n",
    "}\n",
    "\n",
    "TRAINING_CONFIG = {\n",
    "    'model_name': \"mask_rcnn_R_50_FPN_1x\",\n",
    "    'batch_size': 4,\n",
    "    'base_lr': 0.00024,\n",
    "    'max_epochs': 100,\n",
    "    'patience': 15,\n",
    "    'num_workers': 8,  # Windows user please set to 0\n",
    "    'score_thresh_test': 0.3,\n",
    "    'input_min_size_train': (640, 672, 704, 736, 768, 800), \n",
    "    'input_max_size_train': 1333,\n",
    "    'checkpoint_period': 10,\n",
    "    'eval_period': 10,\n",
    "}\n",
    "\n",
    "print(\"Configurations:\")\n",
    "print(\"ISAT converter config:\")\n",
    "for key, value in ISAT_CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "print(\"\\n dataset config:\")\n",
    "for key, value in DATASET_CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "print(\"\\n training config:\")\n",
    "for key, value in TRAINING_CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb7b82da-fe47-4535-b599-7fe5551181a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ISAT converting to COCO format\n",
      "============================================================\n",
      "ISAT paths checked, starting conversion...\n",
      "ISAT2COCO...\n",
      "loaded 10 classes\n",
      "found 3612 images\n",
      "found 727 labels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "匹配图像和标注: 100%|███████████████████████████████████████████████████████████| 3612/3612 [00:00<00:00, 777911.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "successfully matched 727 pairs\n",
      " 2885 unmatched images\n",
      "\n",
      " splitting dataset:\n",
      "  train: 508 images (69.9%)\n",
      "  val: 146 images (20.1%)\n",
      "  test: 73 images (10.0%)\n",
      "\n",
      " transforming train dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "processing train: 100%|██████████████████████████████████████████████████████████████| 508/508 [00:01<00:00, 395.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  generated train.json\n",
      "     images: 508\n",
      "     annotations: 4539\n",
      "     skipped: 0\n",
      "\n",
      " transforming val dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "processing val: 100%|████████████████████████████████████████████████████████████████| 146/146 [00:00<00:00, 394.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  generated val.json\n",
      "     images: 146\n",
      "     annotations: 1280\n",
      "     skipped: 0\n",
      "\n",
      " transforming test dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "processing test: 100%|█████████████████████████████████████████████████████████████████| 73/73 [00:00<00:00, 264.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  generated test.json\n",
      "     images: 73\n",
      "     annotations: 694\n",
      "     skipped: 0\n",
      "\n",
      " ISAT2COCO finished\n",
      " output at: TomatoMAP/TomatoMAP-Seg/cocoOut\n",
      " conversion stats:\n",
      "  total images: 727\n",
      "  total annotations: 6513\n",
      "  skipped annotations: 0\n",
      "\n",
      " Dataset configuration updated:\n",
      "   image path: TomatoMAP/TomatoMAP-Seg/images\n",
      "   annotation path: TomatoMAP/TomatoMAP-Seg/cocoOut\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ISAT converting to COCO format\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "need_conversion = ISAT_CONFIG['auto_convert']\n",
    "\n",
    "coco_files_exist = all(\n",
    "    os.path.exists(os.path.join(ISAT_CONFIG['output_dir'], f\"{split}.json\"))\n",
    "    for split in ['train', 'val', 'test']\n",
    ")\n",
    "\n",
    "if coco_files_exist and not need_conversion:\n",
    "    print(\"COCO format files exist, skipping conversion!\")\n",
    "    print(\"   if you want to reconvert, set ISAT_CONFIG['auto_convert'] = True\")\n",
    "else:\n",
    "    required_isat_paths = [\n",
    "        ISAT_CONFIG['task_dir'],\n",
    "        ISAT_CONFIG['label_dir'], \n",
    "        ISAT_CONFIG['yaml_path']\n",
    "    ]\n",
    "    \n",
    "    missing_paths = [path for path in required_isat_paths if not os.path.exists(path)]\n",
    "    \n",
    "    if missing_paths:\n",
    "        print(\"Following ISAT paths do not exist:\")\n",
    "        for path in missing_paths:\n",
    "            print(f\"   {path}\")\n",
    "        print(\"\\nPlease check ISAT_CONFIG path settings\")\n",
    "        conversion_success = False\n",
    "    else:\n",
    "        print(\"ISAT paths checked, starting conversion...\")\n",
    "        \n",
    "        conversion_success = convert_isat_folder_to_coco(\n",
    "            task_dir=ISAT_CONFIG['task_dir'],\n",
    "            label_dir=ISAT_CONFIG['label_dir'],\n",
    "            yaml_path=ISAT_CONFIG['yaml_path'],\n",
    "            output_dir=ISAT_CONFIG['output_dir'],\n",
    "            train_ratio=ISAT_CONFIG['train_ratio'],\n",
    "            val_ratio=ISAT_CONFIG['val_ratio']\n",
    "        )\n",
    "\n",
    "if 'conversion_success' not in locals():\n",
    "    conversion_success = True\n",
    "\n",
    "if conversion_success:\n",
    "    DATASET_CONFIG['coco_ann_dir'] = ISAT_CONFIG['output_dir']\n",
    "    DATASET_CONFIG['img_dir'] = ISAT_CONFIG['task_dir'] \n",
    "    DATASET_CONFIG['isat_yaml_path'] = ISAT_CONFIG['yaml_path']\n",
    "    print(f\"\\n Dataset configuration updated:\")\n",
    "    print(f\"   image path: {DATASET_CONFIG['img_dir']}\")\n",
    "    print(f\"   annotation path: {DATASET_CONFIG['coco_ann_dir']}\")\n",
    "else:\n",
    "    print(\"\\n Conversion failed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "78e72146-ee66-4937-a2c8-6949de40cc4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " TomatoMAP-Seg Dataset Info:\n",
      "========================================\n",
      "train: 508 images, 4539 annotations\n",
      "val: 146 images, 1280 annotations\n",
      "test: 73 images, 694 annotations\n",
      "\n",
      " Analyze dataset object areas...\n",
      "============================================================\n",
      "\n",
      "TRAIN dataset analysis:\n",
      "----------------------------------------\n",
      "Average image size: 3648 x 5472\n",
      "Total object count: 4539\n",
      "\n",
      " Original image size distribution:\n",
      "  Small objects (<32²): 91 (2.0%)\n",
      "  Medium objects (32²-96²): 529 (11.7%)\n",
      "  Large objects (>96²): 3919 (86.3%)\n",
      "  Min area: 2 pixels²\n",
      "  Max area: 5373098 pixels²\n",
      "  Mean area: 340356 pixels²\n",
      "\n",
      "Scaled to 640-1333:\n",
      "  Small objects (<32²): 1013 (22.3%)\n",
      "  Medium objects (32²-96²): 2109 (46.5%)\n",
      "  Large objects (>96²): 1417 (31.2%)\n",
      "\n",
      "VAL dataset analysis:\n",
      "----------------------------------------\n",
      "Average image size: 3648 x 5472\n",
      "Total object count: 1280\n",
      "\n",
      " Original image size distribution:\n",
      "  Small objects (<32²): 26 (2.0%)\n",
      "  Medium objects (32²-96²): 127 (9.9%)\n",
      "  Large objects (>96²): 1127 (88.0%)\n",
      "  Min area: 3 pixels²\n",
      "  Max area: 4697448 pixels²\n",
      "  Mean area: 364283 pixels²\n",
      "\n",
      "Scaled to 640-1333:\n",
      "  Small objects (<32²): 252 (19.7%)\n",
      "  Medium objects (32²-96²): 622 (48.6%)\n",
      "  Large objects (>96²): 406 (31.7%)\n",
      "\n",
      "TEST dataset analysis:\n",
      "----------------------------------------\n",
      "Average image size: 3648 x 5472\n",
      "Total object count: 694\n",
      "\n",
      " Original image size distribution:\n",
      "  Small objects (<32²): 13 (1.9%)\n",
      "  Medium objects (32²-96²): 72 (10.4%)\n",
      "  Large objects (>96²): 609 (87.8%)\n",
      "  Min area: 33 pixels²\n",
      "  Max area: 5199393 pixels²\n",
      "  Mean area: 386834 pixels²\n",
      "\n",
      "Scaled to 640-1333:\n",
      "  Small objects (<32²): 139 (20.0%)\n",
      "  Medium objects (32²-96²): 322 (46.4%)\n",
      "  Large objects (>96²): 233 (33.6%)\n"
     ]
    }
   ],
   "source": [
    "def analyze_dataset_areas():\n",
    "    print(f\"\\n Analyze dataset object areas...\")\n",
    "    print(f\"=\" * 60)\n",
    "    \n",
    "    for split in ['train', 'val', 'test']:\n",
    "        ann_file = os.path.join(DATASET_CONFIG['coco_ann_dir'], f\"{split}.json\")\n",
    "        if not os.path.exists(ann_file):\n",
    "            print(f\"Annotation file {ann_file} does not exist\")\n",
    "            continue\n",
    "            \n",
    "        with open(ann_file, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        image_info = {img['id']: img for img in data['images']}\n",
    "        \n",
    "        areas_original = []\n",
    "        areas_scaled = []\n",
    "        \n",
    "        min_size = min(TRAINING_CONFIG['input_min_size_train'])\n",
    "        max_size = TRAINING_CONFIG['input_max_size_train']\n",
    "        \n",
    "        for ann in data['annotations']:\n",
    "            if 'area' in ann:\n",
    "                area = ann['area']\n",
    "            else:\n",
    "                bbox = ann.get('bbox', [0, 0, 0, 0])\n",
    "                area = bbox[2] * bbox[3]\n",
    "            areas_original.append(area)\n",
    "            \n",
    "            img_id = ann['image_id']\n",
    "            if img_id in image_info:\n",
    "                img = image_info[img_id]\n",
    "                orig_w, orig_h = img['width'], img['height']\n",
    "                \n",
    "                size = max(orig_w, orig_h)\n",
    "                if size > max_size:\n",
    "                    scale = max_size / size\n",
    "                else:\n",
    "                    scale = min_size / min(orig_w, orig_h)\n",
    "                    if scale * size > max_size:\n",
    "                        scale = max_size / size\n",
    "                \n",
    "                scaled_area = area * (scale ** 2)\n",
    "                areas_scaled.append(scaled_area)\n",
    "        \n",
    "        areas_original = np.array(areas_original)\n",
    "        areas_scaled = np.array(areas_scaled) if areas_scaled else areas_original\n",
    "        \n",
    "        print(f\"\\n{split.upper()} dataset analysis:\")\n",
    "        print(f\"-\" * 40)\n",
    "        \n",
    "        if len(data['images']) > 0:\n",
    "            avg_width = np.mean([img['width'] for img in data['images']])\n",
    "            avg_height = np.mean([img['height'] for img in data['images']])\n",
    "            print(f\"Average image size: {avg_width:.0f} x {avg_height:.0f}\")\n",
    "        \n",
    "        print(f\"Total object count: {len(areas_original)}\")\n",
    "        \n",
    "        print(f\"\\n Original image size distribution:\")\n",
    "        small_orig = np.sum(areas_original < 32**2)\n",
    "        medium_orig = np.sum((areas_original >= 32**2) & (areas_original < 96**2))\n",
    "        large_orig = np.sum(areas_original >= 96**2)\n",
    "        \n",
    "        print(f\"  Small objects (<32²): {small_orig} ({small_orig/len(areas_original)*100:.1f}%)\")\n",
    "        print(f\"  Medium objects (32²-96²): {medium_orig} ({medium_orig/len(areas_original)*100:.1f}%)\")\n",
    "        print(f\"  Large objects (>96²): {large_orig} ({large_orig/len(areas_original)*100:.1f}%)\")\n",
    "        print(f\"  Min area: {np.min(areas_original):.0f} pixels²\")\n",
    "        print(f\"  Max area: {np.max(areas_original):.0f} pixels²\")\n",
    "        print(f\"  Mean area: {np.mean(areas_original):.0f} pixels²\")\n",
    "        \n",
    "        print(f\"\\nScaled to {min_size}-{max_size}:\")\n",
    "        small_scaled = np.sum(areas_scaled < 32**2)\n",
    "        medium_scaled = np.sum((areas_scaled >= 32**2) & (areas_scaled < 96**2))\n",
    "        large_scaled = np.sum(areas_scaled >= 96**2)\n",
    "        \n",
    "        print(f\"  Small objects (<32²): {small_scaled} ({small_scaled/len(areas_scaled)*100:.1f}%)\")\n",
    "        print(f\"  Medium objects (32²-96²): {medium_scaled} ({medium_scaled/len(areas_scaled)*100:.1f}%)\")\n",
    "        print(f\"  Large objects (>96²): {large_scaled} ({large_scaled/len(areas_scaled)*100:.1f}%)\")\n",
    "        \n",
    "        if small_scaled == 0:\n",
    "            print(f\"\\n After scaling, no small objects - APs will be -1\")\n",
    "        if medium_scaled == 0:\n",
    "            print(f\" After scaling, no medium objects - APm will be -1\")\n",
    "        if large_scaled == 0:\n",
    "            print(f\" After scaling, no large objects - APl will be -1\")\n",
    "\n",
    "def get_dataset_info():\n",
    "    print(f\"\\n TomatoMAP-Seg Dataset Info:\")\n",
    "    print(f\"=\" * 40)\n",
    "    \n",
    "    for split in ['train', 'val', 'test']:\n",
    "        ann_file = os.path.join(DATASET_CONFIG['coco_ann_dir'], f\"{split}.json\")\n",
    "        if os.path.exists(ann_file):\n",
    "            with open(ann_file, 'r') as f:\n",
    "                data = json.load(f)\n",
    "            print(f\"{split}: {len(data['images'])} images, {len(data['annotations'])} annotations\")\n",
    "    \n",
    "    return True\n",
    "\n",
    "if conversion_success:\n",
    "    get_dataset_info()\n",
    "    analyze_dataset_areas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "80ca5eb8-9ece-4fdc-8240-54e8df7b7f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BestModelHook(HookBase):\n",
    "    # hook to save the best model based on validation segmentation mAP\n",
    "    \n",
    "    def __init__(self, cfg, eval_period, patience=10):\n",
    "        self.cfg = cfg.clone()\n",
    "        self.eval_period = eval_period\n",
    "        self.patience = patience\n",
    "        self.best_score = 0\n",
    "        self.best_metric_name = None\n",
    "        self.best_epoch = -1\n",
    "        self.epochs_without_improvement = 0\n",
    "        self.should_stop = False\n",
    "        self.history = []\n",
    "        \n",
    "    def get_valid_score(self, segm_results):\n",
    "        priority_metrics = [\"AP\", \"AP50\", \"AP75\", \"APm\", \"APl\"]\n",
    "        \n",
    "        for metric in priority_metrics:\n",
    "            value = segm_results.get(metric, -1)\n",
    "            if value != -1:\n",
    "                return metric, value\n",
    "        \n",
    "        return None, None\n",
    "    \n",
    "    def after_step(self):\n",
    "        next_iter = self.trainer.iter + 1\n",
    "        is_final_iter = next_iter == self.trainer.max_iter\n",
    "        \n",
    "        if (next_iter % self.eval_period == 0 and not is_final_iter):\n",
    "            current_epoch = (next_iter // self.eval_period)\n",
    "            \n",
    "            results = self._do_eval()\n",
    "            if results is None:\n",
    "                print(f\"Epoch {current_epoch}: evaluation failed\")\n",
    "                return\n",
    "            \n",
    "            segm_results = results.get(\"segm\", {})\n",
    "            bbox_results = results.get(\"bbox\", {})\n",
    "            \n",
    "            metric_name, current_score = self.get_valid_score(segm_results)\n",
    "            \n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(f\"Epoch {current_epoch} evaluation results:\")\n",
    "            print(f\"{'='*60}\")\n",
    "            \n",
    "            print(\"\\n Bbox metrics:\")\n",
    "            for key in [\"AP\", \"AP50\", \"AP75\", \"APs\", \"APm\", \"APl\"]:\n",
    "                value = bbox_results.get(key, -1)\n",
    "                if value != -1:\n",
    "                    print(f\"  {key}: {value:.4f} ✓\")\n",
    "                else:\n",
    "                    print(f\"  {key}: N/A\")\n",
    "            \n",
    "            print(\"\\n Segmentation metrics:\")\n",
    "            for key in [\"AP\", \"AP50\", \"AP75\", \"APs\", \"APm\", \"APl\"]:\n",
    "                value = segm_results.get(key, -1)\n",
    "                if value != -1:\n",
    "                    print(f\"  {key}: {value:.4f} ✓\")\n",
    "                else:\n",
    "                    print(f\"  {key}: N/A (no objects for this size)\")\n",
    "            \n",
    "            if metric_name is None:\n",
    "                print(\"\\n Warning! No valid metrics found\")\n",
    "                print(\"Please check TomatoMAP-Seg dataset structure\")\n",
    "\n",
    "                metric_name, current_score = self.get_valid_score(bbox_results)\n",
    "                if metric_name is not None:\n",
    "                    print(f\"Using bbox metric instead: {metric_name} = {current_score:.4f}\")\n",
    "                else:\n",
    "                    return\n",
    "            \n",
    "            print(f\"\\n Main metric: {metric_name} = {current_score:.4f}\")\n",
    "            \n",
    "            self.history.append({\n",
    "                'epoch': current_epoch,\n",
    "                'metric': metric_name,\n",
    "                'score': current_score,\n",
    "                'all_metrics': {**segm_results, **{'bbox_' + k: v for k, v in bbox_results.items()}}\n",
    "            })\n",
    "            \n",
    "            if current_score > self.best_score:\n",
    "                improvement = current_score - self.best_score\n",
    "                self.best_score = current_score\n",
    "                self.best_metric_name = metric_name\n",
    "                self.best_epoch = current_epoch\n",
    "                self.epochs_without_improvement = 0\n",
    "                \n",
    "                self.trainer.checkpointer.save(\"model_best\")\n",
    "                print(f\"\\n Best model saved!\")\n",
    "                print(f\"   Score: {current_score:.4f} (↑{improvement:.4f})\")\n",
    "                \n",
    "                best_results_file = os.path.join(self.cfg.OUTPUT_DIR, \"best_results.json\")\n",
    "                with open(best_results_file, 'w') as f:\n",
    "                    json.dump({\n",
    "                        'epoch': current_epoch,\n",
    "                        'metric': metric_name,\n",
    "                        'score': current_score,\n",
    "                        'segm_results': segm_results,\n",
    "                        'bbox_results': bbox_results\n",
    "                    }, f, indent=2)\n",
    "            else:\n",
    "                self.epochs_without_improvement += 1\n",
    "                gap = self.best_score - current_score\n",
    "                print(f\"\\nCurrent: {current_score:.4f} | Best: {self.best_score:.4f} (gap: {gap:.4f})\")\n",
    "                print(f\"No improvement for {self.epochs_without_improvement}/{self.patience} epochs\")\n",
    "            \n",
    "            if self.epochs_without_improvement >= self.patience:\n",
    "                print(f\"\\n{'='*60}\")\n",
    "                print(f\"Early stopping triggered!\")\n",
    "                print(f\"   Best {self.best_metric_name}: {self.best_score:.4f} (epoch {self.best_epoch})\")\n",
    "                print(f\"   Total epochs: {current_epoch}\")\n",
    "                print(f\"{'='*60}\")\n",
    "                self.should_stop = True\n",
    "\n",
    "                self.trainer.storage._iter = self.trainer.max_iter\n",
    "    \n",
    "    def _do_eval(self):\n",
    "        try:\n",
    "            evaluator = COCOEvaluator(\"tomato_val\", self.cfg, False, \n",
    "                                    output_dir=os.path.join(self.cfg.OUTPUT_DIR, \"inference\"))\n",
    "            val_loader = build_detection_test_loader(self.cfg, \"tomato_val\")\n",
    "            results = inference_on_dataset(self.trainer.model, val_loader, evaluator)\n",
    "            return results\n",
    "        except Exception as e:\n",
    "            print(f\"Evaluation failed: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            return None\n",
    "\n",
    "class MyTrainer(DefaultTrainer):\n",
    "    \n",
    "    def __init__(self, cfg, patience=None):\n",
    "        self.patience = patience if patience is not None else TRAINING_CONFIG['patience']\n",
    "        super().__init__(cfg)\n",
    "    \n",
    "    @classmethod\n",
    "    def build_evaluator(cls, cfg, dataset_name, output_folder=None):\n",
    "        if output_folder is None:\n",
    "            output_folder = os.path.join(cfg.OUTPUT_DIR, \"inference\")\n",
    "        return COCOEvaluator(\n",
    "            dataset_name=dataset_name,\n",
    "            distributed=False,\n",
    "            output_dir=output_folder,\n",
    "            use_fast_impl=True,\n",
    "            tasks=(\"bbox\", \"segm\"),\n",
    "        )\n",
    "    \n",
    "    def build_hooks(self):\n",
    "        hooks = super().build_hooks()\n",
    "        \n",
    "        try:\n",
    "            train_loader = build_detection_train_loader(self.cfg)\n",
    "            iters_per_epoch = len(train_loader) // self.cfg.SOLVER.IMS_PER_BATCH\n",
    "            print(f\"Iterations per epoch: {iters_per_epoch}\")\n",
    "        except:\n",
    "            iters_per_epoch = 127\n",
    "            print(f\"Using default iterations per epoch: {iters_per_epoch}\")\n",
    "        \n",
    "        eval_period = iters_per_epoch * TRAINING_CONFIG['eval_period']\n",
    "        \n",
    "        best_model_hook = BestModelHook(self.cfg, eval_period, self.patience)\n",
    "        hooks.append(best_model_hook)\n",
    "        \n",
    "        self.best_model_hook = best_model_hook\n",
    "        \n",
    "        return hooks\n",
    "    \n",
    "    def run_step(self):\n",
    "        super().run_step()\n",
    "        \n",
    "        if hasattr(self, 'best_model_hook') and self.best_model_hook.should_stop:\n",
    "            print(\"Early stopping triggered, training stopped\")\n",
    "            self.storage._iter = self.max_iter\n",
    "    \n",
    "    def train(self):\n",
    "        super().train()\n",
    "        \n",
    "        if hasattr(self, 'best_model_hook'):\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(f\"Training Summary:\")\n",
    "            print(f\"{'='*60}\")\n",
    "            if self.best_model_hook.best_score > 0:\n",
    "                print(f\"Best {self.best_model_hook.best_metric_name}: {self.best_model_hook.best_score:.4f}\")\n",
    "                print(f\"Best epoch: {self.best_model_hook.best_epoch}\")\n",
    "                print(f\"Best model saved as: model_best.pth\")\n",
    "            else:\n",
    "                print(\"No valid metrics found during training\")\n",
    "            \n",
    "            history_file = os.path.join(self.cfg.OUTPUT_DIR, \"training_history.json\")\n",
    "            with open(history_file, 'w') as f:\n",
    "                json.dump(self.best_model_hook.history, f, indent=2)\n",
    "            print(f\"Training history saved at: {history_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9f48af04-6a2d-4ee7-8647-53efb20c376f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "TomatoMAP-Seg Dataset Registration\n",
      "============================================================\n",
      "Registering datasets...\n",
      "TomatoMAP-Seg datasets registered successfully\n",
      "Configuration built successfully\n",
      "Training Configuration Summary:\n",
      "  Model: mask_rcnn_R_50_FPN_1x\n",
      "  Number of classes: 10\n",
      "  Batch size: 4\n",
      "  Learning rate: 0.00024\n",
      "  Max epochs: 100\n",
      "  Patience: 15\n",
      "  Input size range: 640-1333\n",
      "  Output directory: TomatoMAP/TomatoMAP-Seg/output\n",
      "  Device: cuda\n"
     ]
    }
   ],
   "source": [
    "def register_all_datasets():\n",
    "    print(\"Registering datasets...\")\n",
    "    \n",
    "    # Register COCO format datasets\n",
    "    register_coco_instances(\"tomato_train\", {}, \n",
    "                           os.path.join(DATASET_CONFIG['coco_ann_dir'], \"train.json\"), \n",
    "                           DATASET_CONFIG['img_dir'])\n",
    "    register_coco_instances(\"tomato_val\", {}, \n",
    "                           os.path.join(DATASET_CONFIG['coco_ann_dir'], \"val.json\"), \n",
    "                           DATASET_CONFIG['img_dir'])\n",
    "    register_coco_instances(\"tomato_test\", {}, \n",
    "                           os.path.join(DATASET_CONFIG['coco_ann_dir'], \"test.json\"), \n",
    "                           DATASET_CONFIG['img_dir'])\n",
    "    \n",
    "    # Load class labels from YAML\n",
    "    with open(DATASET_CONFIG['isat_yaml_path'], 'r', encoding='utf-8') as f:\n",
    "        data = yaml.safe_load(f)\n",
    "    labels = [item['name'] for item in data['label'] if item['name'] != '__background__']\n",
    "    \n",
    "    return labels\n",
    "\n",
    "def build_cfg():\n",
    "    cfg = get_cfg()\n",
    "    \n",
    "    model_config_file = f\"COCO-InstanceSegmentation/{TRAINING_CONFIG['model_name']}.yaml\"\n",
    "    cfg.merge_from_file(model_zoo.get_config_file(model_config_file))\n",
    "    \n",
    "    cfg.DATASETS.TRAIN = (\"tomato_train\",)\n",
    "    cfg.DATASETS.TEST = (\"tomato_val\",)\n",
    "    \n",
    "    cfg.DATALOADER.NUM_WORKERS = TRAINING_CONFIG['num_workers']\n",
    "    \n",
    "    cfg.MODEL.ROI_HEADS.NUM_CLASSES = DATASET_CONFIG['num_classes']\n",
    "    cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(model_config_file)\n",
    "    \n",
    "    cfg.SOLVER.IMS_PER_BATCH = TRAINING_CONFIG['batch_size']\n",
    "    cfg.SOLVER.BASE_LR = TRAINING_CONFIG['base_lr']\n",
    "    \n",
    "    estimated_iters_per_epoch = 127\n",
    "    cfg.SOLVER.MAX_ITER = estimated_iters_per_epoch * TRAINING_CONFIG['max_epochs']\n",
    "    \n",
    "    cfg.SOLVER.STEPS = (int(cfg.SOLVER.MAX_ITER * 0.7), int(cfg.SOLVER.MAX_ITER * 0.9))\n",
    "    cfg.SOLVER.GAMMA = 0.1\n",
    "    \n",
    "    cfg.INPUT.MIN_SIZE_TRAIN = TRAINING_CONFIG['input_min_size_train']\n",
    "    cfg.INPUT.MAX_SIZE_TRAIN = TRAINING_CONFIG['input_max_size_train']\n",
    "    \n",
    "    cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = TRAINING_CONFIG['score_thresh_test']\n",
    "    \n",
    "    # RPN settings\n",
    "    cfg.MODEL.RPN.IOU_THRESHOLDS = [0.3, 0.7]\n",
    "    cfg.MODEL.RPN.BATCH_SIZE_PER_IMAGE = 512\n",
    "    cfg.MODEL.RPN.POSITIVE_FRACTION = 0.5\n",
    "\n",
    "    # ROI Head settings  \n",
    "    cfg.MODEL.ROI_HEADS.IOU_THRESHOLDS = [0.3]  # Lower IoU threshold\n",
    "    cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 512\n",
    "    cfg.MODEL.ROI_HEADS.POSITIVE_FRACTION = 0.25\n",
    "    cfg.MODEL.ROI_HEADS.NMS_THRESH_TEST = 0.5\n",
    "\n",
    "    # Anchor settings\n",
    "    cfg.MODEL.ANCHOR_GENERATOR.SIZES = [[16, 32, 64], [32, 64, 128], [64, 128, 256], [128, 256, 512], [256, 512, 1024]]\n",
    "    cfg.MODEL.ANCHOR_GENERATOR.ASPECT_RATIOS = [[0.5, 1.0, 2.0]]\n",
    "    \n",
    "    cfg.OUTPUT_DIR = DATASET_CONFIG['output_dir']\n",
    "    os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
    "    \n",
    "    cfg.MODEL.DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    \n",
    "    cfg.SOLVER.CHECKPOINT_PERIOD = estimated_iters_per_epoch * TRAINING_CONFIG['checkpoint_period']\n",
    "    \n",
    "    return cfg\n",
    "\n",
    "if conversion_success:\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"TomatoMAP-Seg Dataset Registration\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    class_labels = register_all_datasets()\n",
    "    \n",
    "    if class_labels is not None:\n",
    "        print(\"TomatoMAP-Seg datasets registered successfully\")\n",
    "        \n",
    "        cfg = build_cfg()\n",
    "        print(\"Configuration built successfully\")\n",
    "        \n",
    "        print(f\"Training Configuration Summary:\")\n",
    "        print(f\"  Model: {TRAINING_CONFIG['model_name']}\")\n",
    "        print(f\"  Number of classes: {DATASET_CONFIG['num_classes']}\")\n",
    "        print(f\"  Batch size: {TRAINING_CONFIG['batch_size']}\")\n",
    "        print(f\"  Learning rate: {TRAINING_CONFIG['base_lr']}\")\n",
    "        print(f\"  Max epochs: {TRAINING_CONFIG['max_epochs']}\")\n",
    "        print(f\"  Patience: {TRAINING_CONFIG['patience']}\")\n",
    "        print(f\"  Input size range: {TRAINING_CONFIG['input_min_size_train'][0]}-{TRAINING_CONFIG['input_max_size_train']}\")\n",
    "        print(f\"  Output directory: {cfg.OUTPUT_DIR}\")\n",
    "        print(f\"  Device: {cfg.MODEL.DEVICE}\")\n",
    "    else:\n",
    "        print(\"Dataset registration failed\")\n",
    "        conversion_success = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1358f40d-8f79-473b-9838-f9466eace435",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "检查标注:\n",
      "标注0: bbox=[703.5, 1221.3675444679664, 1089.5, 2113.474341649025], area=158628.0, category=8\n",
      "标注1: bbox=[1577.5, 1996.4246035444312, 2297.5, 2408.4991371865876], area=124862.5, category=8\n",
      "标注2: bbox=[1602.5, 3036.4418436943483, 2015.5, 3513.5], area=80343.5, category=7\n",
      "标注3: bbox=[1354.5, 3011.32917960675, 1519.5, 3302.5], area=25997.0, category=6\n",
      "标注4: bbox=[1526.5, 2799.3675444679666, 1799.4850712500727, 3023.5], area=25355.0, category=7\n"
     ]
    }
   ],
   "source": [
    "# 检查标注质量\n",
    "with open(\"TomatoMAP/TomatoMAP-Seg/cocoOut/val.json\", 'r') as f:\n",
    "    val_data = json.load(f)\n",
    "\n",
    "print(\"检查标注:\")\n",
    "for i, ann in enumerate(val_data['annotations'][:5]):\n",
    "    print(f\"标注{i}: bbox={ann['bbox']}, area={ann['area']}, category={ann['category_id']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c32a6e5-3c8b-4dcc-8330-90af90c680ac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Ready to start training\n",
      "============================================================\n",
      "Starting TomatoMAP-Seg Training\n",
      "\u001b[32m[07/17 15:28:22 d2.engine.defaults]: \u001b[0mModel:\n",
      "GeneralizedRCNN(\n",
      "  (backbone): FPN(\n",
      "    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (top_block): LastLevelMaxPool()\n",
      "    (bottom_up): ResNet(\n",
      "      (stem): BasicStem(\n",
      "        (conv1): Conv2d(\n",
      "          3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "        )\n",
      "      )\n",
      "      (res2): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (res3): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (3): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (res4): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (3): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (4): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (5): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (res5): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (proposal_generator): RPN(\n",
      "    (rpn_head): StandardRPNHead(\n",
      "      (conv): Conv2d(\n",
      "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (objectness_logits): Conv2d(256, 9, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (anchor_deltas): Conv2d(256, 36, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "    (anchor_generator): DefaultAnchorGenerator(\n",
      "      (cell_anchors): BufferList()\n",
      "    )\n",
      "  )\n",
      "  (roi_heads): StandardROIHeads(\n",
      "    (box_pooler): ROIPooler(\n",
      "      (level_poolers): ModuleList(\n",
      "        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n",
      "        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n",
      "        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n",
      "        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n",
      "      )\n",
      "    )\n",
      "    (box_head): FastRCNNConvFCHead(\n",
      "      (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "      (fc1): Linear(in_features=12544, out_features=1024, bias=True)\n",
      "      (fc_relu1): ReLU()\n",
      "      (fc2): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (fc_relu2): ReLU()\n",
      "    )\n",
      "    (box_predictor): FastRCNNOutputLayers(\n",
      "      (cls_score): Linear(in_features=1024, out_features=11, bias=True)\n",
      "      (bbox_pred): Linear(in_features=1024, out_features=40, bias=True)\n",
      "    )\n",
      "    (mask_pooler): ROIPooler(\n",
      "      (level_poolers): ModuleList(\n",
      "        (0): ROIAlign(output_size=(14, 14), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n",
      "        (1): ROIAlign(output_size=(14, 14), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n",
      "        (2): ROIAlign(output_size=(14, 14), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n",
      "        (3): ROIAlign(output_size=(14, 14), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n",
      "      )\n",
      "    )\n",
      "    (mask_head): MaskRCNNConvUpsampleHead(\n",
      "      (mask_fcn1): Conv2d(\n",
      "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (mask_fcn2): Conv2d(\n",
      "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (mask_fcn3): Conv2d(\n",
      "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (mask_fcn4): Conv2d(\n",
      "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (deconv): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))\n",
      "      (deconv_relu): ReLU()\n",
      "      (predictor): Conv2d(256, 10, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\u001b[32m[07/17 15:28:22 d2.data.datasets.coco]: \u001b[0mLoaded 508 images in COCO format from TomatoMAP/TomatoMAP-Seg/cocoOut/train.json\n",
      "\u001b[32m[07/17 15:28:22 d2.data.build]: \u001b[0mRemoved 2 images with no usable annotations. 506 images left.\n",
      "\u001b[32m[07/17 15:28:22 d2.data.build]: \u001b[0mDistribution of instances among all 10 categories:\n",
      "\u001b[36m|  category  | #instances   |  category  | #instances   |   category    | #instances   |\n",
      "|:----------:|:-------------|:----------:|:-------------|:-------------:|:-------------|\n",
      "|  nascent   | 425          |    mini    | 196          | unripe gree.. | 682          |\n",
      "| semi ripe  | 225          | fully ripe | 1459         |      2mm      | 181          |\n",
      "|    4mm     | 453          |    6mm     | 518          |      8mm      | 209          |\n",
      "|    12mm    | 191          |            |              |               |              |\n",
      "|   total    | 4539         |            |              |               |              |\u001b[0m\n",
      "\u001b[32m[07/17 15:28:22 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip()]\n",
      "\u001b[32m[07/17 15:28:22 d2.data.build]: \u001b[0mUsing training sampler TrainingSampler\n",
      "\u001b[32m[07/17 15:28:22 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001b[32m[07/17 15:28:22 d2.data.common]: \u001b[0mSerializing 506 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[07/17 15:28:22 d2.data.common]: \u001b[0mSerialized dataset takes 18.57 MiB\n",
      "\u001b[32m[07/17 15:28:22 d2.data.build]: \u001b[0mMaking batched data loader with batch_size=4\n",
      "\u001b[32m[07/17 15:28:23 d2.data.datasets.coco]: \u001b[0mLoaded 508 images in COCO format from TomatoMAP/TomatoMAP-Seg/cocoOut/train.json\n",
      "\u001b[32m[07/17 15:28:23 d2.data.build]: \u001b[0mRemoved 2 images with no usable annotations. 506 images left.\n",
      "\u001b[32m[07/17 15:28:23 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip()]\n",
      "\u001b[32m[07/17 15:28:23 d2.data.build]: \u001b[0mUsing training sampler TrainingSampler\n",
      "\u001b[32m[07/17 15:28:23 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001b[32m[07/17 15:28:23 d2.data.common]: \u001b[0mSerializing 506 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[07/17 15:28:23 d2.data.common]: \u001b[0mSerialized dataset takes 18.57 MiB\n",
      "\u001b[32m[07/17 15:28:23 d2.data.build]: \u001b[0mMaking batched data loader with batch_size=4\n",
      "Using default iterations per epoch: 127\n",
      "\u001b[32m[07/17 15:28:23 d2.checkpoint.detection_checkpoint]: \u001b[0m[DetectionCheckpointer] Loading from https://dl.fbaipublicfiles.com/detectron2/COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_1x/137260431/model_final_a54504.pkl ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skip loading parameter 'proposal_generator.rpn_head.objectness_logits.weight' to the model due to incompatible shapes: (3, 256, 1, 1) in the checkpoint but (9, 256, 1, 1) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'proposal_generator.rpn_head.objectness_logits.bias' to the model due to incompatible shapes: (3,) in the checkpoint but (9,) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'proposal_generator.rpn_head.anchor_deltas.weight' to the model due to incompatible shapes: (12, 256, 1, 1) in the checkpoint but (36, 256, 1, 1) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'proposal_generator.rpn_head.anchor_deltas.bias' to the model due to incompatible shapes: (12,) in the checkpoint but (36,) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'roi_heads.box_predictor.cls_score.weight' to the model due to incompatible shapes: (81, 1024) in the checkpoint but (11, 1024) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'roi_heads.box_predictor.cls_score.bias' to the model due to incompatible shapes: (81,) in the checkpoint but (11,) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'roi_heads.box_predictor.bbox_pred.weight' to the model due to incompatible shapes: (320, 1024) in the checkpoint but (40, 1024) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'roi_heads.box_predictor.bbox_pred.bias' to the model due to incompatible shapes: (320,) in the checkpoint but (40,) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'roi_heads.mask_head.predictor.weight' to the model due to incompatible shapes: (80, 256, 1, 1) in the checkpoint but (10, 256, 1, 1) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'roi_heads.mask_head.predictor.bias' to the model due to incompatible shapes: (80,) in the checkpoint but (10,) in the model! You might want to double check if this is expected.\n",
      "Some model parameters or buffers are not found in the checkpoint:\n",
      "\u001b[34mproposal_generator.rpn_head.anchor_deltas.{bias, weight}\u001b[0m\n",
      "\u001b[34mproposal_generator.rpn_head.objectness_logits.{bias, weight}\u001b[0m\n",
      "\u001b[34mroi_heads.box_predictor.bbox_pred.{bias, weight}\u001b[0m\n",
      "\u001b[34mroi_heads.box_predictor.cls_score.{bias, weight}\u001b[0m\n",
      "\u001b[34mroi_heads.mask_head.predictor.{bias, weight}\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Training Configuration:\n",
      "  Model: mask_rcnn_R_50_FPN_1x\n",
      "  Max epochs: 100\n",
      "  Patience: 15 epochs\n",
      "  Evaluation period: every 10 epochs\n",
      "  Checkpoint saving: every 10 epochs\n",
      "  Multi-scale training: 640-1333\n",
      "\n",
      "============================================================\n",
      "Training Started\n",
      "============================================================\n",
      "\u001b[32m[07/17 15:28:23 d2.engine.train_loop]: \u001b[0mStarting training from iteration 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/envs/py310/lib/python3.10/site-packages/torch/functional.py:554: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /pytorch/aten/src/ATen/native/TensorShape.cpp:4314.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[07/17 15:28:33 d2.utils.events]: \u001b[0m eta: 1:28:42  iter: 19  total_loss: 4.989  loss_cls: 2.755  loss_box_reg: 0.5471  loss_mask: 0.6949  loss_rpn_cls: 0.7316  loss_rpn_loc: 0.2247    time: 0.4265  last_time: 0.4423  data_time: 0.0677  last_data_time: 0.0115   lr: 4.7954e-06  max_mem: 4480M\n",
      "\u001b[32m[07/17 15:28:42 d2.utils.events]: \u001b[0m eta: 1:28:33  iter: 39  total_loss: 4.638  loss_cls: 2.409  loss_box_reg: 0.5982  loss_mask: 0.6931  loss_rpn_cls: 0.7198  loss_rpn_loc: 0.2258    time: 0.4325  last_time: 0.6387  data_time: 0.0273  last_data_time: 0.2149   lr: 9.5906e-06  max_mem: 4482M\n",
      "\u001b[32m[07/17 15:28:51 d2.utils.events]: \u001b[0m eta: 1:28:56  iter: 59  total_loss: 4.038  loss_cls: 1.791  loss_box_reg: 0.7335  loss_mask: 0.6904  loss_rpn_cls: 0.7067  loss_rpn_loc: 0.1572    time: 0.4310  last_time: 0.4100  data_time: 0.0107  last_data_time: 0.0161   lr: 1.4386e-05  max_mem: 4483M\n",
      "\u001b[32m[07/17 15:28:59 d2.utils.events]: \u001b[0m eta: 1:29:35  iter: 79  total_loss: 3.469  loss_cls: 1.051  loss_box_reg: 0.6718  loss_mask: 0.6843  loss_rpn_cls: 0.6801  loss_rpn_loc: 0.208    time: 0.4306  last_time: 0.4258  data_time: 0.0188  last_data_time: 0.0169   lr: 1.9181e-05  max_mem: 4483M\n"
     ]
    }
   ],
   "source": [
    "def train_model():\n",
    "    print(\"Starting TomatoMAP-Seg Training\")\n",
    "    \n",
    "    trainer = MyTrainer(cfg, patience=TRAINING_CONFIG['patience'])\n",
    "    trainer.resume_or_load(resume=False)\n",
    "    \n",
    "    print(f\"\\n Training Configuration:\")\n",
    "    print(f\"  Model: {TRAINING_CONFIG['model_name']}\")\n",
    "    print(f\"  Max epochs: {TRAINING_CONFIG['max_epochs']}\")\n",
    "    print(f\"  Patience: {TRAINING_CONFIG['patience']} epochs\")\n",
    "    print(f\"  Evaluation period: every {TRAINING_CONFIG['eval_period']} epochs\")\n",
    "    print(f\"  Checkpoint saving: every {TRAINING_CONFIG['checkpoint_period']} epochs\")\n",
    "    print(f\"  Multi-scale training: {TRAINING_CONFIG['input_min_size_train'][0]}-{TRAINING_CONFIG['input_max_size_train']}\")\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Training Started\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    try:\n",
    "        trainer.train()\n",
    "        \n",
    "        print(\"\\n Training completed successfully\")\n",
    "        \n",
    "        config_path = os.path.join(cfg.OUTPUT_DIR, \"config.yaml\")\n",
    "        with open(config_path, \"w\") as f:\n",
    "            f.write(cfg.dump())\n",
    "        print(f\"Configuration saved: {config_path}\")\n",
    "        \n",
    "        return trainer, cfg\n",
    "        \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n Training interrupted by user\")\n",
    "        return None, cfg\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n Error occurred during training:\")\n",
    "        print(f\"   Error details: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None, cfg\n",
    "\n",
    "if 'class_labels' in locals() and class_labels is not None and conversion_success:\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"Ready to start training\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    trainer, cfg = train_model()\n",
    "    \n",
    "    if trainer is not None:\n",
    "        print(\"\\n Training finished successfully\")\n",
    "        \n",
    "        print(f\"\\n Output directory contents:\")\n",
    "        output_dir = Path(cfg.OUTPUT_DIR)\n",
    "        if output_dir.exists():\n",
    "            for file in output_dir.iterdir():\n",
    "                if file.is_file():\n",
    "                    print(f\"  📄 {file.name}\")\n",
    "    else:\n",
    "        print(\"\\n Training failed or was interrupted\")\n",
    "else:\n",
    "    print(\"Cannot start training - please check dataset structure and conversion status\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "be5227b6-3e5a-4f40-8192-9a9148fe2aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model_path=\"model_best.pth\", dataset_name=\"tomato_test\"):\n",
    "    print(f\"Evaluating model on {dataset_name}...\")\n",
    "    \n",
    "    eval_cfg = build_cfg()\n",
    "    \n",
    "    full_model_path = os.path.join(eval_cfg.OUTPUT_DIR, model_path)\n",
    "    if not os.path.exists(full_model_path):\n",
    "        print(f\"Model file does not exist: {full_model_path}\")\n",
    "\n",
    "        final_model_path = os.path.join(eval_cfg.OUTPUT_DIR, \"model_final.pth\")\n",
    "        if os.path.exists(final_model_path):\n",
    "            full_model_path = final_model_path\n",
    "            print(f\"Using final model instead: {final_model_path}\")\n",
    "        else:\n",
    "            print(\"No model files found for evaluation\")\n",
    "            return None\n",
    "    \n",
    "    eval_cfg.MODEL.WEIGHTS = full_model_path\n",
    "    print(f\"Loading model: {full_model_path}\")\n",
    "    \n",
    "    try:\n",
    "        evaluator = COCOEvaluator(dataset_name, eval_cfg, False, output_dir=eval_cfg.OUTPUT_DIR)\n",
    "        test_loader = build_detection_test_loader(eval_cfg, dataset_name)\n",
    "        \n",
    "        model = MyTrainer.build_model(eval_cfg)\n",
    "        \n",
    "        print(\"Starting evaluation...\")\n",
    "        results = inference_on_dataset(model, test_loader, evaluator)\n",
    "        \n",
    "        print(\"\\n Evaluation Results:\")\n",
    "        \n",
    "        if \"bbox\" in results:\n",
    "            print(\"\\n Bounding Box Results:\")\n",
    "            bbox_results = results[\"bbox\"]\n",
    "            for key in [\"AP\", \"AP50\", \"AP75\", \"APs\", \"APm\", \"APl\"]:\n",
    "                value = bbox_results.get(key, -1)\n",
    "                if value != -1:\n",
    "                    print(f\"  {key}: {value:.4f}\")\n",
    "                else:\n",
    "                    print(f\"  {key}: N/A\")\n",
    "        \n",
    "        if \"segm\" in results:\n",
    "            print(\"\\n Segmentation Results:\")\n",
    "            segm_results = results[\"segm\"]\n",
    "            for key in [\"AP\", \"AP50\", \"AP75\", \"APs\", \"APm\", \"APl\"]:\n",
    "                value = segm_results.get(key, -1)\n",
    "                if value != -1:\n",
    "                    print(f\"  {key}: {value:.4f}\")\n",
    "                else:\n",
    "                    print(f\"  {key}: N/A (no objects in this size category)\")\n",
    "        \n",
    "        results_file = os.path.join(eval_cfg.OUTPUT_DIR, f\"eval_results_{dataset_name}.json\")\n",
    "        with open(results_file, 'w') as f:\n",
    "            json.dump(results, f, indent=2)\n",
    "        print(f\"\\n Evaluation results saved to: {results_file}\")\n",
    "        \n",
    "        return results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Evaluation failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "if 'trainer' in locals() and trainer is not None:\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"Model Evaluation\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    test_results = evaluate_model(\"model_best.pth\", \"tomato_test\")\n",
    "    \n",
    "    final_results = evaluate_model(\"model_final.pth\", \"tomato_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "008a2d8d-18f5-4200-bd84-a57d6f251d76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output saved.\n"
     ]
    }
   ],
   "source": [
    "def visualize_predictions(dataset_name=\"tomato_test\", num_samples=5, model_path=\"model_best.pth\"):\n",
    "    print(f\"Generating prediction visualizations for {dataset_name}\")\n",
    "    \n",
    "    vis_cfg = build_cfg()\n",
    "    full_model_path = os.path.join(vis_cfg.OUTPUT_DIR, model_path)\n",
    "    \n",
    "    if not os.path.exists(full_model_path):\n",
    "        print(f\"Model file does not exist: {full_model_path}\")\n",
    "\n",
    "        final_model_path = os.path.join(vis_cfg.OUTPUT_DIR, \"model_final.pth\")\n",
    "        if os.path.exists(final_model_path):\n",
    "            full_model_path = final_model_path\n",
    "            print(f\"Using final model instead: {final_model_path}\")\n",
    "        else:\n",
    "            print(\"No model files found for visualization\")\n",
    "            return\n",
    "    \n",
    "    vis_cfg.MODEL.WEIGHTS = full_model_path\n",
    "    \n",
    "    predictor = DefaultPredictor(vis_cfg)\n",
    "    \n",
    "    try:\n",
    "        metadata = MetadataCatalog.get(dataset_name)\n",
    "    except:\n",
    "        print(f\"Cannot get metadata for {dataset_name}\")\n",
    "        metadata = None\n",
    "    \n",
    "    img_dir = DATASET_CONFIG['img_dir']\n",
    "    if not os.path.exists(img_dir):\n",
    "        print(f\"Image directory does not exist: {img_dir}\")\n",
    "        return\n",
    "    \n",
    "    img_list = [f for f in os.listdir(img_dir) \n",
    "                if f.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp', '.tiff'))]\n",
    "    \n",
    "    if not img_list:\n",
    "        print(f\"No images found in {img_dir}\")\n",
    "        return\n",
    "    \n",
    "    random.shuffle(img_list)\n",
    "    shown = 0\n",
    "    \n",
    "    print(f\"Using {model_path} to generate {num_samples} visualization samples...\")\n",
    "    \n",
    "    for file in img_list:\n",
    "        try:\n",
    "            img_path = os.path.join(img_dir, file)\n",
    "            im = cv2.imread(img_path)\n",
    "            \n",
    "            if im is None:\n",
    "                print(f\"Failed to load image: {img_path}\")\n",
    "                continue\n",
    "            \n",
    "            outputs = predictor(im)\n",
    "            \n",
    "            v = Visualizer(im[:, :, ::-1], metadata=metadata, scale=1.2)\n",
    "            v._default_font_size = 20\n",
    "            out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
    "            \n",
    "            save_path = os.path.join(vis_cfg.OUTPUT_DIR, f\"prediction_{shown+1}_{file}\")\n",
    "            cv2.imwrite(save_path, out.get_image()[:, :, ::-1])\n",
    "            print(f\"  Saved visualization: {save_path}\")\n",
    "            \n",
    "            shown += 1\n",
    "            if shown >= num_samples:\n",
    "                break\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    print(\"Visualization generation completed!\")\n",
    "\n",
    "def plot_training_history():\n",
    "    history_file = os.path.join(cfg.OUTPUT_DIR, \"training_history.json\")\n",
    "    \n",
    "    if not os.path.exists(history_file):\n",
    "        print(f\"Training history file not found: {history_file}\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        with open(history_file, 'r') as f:\n",
    "            history = json.load(f)\n",
    "        \n",
    "        if not history:\n",
    "            print(\"Training history is empty\")\n",
    "            return\n",
    "        \n",
    "        epochs = [h['epoch'] for h in history]\n",
    "        scores = [h['score'] for h in history]\n",
    "        \n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(epochs, scores, 'b-o', linewidth=2, markersize=6)\n",
    "        plt.title(f'Training Progress - {history[0][\"metric\"]}', fontsize=14)\n",
    "        plt.xlabel('Epoch', fontsize=12)\n",
    "        plt.ylabel(f'{history[0][\"metric\"]}', fontsize=12)\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        best_idx = scores.index(max(scores))\n",
    "        plt.annotate(f'Best: {max(scores):.4f}', \n",
    "                    xy=(epochs[best_idx], scores[best_idx]),\n",
    "                    xytext=(10, 10), textcoords='offset points',\n",
    "                    bbox=dict(boxstyle='round,pad=0.3', facecolor='yellow', alpha=0.7),\n",
    "                    arrowprops=dict(arrowstyle='->', connectionstyle='arc3,rad=0'))\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        curve_path = os.path.join(cfg.OUTPUT_DIR, \"training_curve.png\")\n",
    "        plt.savefig(curve_path, dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        print(f\"Training curve saved to: {curve_path}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Failed to plot training history: {e}\")\n",
    "\n",
    "if 'trainer' in locals() and trainer is not None:\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"Generating Visualizations\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    visualize_predictions(\"tomato_test\", num_samples=3, model_path=\"model_best.pth\")\n",
    "    \n",
    "    plot_training_history()\n",
    "    \n",
    "    print(\"All visualizations completed!\")\n",
    "\n",
    "print(\"TomatoMAP-Seg training pipeline finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6461689-1c02-4110-b91b-b731669c5109",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
