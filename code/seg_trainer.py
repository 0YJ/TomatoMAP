import os
import cv2
import random
import torch
import json
import numpy as np
from collections import OrderedDict

from detectron2.engine import DefaultTrainer, HookBase
from detectron2.engine import DefaultPredictor
from detectron2.config import get_cfg
from detectron2 import model_zoo
from detectron2.data import MetadataCatalog, build_detection_test_loader, build_detection_train_loader
from detectron2.data.datasets import register_coco_instances
from detectron2.evaluation import COCOEvaluator, inference_on_dataset
from detectron2.utils.visualizer import Visualizer
from detectron2.utils.events import get_event_storage

DATASET_ROOT = "./"
IMG_DIR = os.path.join(DATASET_ROOT, "TomatoMAP/TomatoMAP-Seg/images")
ANN_DIR = os.path.join(DATASET_ROOT, "TomatoMAP/TomatoMAP-Seg/cocoOut")
DEFAULT_OUTPUT_DIR = "./output"  # é»˜è®¤è¾“å‡ºç›®å½•
NUM_CLASSES = 10
MAX_EPOCHS = 100  # è®¾ç½®æœ€å¤§epochæ•°
PATIENCE = 5     # æ—©åœpatience

# æ£€æŸ¥æ•°æ®é›†è·¯å¾„æ˜¯å¦å­˜åœ¨
if not os.path.exists(IMG_DIR):
    print(f"Warning: Image directory {IMG_DIR} does not exist!")
if not os.path.exists(ANN_DIR):
    print(f"Warning: Annotation directory {ANN_DIR} does not exist!")

# æ³¨å†Œæ•°æ®é›†
register_coco_instances("tomato_train", {}, os.path.join(ANN_DIR, "train.json"), IMG_DIR)
register_coco_instances("tomato_val", {}, os.path.join(ANN_DIR, "val.json"), IMG_DIR)
register_coco_instances("tomato_test", {}, os.path.join(ANN_DIR, "test.json"), IMG_DIR)

class BestModelHook(HookBase):
    """Hook to save the best model based on validation segmentation mAP"""
    
    def __init__(self, cfg, eval_period, patience=10):
        self.cfg = cfg.clone()
        self.eval_period = eval_period
        self.patience = patience
        self.best_score = 0  # ä½¿ç”¨0è€Œä¸æ˜¯-1
        self.best_metric_name = None
        self.best_epoch = -1
        self.epochs_without_improvement = 0
        self.should_stop = False
        self.history = []  # è®°å½•å†å²
        
    def get_valid_score(self, segm_results):
        """è·å–æœ‰æ•ˆçš„è¯„ä¼°åˆ†æ•°ï¼Œå¿½ç•¥-1å€¼"""
        # æŒ‰ä¼˜å…ˆçº§å°è¯•ä¸åŒçš„æŒ‡æ ‡
        priority_metrics = ["AP", "AP50", "AP75", "APm", "APl"]
        
        for metric in priority_metrics:
            value = segm_results.get(metric, -1)
            if value != -1:  # åªè¿”å›æœ‰æ•ˆå€¼
                return metric, value
        
        return None, None
    
    def after_step(self):
        next_iter = self.trainer.iter + 1
        is_final_iter = next_iter == self.trainer.max_iter
        
        if (next_iter % self.eval_period == 0 and not is_final_iter):
            current_epoch = (next_iter // self.eval_period)
            
            # æ‰§è¡Œè¯„ä¼°
            results = self._do_eval()
            if results is None:
                print(f"Epoch {current_epoch}: Evaluation failed")
                return
            
            segm_results = results.get("segm", {})
            bbox_results = results.get("bbox", {})
            
            # è·å–æœ‰æ•ˆåˆ†æ•°
            metric_name, current_score = self.get_valid_score(segm_results)
            
            # æ‰“å°æ‰€æœ‰ç»“æœ
            print(f"\n{'='*60}")
            print(f"Epoch {current_epoch} Evaluation Results:")
            print(f"{'='*60}")
            
            # æ‰“å°bboxæŒ‡æ ‡
            print("\nBounding Box Metrics:")
            for key in ["AP", "AP50", "AP75", "APs", "APm", "APl"]:
                value = bbox_results.get(key, -1)
                if value != -1:
                    print(f"  {key}: {value:.4f} âœ“")
                else:
                    print(f"  {key}: N/A")
            
            # æ‰“å°segmentationæŒ‡æ ‡
            print("\nSegmentation Metrics:")
            for key in ["AP", "AP50", "AP75", "APs", "APm", "APl"]:
                value = segm_results.get(key, -1)
                if value != -1:
                    print(f"  {key}: {value:.4f} âœ“")
                else:
                    print(f"  {key}: N/A (no objects in this category)")
            
            # å¦‚æœæ²¡æœ‰æœ‰æ•ˆæŒ‡æ ‡
            if metric_name is None:
                print("\nâš ï¸ Warning: No valid metrics available for evaluation!")
                print("This might happen if your dataset has no objects after augmentation.")
                # å°è¯•ä½¿ç”¨bboxæŒ‡æ ‡ä½œä¸ºå¤‡é€‰
                metric_name, current_score = self.get_valid_score(bbox_results)
                if metric_name is not None:
                    print(f"Using bbox metric instead: {metric_name} = {current_score:.4f}")
                else:
                    return
            
            print(f"\n Primary metric: {metric_name} = {current_score:.4f}")
            
            # è®°å½•å†å²
            self.history.append({
                'epoch': current_epoch,
                'metric': metric_name,
                'score': current_score,
                'all_metrics': {**segm_results, **{'bbox_' + k: v for k, v in bbox_results.items()}}
            })
            
            # æ£€æŸ¥æ˜¯å¦æ”¹å–„
            if current_score > self.best_score:
                improvement = current_score - self.best_score
                self.best_score = current_score
                self.best_metric_name = metric_name
                self.best_epoch = current_epoch
                self.epochs_without_improvement = 0
                
                # ä¿å­˜æ¨¡å‹
                self.trainer.checkpointer.save("model_best")
                print(f"\nğŸ‰ New best model saved!")
                print(f"   Score: {current_score:.4f} (â†‘{improvement:.4f})")
                
                # ä¿å­˜è¯„ä¼°ç»“æœ
                best_results_file = os.path.join(self.cfg.OUTPUT_DIR, "best_results.json")
                with open(best_results_file, 'w') as f:
                    json.dump({
                        'epoch': current_epoch,
                        'metric': metric_name,
                        'score': current_score,
                        'segm_results': segm_results,
                        'bbox_results': bbox_results
                    }, f, indent=2)
            else:
                self.epochs_without_improvement += 1
                gap = self.best_score - current_score
                print(f"\nCurrent: {current_score:.4f} | Best: {self.best_score:.4f} (gap: {gap:.4f})")
                print(f"No improvement for {self.epochs_without_improvement}/{self.patience} epochs")
            
            # æ—©åœæ£€æŸ¥
            if self.epochs_without_improvement >= self.patience:
                print(f"\n{'='*60}")
                print(f"EARLY STOPPING TRIGGERED")
                print(f"   Best {self.best_metric_name}: {self.best_score:.4f} at epoch {self.best_epoch}")
                print(f"   Total epochs trained: {current_epoch}")
                print(f"{'='*60}")
                self.should_stop = True
                # ç«‹å³åœæ­¢è®­ç»ƒ
                self.trainer.storage._iter = self.trainer.max_iter
    
    def _do_eval(self):
        """æ‰§è¡ŒéªŒè¯è¯„ä¼°"""
        try:
            evaluator = COCOEvaluator("tomato_val", self.cfg, False, 
                                    output_dir=os.path.join(self.cfg.OUTPUT_DIR, "inference"))
            val_loader = build_detection_test_loader(self.cfg, "tomato_val")
            results = inference_on_dataset(self.trainer.model, val_loader, evaluator)
            return results
        except Exception as e:
            print(f"Evaluation failed: {e}")
            import traceback
            traceback.print_exc()
            return None

class MyTrainer(DefaultTrainer):
    def __init__(self, cfg, patience=None):
        #super().__init__(cfg)
        self.patience = patience if patience is not None else PATIENCE
        super().__init__(cfg)
    @classmethod
    def build_evaluator(cls, cfg, dataset_name, output_folder=None):
        if output_folder is None:
            output_folder = os.path.join(cfg.OUTPUT_DIR, "inference")
        return COCOEvaluator(
            dataset_name=dataset_name,
            distributed=False,
            output_dir=output_folder,
            use_fast_impl=True,
            tasks=("bbox", "segm"),
        )
    
    def build_hooks(self):
        hooks = super().build_hooks()
        
        # è®¡ç®—æ¯ä¸ªepochçš„è¿­ä»£æ¬¡æ•°
        try:
            train_loader = build_detection_train_loader(self.cfg)
            iters_per_epoch = len(train_loader) // self.cfg.SOLVER.IMS_PER_BATCH
            print(f"Iterations per epoch: {iters_per_epoch}")
        except:
            # å¦‚æœæ— æ³•è·å–ç¡®åˆ‡æ•°é‡ï¼Œä½¿ç”¨ä¼°ç®—å€¼
            iters_per_epoch = 127
            print(f"Using estimated iterations per epoch: {iters_per_epoch}")
        
        eval_period = iters_per_epoch * 10  # æ¯10ä¸ªepochè¯„ä¼°ä¸€æ¬¡,5ä¸ªpatient
        
        # æ·»åŠ æœ€ä½³æ¨¡å‹ä¿å­˜å’Œæ—©åœhook
        best_model_hook = BestModelHook(self.cfg, eval_period, self.patience)
        hooks.append(best_model_hook)
        
        # å­˜å‚¨hookå¼•ç”¨ä»¥ä¾¿æ£€æŸ¥æ—©åœæ¡ä»¶
        self.best_model_hook = best_model_hook
        
        return hooks
    
    def run_step(self):
        """é‡å†™ run_step ä»¥æ”¯æŒæ—©åœ"""
        super().run_step()
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦æ—©åœ
        if hasattr(self, 'best_model_hook') and self.best_model_hook.should_stop:
            print("Early stopping condition met. Stopping training...")
            self.storage._iter = self.max_iter
    
    def train(self):
        """é‡å†™trainæ–¹æ³•ä»¥æ”¯æŒæ—©åœ"""
        super().train()
        
        # è®­ç»ƒç»“æŸåæ‰“å°æœ€ä½³ç»“æœ
        if hasattr(self, 'best_model_hook'):
            print(f"\n{'='*60}")
            print(f"Training Summary:")
            print(f"{'='*60}")
            if self.best_model_hook.best_score > 0:
                print(f"Best {self.best_model_hook.best_metric_name}: {self.best_model_hook.best_score:.4f}")
                print(f"Best epoch: {self.best_model_hook.best_epoch}")
                print(f"Best model saved as: model_best.pth")
            else:
                print("No valid metrics were found during training.")
            
            # ä¿å­˜è®­ç»ƒå†å²
            history_file = os.path.join(self.cfg.OUTPUT_DIR, "training_history.json")
            with open(history_file, 'w') as f:
                json.dump(self.best_model_hook.history, f, indent=2)
            print(f"Training history saved to: {history_file}")

def build_cfg(model_config=None, base_lr=None, max_epochs=None, output_dir=None):
    cfg = get_cfg()
    
    # ä½¿ç”¨æŒ‡å®šçš„æ¨¡å‹é…ç½®ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨é»˜è®¤
    if model_config is None:
        model_config = "COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_1x.yaml"
    
    cfg.merge_from_file(model_zoo.get_config_file(model_config))

    cfg.DATASETS.TRAIN = ("tomato_train",)
    cfg.DATASETS.TEST = ("tomato_val",)
    cfg.DATALOADER.NUM_WORKERS = 8

    cfg.MODEL.ROI_HEADS.NUM_CLASSES = NUM_CLASSES
    cfg.SOLVER.IMS_PER_BATCH = 4
    cfg.SOLVER.BASE_LR = base_lr if base_lr is not None else 0.0001
    
    # å¯¹äº5Kå›¾åƒï¼Œå¯ä»¥è€ƒè™‘ä½¿ç”¨æ›´å¤§çš„è®­ç»ƒå°ºå¯¸
    # é»˜è®¤è®¾ç½®
    cfg.INPUT.MIN_SIZE_TRAIN = (640, 672, 704, 736, 768, 800)
    cfg.INPUT.MAX_SIZE_TRAIN = 1333
    
    # ä½¿ç”¨epochè€Œä¸æ˜¯iter - ä¼°ç®—æ€»è¿­ä»£æ¬¡æ•°
    # æ ¹æ®å®é™…è®­ç»ƒé›†å¤§å°è°ƒæ•´
    estimated_iters_per_epoch = 127
    epochs = max_epochs if max_epochs is not None else MAX_EPOCHS
    cfg.SOLVER.MAX_ITER = estimated_iters_per_epoch * epochs
    
    cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.1

    # æ ¹æ®æ¨¡å‹é…ç½®æ–‡ä»¶è·å–å¯¹åº”çš„é¢„è®­ç»ƒæƒé‡
    # å°†é…ç½®æ–‡ä»¶åä¸­çš„yamlæ›¿æ¢æ‰ä»¥åŒ¹é…æƒé‡URL
    weight_config = model_config.replace("_1x.yaml", "_3x.yaml").replace("_3x.yaml", "_3x.yaml")
    cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(weight_config)
    
    cfg.OUTPUT_DIR = output_dir if output_dir is not None else DEFAULT_OUTPUT_DIR
    os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)
    
    # è®¾ç½®checkpointä¿å­˜å‘¨æœŸï¼ˆæ¯5ä¸ªepochä¿å­˜ä¸€æ¬¡ï¼‰
    cfg.SOLVER.CHECKPOINT_PERIOD = estimated_iters_per_epoch * 10

    return cfg

def train(model_config=None, base_lr=None, max_epochs=None, patience=None, output_dir=None):
    """è®­ç»ƒå‡½æ•°"""
    cfg = build_cfg(model_config=model_config, base_lr=base_lr, max_epochs=max_epochs, output_dir=output_dir)
    trainer = MyTrainer(cfg, patience=patience)
    trainer.resume_or_load(resume=False)
    
    epochs = max_epochs if max_epochs is not None else MAX_EPOCHS
    pat = patience if patience is not None else PATIENCE
    
    print(f"\n{'='*60}")
    print(f"Starting training with the following settings:")
    print(f"  Model: {model_config or 'mask_rcnn_R_50_FPN_1x.yaml'}")
    print(f"  Maximum epochs: {epochs}")
    print(f"  Early stopping patience: {pat} epochs")
    print(f"  Output directory: {cfg.OUTPUT_DIR}")
    print(f"  Images per batch: {cfg.SOLVER.IMS_PER_BATCH}")
    print(f"  Base learning rate: {cfg.SOLVER.BASE_LR}")
    print(f"  Input size range: {cfg.INPUT.MIN_SIZE_TRAIN[0]}-{cfg.INPUT.MAX_SIZE_TRAIN}")
    print(f"{'='*60}\n")
    
    trainer.train()

def evaluate(model_path="model_best.pth", output_dir=None):
    """åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°æ¨¡å‹"""
    cfg = build_cfg(output_dir=output_dir)  # ä½¿ç”¨é»˜è®¤é…ç½®
    out_dir = output_dir if output_dir is not None else DEFAULT_OUTPUT_DIR
    model_file = os.path.join(out_dir, model_path)
    
    if not os.path.exists(model_file):
        print(f"Model file {model_file} not found! Using model_final.pth instead.")
        model_file = os.path.join(out_dir, "model_final.pth")
    
    if not os.path.exists(model_file):
        print(f"No trained model found in {out_dir}")
        return
    
    cfg.MODEL.WEIGHTS = model_file
    print(f"Evaluating model: {model_file}")
    
    try:
        evaluator = COCOEvaluator("tomato_test", cfg, False, output_dir=out_dir)
        val_loader = build_detection_test_loader(cfg, "tomato_test")
        model = MyTrainer.build_model(cfg)
        DefaultTrainer._load_checkpoint_to_model(model, model_file)
        results = inference_on_dataset(model, val_loader, evaluator)
        
        print("\nTest Evaluation Results:")
        print(json.dumps(results, indent=2))
        
        # ä¿å­˜ç»“æœåˆ°æ–‡ä»¶
        results_file = os.path.join(out_dir, f"test_results_{model_path.replace('.pth', '')}.json")
        with open(results_file, 'w') as f:
            json.dump(results, f, indent=2)
        print(f"Results saved to: {results_file}")
        
    except Exception as e:
        print(f"Evaluation failed: {e}")
        import traceback
        traceback.print_exc()

def visualize_random_sample(n=3, model_path="model_best.pth", output_dir=None):
    """å¯è§†åŒ–éšæœºæ ·æœ¬çš„é¢„æµ‹ç»“æœ"""
    cfg = build_cfg(output_dir=output_dir)  # ä½¿ç”¨é»˜è®¤é…ç½®
    out_dir = output_dir if output_dir is not None else DEFAULT_OUTPUT_DIR
    model_file = os.path.join(out_dir, model_path)
    
    if not os.path.exists(model_file):
        print(f"Model file {model_file} not found! Using model_final.pth instead.")
        model_file = os.path.join(out_dir, "model_final.pth")
    
    if not os.path.exists(model_file):
        print(f"No trained model found in {out_dir}")
        return
    
    cfg.MODEL.WEIGHTS = model_file
    predictor = DefaultPredictor(cfg)
    
    try:
        metadata = MetadataCatalog.get("tomato_val")
    except:
        print("Warning: Could not get metadata for tomato_val")
        metadata = None

    if not os.path.exists(IMG_DIR):
        print(f"Image directory {IMG_DIR} does not exist!")
        return
    
    img_list = [f for f in os.listdir(IMG_DIR) 
                if f.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp', '.tiff'))]
    
    if not img_list:
        print(f"No image files found in {IMG_DIR}")
        return
    
    random.shuffle(img_list)
    shown = 0

    print(f"Generating {n} visualization samples using {model_path}...")
    
    for file in img_list:
        try:
            img_path = os.path.join(IMG_DIR, file)
            im = cv2.imread(img_path)
            
            if im is None:
                print(f"Failed to load image: {img_path}")
                continue
                
            outputs = predictor(im)
            v = Visualizer(im[:, :, ::-1], metadata=metadata, scale=1.2)
            out = v.draw_instance_predictions(outputs["instances"].to("cpu"))

            save_path = os.path.join(out_dir, f"prediction_{shown}_{file}")
            cv2.imwrite(save_path, out.get_image()[:, :, ::-1])
            print(f"Saved: {save_path}")

            shown += 1
            if shown >= n:
                break
                
        except Exception as e:
            print(f"Error processing {file}: {e}")
            continue

def get_dataset_info():
    """è·å–æ•°æ®é›†ä¿¡æ¯ä»¥å¸®åŠ©è®¾ç½®æ­£ç¡®çš„å‚æ•°"""
    try:
        cfg = build_cfg()  # ä½¿ç”¨é»˜è®¤é…ç½®
        train_loader = build_detection_train_loader(cfg)
        train_size = len(train_loader)
        
        print(f"\nDataset Information:")
        print(f"{'='*40}")
        print(f"Training dataset size: {train_size} images")
        print(f"Images per batch: {cfg.SOLVER.IMS_PER_BATCH}")
        print(f"Iterations per epoch: {train_size // cfg.SOLVER.IMS_PER_BATCH}")
        print(f"Total iterations for {MAX_EPOCHS} epochs: {(train_size // cfg.SOLVER.IMS_PER_BATCH) * MAX_EPOCHS}")
        
        return train_size
    except Exception as e:
        print(f"Could not determine dataset size: {e}")
        return None

def analyze_dataset_areas():
    """åˆ†ææ•°æ®é›†ä¸­ç‰©ä½“çš„é¢ç§¯åˆ†å¸ƒ"""
    import json
    import numpy as np
    
    print(f"\nAnalyzing object area distribution for 5K images...")
    print(f"{'='*60}")
    
    for split in ['train', 'val', 'test']:
        ann_file = os.path.join(ANN_DIR, f"{split}.json")
        if not os.path.exists(ann_file):
            print(f"Annotation file {ann_file} not found")
            continue
            
        with open(ann_file, 'r') as f:
            data = json.load(f)
        
        # åˆ›å»ºå›¾åƒIDåˆ°å›¾åƒä¿¡æ¯çš„æ˜ å°„
        image_info = {img['id']: img for img in data['images']}
        
        areas_original = []
        areas_scaled = []
        
        # è·å–é…ç½®ä»¥äº†è§£ç¼©æ”¾å‚æ•°
        cfg = build_cfg()  # ä½¿ç”¨é»˜è®¤é…ç½®
        min_size = min(cfg.INPUT.MIN_SIZE_TRAIN)
        max_size = cfg.INPUT.MAX_SIZE_TRAIN
        
        for ann in data['annotations']:
            # åŸå§‹é¢ç§¯
            if 'area' in ann:
                area = ann['area']
            else:
                bbox = ann.get('bbox', [0, 0, 0, 0])
                area = bbox[2] * bbox[3]
            areas_original.append(area)
            
            # ä¼°ç®—ç¼©æ”¾åçš„é¢ç§¯
            img_id = ann['image_id']
            if img_id in image_info:
                img = image_info[img_id]
                orig_w, orig_h = img['width'], img['height']
                
                # æ¨¡æ‹ŸDetectron2çš„ç¼©æ”¾é€»è¾‘
                size = max(orig_w, orig_h)
                if size > max_size:
                    scale = max_size / size
                else:
                    scale = min_size / min(orig_w, orig_h)
                    if scale * size > max_size:
                        scale = max_size / size
                
                scaled_area = area * (scale ** 2)
                areas_scaled.append(scaled_area)
        
        areas_original = np.array(areas_original)
        areas_scaled = np.array(areas_scaled) if areas_scaled else areas_original
        
        print(f"\n{split.upper()} Set Analysis:")
        print(f"-" * 40)
        
        if len(data['images']) > 0:
            avg_width = np.mean([img['width'] for img in data['images']])
            avg_height = np.mean([img['height'] for img in data['images']])
            print(f"Average image size: {avg_width:.0f} x {avg_height:.0f}")
        
        print(f"Total objects: {len(areas_original)}")
        
        # åŸå§‹å›¾åƒä¸­çš„åˆ†å¸ƒ
        print(f"\nOriginal image object areas:")
        small_orig = np.sum(areas_original < 32**2)
        medium_orig = np.sum((areas_original >= 32**2) & (areas_original < 96**2))
        large_orig = np.sum(areas_original >= 96**2)
        
        print(f"  Small (<32Â²): {small_orig} ({small_orig/len(areas_original)*100:.1f}%)")
        print(f"  Medium (32Â²-96Â²): {medium_orig} ({medium_orig/len(areas_original)*100:.1f}%)")
        print(f"  Large (>96Â²): {large_orig} ({large_orig/len(areas_original)*100:.1f}%)")
        print(f"  Min area: {np.min(areas_original):.0f} pixelsÂ²")
        print(f"  Max area: {np.max(areas_original):.0f} pixelsÂ²")
        print(f"  Mean area: {np.mean(areas_original):.0f} pixelsÂ²")
        
        # ç¼©æ”¾åçš„åˆ†å¸ƒ
        print(f"\nAfter scaling to {min_size}-{max_size}:")
        small_scaled = np.sum(areas_scaled < 32**2)
        medium_scaled = np.sum((areas_scaled >= 32**2) & (areas_scaled < 96**2))
        large_scaled = np.sum(areas_scaled >= 96**2)
        
        print(f"  Small (<32Â²): {small_scaled} ({small_scaled/len(areas_scaled)*100:.1f}%)")
        print(f"  Medium (32Â²-96Â²): {medium_scaled} ({medium_scaled/len(areas_scaled)*100:.1f}%)")
        print(f"  Large (>96Â²): {large_scaled} ({large_scaled/len(areas_scaled)*100:.1f}%)")
        
        if small_scaled == 0:
            print(f"\n ! No small objects after scaling - APs metric will be -1")
        if medium_scaled == 0:
            print(f"! No medium objects after scaling - APm metric will be -1")
        if large_scaled == 0:
            print(f"! No large objects after scaling - APl metric will be -1")

if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description="Detectron2 Training Script for Instance Segmentation")
    
    # ä¸»è¦åŠ¨ä½œ
    parser.add_argument("action", choices=["train", "eval", "vis", "info", "analyze"], 
                       help="Action to perform: train / eval / vis / info / analyze")
    
    # è®­ç»ƒç›¸å…³å‚æ•°
    parser.add_argument("--lr", type=float, default=None,
                       help="Base learning rate (default: 0.0001)")
    parser.add_argument("--epochs", type=int, default=None,
                       help="Maximum number of epochs to train (default: 100)")
    parser.add_argument("--patience", type=int, default=None,
                       help="Early stopping patience in epochs (default: 10)")
    parser.add_argument("--model", type=str, default=None,
                       help="Model config file, e.g., 'COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml' (default: mask_rcnn_R_50_FPN_1x.yaml)")
    parser.add_argument("--output", type=str, default=None,
                       help="Output directory for saving models and results (default: ./r50x1)")
    
    # è¯„ä¼°å’Œå¯è§†åŒ–ç›¸å…³å‚æ•°
    parser.add_argument("--model-path", type=str, default="model_best.pth",
                       help="Model checkpoint file to use for eval/vis (default: model_best.pth)")
    parser.add_argument("--n", type=int, default=3,
                       help="Number of images to visualize (default: 3)")
    
    args = parser.parse_args()

    if args.action == "train":
        # å¯ç”¨çš„æ¨¡å‹é…ç½®ç¤ºä¾‹
        print("\nAvailable model configs:")
        print("  - COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_1x.yaml")
        print("  - COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml")
        print("  - COCO-InstanceSegmentation/mask_rcnn_R_101_FPN_3x.yaml")
        print("  - COCO-InstanceSegmentation/mask_rcnn_X_101_32x8d_FPN_3x.yaml")
        print("")
        
        train(model_config=args.model, 
              base_lr=args.lr, 
              max_epochs=args.epochs, 
              patience=args.patience,
              output_dir=args.output)
    
    elif args.action == "eval":
        evaluate(args.model_path, output_dir=args.output)
    
    elif args.action == "vis":
        visualize_random_sample(n=args.n, model_path=args.model_path, output_dir=args.output)
    
    elif args.action == "info":
        get_dataset_info()
    
    elif args.action == "analyze":
        analyze_dataset_areas()


# python seg.py train --model COCO-InstanceSegmentation/mask_rcnn_R_101_FPN_3x.yaml --lr 0.00096 --epoch 100 --patience 5